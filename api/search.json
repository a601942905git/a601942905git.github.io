[{"id":"a7366034e22a497a711893a0546ee23b","title":"系统重启你的应用会怎么样？","content":"1.前言近日就系统重启引发了一些思考，在系统重启过程中，正在进行的请求会如何被处理？正在消费的消息会不会丢失？异步执行的任务会不会被中断？既然存在这些问题，那我们的应用程序是不是就不能重启？但是，我们的应用程序随着版本迭代也在不断重启为什么这些问题没有出现呢？还是应用做了额外处理？带着这些疑问，结合场景模拟，看看实际情况怎么处理。\n2. 场景2.1 http请求2.1.1 创建请求@RestController\npublic class ShutDownController &#123;\n\n    @RequestMapping(\"shut/down\")\n    public String shutDown() throws InterruptedException &#123;\n        TimeUnit.SECONDS.sleep(20);\n        return \"hello\";\n    &#125;\n&#125;\n\n2.1.2 调用请求http://localhost:8080/shut/down\n2.1.3 模拟重启kill -2 应用pid\n\n2.1.4 现象\n2.1.5 结论请求执行过程中，关闭应用程序出现无法访问提示\n2.1.6 开启优雅关机如上出现的现象对用户来说很不友好，会造成用户一脸懵逼，那么有没有什么措施可以避免这种现象的出现呢？是否可以在应用关闭前执行完已经接受的请求，拒绝新的请求呢？答案可以的，只需要在配置文件中新增优雅关机配置\nserver:\n  shutdown: graceful # 设置优雅关闭，该功能在Spring Boot2.3版本中才有。注意：需要使用Kill -2 触发来关闭应用，该命令会触发shutdownHook\n\nspring:\n  lifecycle:\n    timeout-per-shutdown-phase: 30s # 设置缓冲时间，注意需要带上时间单位(该时间用于等待任务执行完成)\n\n添加完配置后，再次执行2.1.2和2.1.3流程，就会看到如下效果\n\n\n\n\n\n\n\n\n\n\n可以看到，即便在请求执行过程中关闭应用，已接收的请求依然会执行下去\n2.2 消息消费在前言提到过，消息消费过程中，关闭应用，消息是会丢失还是会被重新放入消息队列中呢？\n2.2.1 创建生产者@RestController\npublic class RabbitMqController &#123;\n\n    @Autowired\n    private RabbitTemplate rabbitTemplate;\n\n    @GetMapping(\"/sendBusinessMessage\")\n    public void sendBusinessMessage() throws InterruptedException &#123;\n        rabbitTemplate.convertAndSend(RabbitmqConfig.BUSINESS_EXCHANGE, RabbitmqConfig.BUSINESS_ROUTING_KEY, \"send message\");\n        TimeUnit.SECONDS.sleep(10000);\n    &#125;\n&#125;\n\n2.2.2 创建消费者@Component\n@RabbitListener(queues = RabbitmqConfig.BUSINESS_QUEUE_NAME)\n@Slf4j\npublic class BusinessConsumer &#123;\n\n    /**\n     * 操作场景：\n     * 1.通过RabbitmqApplication启动类启动应用程序\n     * 2.调用/sendBusinessMessage接口发送消息\n     * 3.RabbitMQ broker将消息发送给消费者\n     * 4.消费者收到消息后进行消费\n     * 5.消费者消费消息过程中，应用程序关闭，断开channel，断开connection，未ack的消息会被重新放入broker中\n     *\n     * @param content 消息内容\n     * @param channel channel通道\n     * @param message message对象\n     */\n    @RabbitHandler\n    public void helloConsumer(String content, Channel channel, Message message) &#123;\n        log.info(\"business consumer receive message：&#123;&#125;\", content);\n        try &#123;\n            // 模拟业务执行耗时\n            TimeUnit.SECONDS.sleep(10000);\n        &#125; catch (InterruptedException e) &#123;\n            e.printStackTrace();\n        &#125;\n    &#125;\n&#125;\n\n2.2.3 调用请求http://localhost:8080/sendBusinessMessage\n2.2.4 未关闭应用前\n2.2.5 关闭应用后\n2.2.6 结论消息消费过程中，关闭应用，未ack的消息会被重新放入消息队列中，以此来保证消息一定会被消费\n2.3 异步任务2.3.1 线程池配置@Component\npublic class ThreadPoolConfig &#123;\n\n    @Bean\n    public ThreadPoolTaskExecutor threadPoolTaskExecutor() &#123;\n        ThreadPoolTaskExecutor threadPoolTaskExecutor = new ThreadPoolTaskExecutor();\n        threadPoolTaskExecutor.setThreadNamePrefix(\"test-\");\n        threadPoolTaskExecutor.setCorePoolSize(3);\n        threadPoolTaskExecutor.setMaxPoolSize(3);\n        threadPoolTaskExecutor.setQueueCapacity(100);\n        return threadPoolTaskExecutor;\n    &#125;\n&#125;\n\n2.3.2 异步任务请求@Autowired\nprivate ThreadPoolTaskExecutor threadPoolTaskExecutor;\n\n@RequestMapping(\"async/task\")\npublic void asyncTask() throws InterruptedException &#123;\n  for (int i = 0; i &lt; 10; i++) &#123;\n    threadPoolTaskExecutor.execute(() -> &#123;\n      try &#123;\n        TimeUnit.SECONDS.sleep(10);\n      &#125; catch (InterruptedException e) &#123;\n        throw new RuntimeException();\n      &#125;\n      log.info(\"task execute complete...\");\n    &#125;);\n  &#125;\n&#125;\n\n2.3.3 调用请求http://localhost:8080/async/task\n2.3.4 模拟重启kill -2 应用pid\n\n2.3.5 现象Exception in thread \"test-2\" Exception in thread \"test-1\" Exception in thread \"test-3\" java.lang.RuntimeException\n\tat com.boot.example.ShutDownController.lambda$asyncTask$0(ShutDownController.java:37)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\njava.lang.RuntimeException\n\tat com.boot.example.ShutDownController.lambda$asyncTask$0(ShutDownController.java:37)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\njava.lang.RuntimeException\n\tat com.boot.example.ShutDownController.lambda$asyncTask$0(ShutDownController.java:37)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\n2.3.6 修改线程池配置在线程池配置中添加如下配置：\nthreadPoolTaskExecutor.setWaitForTasksToCompleteOnShutdown(true);\nthreadPoolTaskExecutor.setAwaitTerminationSeconds(120);\n\n2.3.7 修改配置后现象2021-12-09 17:09:40.054  INFO 22383 --- [         test-1] com.boot.example.ShutDownController      : task execute complete...\n2021-12-09 17:09:40.055  INFO 22383 --- [         test-3] com.boot.example.ShutDownController      : task execute complete...\n2021-12-09 17:09:40.055  INFO 22383 --- [         test-2] com.boot.example.ShutDownController      : task execute complete...\n2021-12-09 17:09:50.059  INFO 22383 --- [         test-3] com.boot.example.ShutDownController      : task execute complete...\n2021-12-09 17:09:50.059  INFO 22383 --- [         test-1] com.boot.example.ShutDownController      : task execute complete...\n2021-12-09 17:09:50.060  INFO 22383 --- [         test-2] com.boot.example.ShutDownController      : task execute complete...\n2021-12-09 17:10:00.062  INFO 22383 --- [         test-2] com.boot.example.ShutDownController      : task execute complete...\n2021-12-09 17:10:00.062  INFO 22383 --- [         test-1] com.boot.example.ShutDownController      : task execute complete...\n2021-12-09 17:10:00.065  INFO 22383 --- [         test-3] com.boot.example.ShutDownController      : task execute complete...\n2021-12-09 17:10:10.066  INFO 22383 --- [         test-1] com.boot.example.ShutDownController      : task execute complete...\n\n2.3.8 结论使用线程池执行异步任务，在没有添加配置的情况下，任务无法执行完成，在添加配置的情况下，任务依然可以执行完成。\n3. 总结为了保证在应用程序重启过程中任务仍然可以执行完成，需要开启优雅关机配置并对线程池添加等待任务执行完成以及等待时间配置\n","slug":"系统重启你的应用会怎么样？","date":"2022-12-10T13:58:00.000Z","categories_index":"微服务","tags_index":"","author_index":"黑白搬砖工"},{"id":"24fad9e0b20684c1cf6b335f2dccd634","title":"三种你需要考虑的一致性问题的场景","content":"1.前言分布式系统开发过程中往往会涉及到很多需要保证数据一致性问题的场景，比如接收mq消息、接收http请求、内部业务处理。如果你还不了解这些场景或者不知道如何处理，请继续往下阅读。\n2.接收mq消息\n接收mq消息场景在分布式系统开发过程中想必是一种比较常见的场景，具体过程就是，外围系统推送mq消息到开发系统，开发系统接收消息后进行业务逻辑处理。从表面上看是一个十分简单的流程，但是如果涉及数据一致性问题，就不那么简单了。\n为什么说不那么简单呢？先来看看以下场景：\n2.1 先ack消息再处理业务ack();\n// 处理业务逻辑\n\n先ack消息再处理业务在理想场景下是不会有什么问题，看到这里你可能会有疑惑，没有问题那不就得了，还需要考虑什么呢？请注意这种没问题是建立在理想的前提下，如果业务处理过程中调用外部接口异常或者数据库宕机，就会导致消息丢失，进而出现数据不一致性的问题。\n2.2 先处理业务再ack消息// 处理业务逻辑\nack();\n\n既然先ack消息再处理业务这条路走不通，那么就先处理业务再ack消息总没问题了吧。是的，即便业务处理失败消息没有被ack，消息还会被重新消费，不会出现数据不一致性问题。但是这会涉及到另外一个问题，那就是幂等性问题，幂等问题处理不好，还是会引起数据不一致性问题。\n其实先处理业务再ack消息还会引起另一个问题，如果业务系统有bug会造成消息一直无法被ack，进而会导致消息处理进入死循环。\n这样也不行，那样也不行，就没有解决方案了？当然不是，方案还是有的，且听我慢慢跟你说\n2.3  结合消息ack机制 + 数据库 + 定时任务方案一try &#123;\n    try&#123;\n        // 根据消息唯一编号查询该消息是否已处理过，如果没有处理过，进行处理业务；如果处理过，则说明都不做\n    &#125; catch (Exception e) &#123;\n       // 将处理异常的消息插入数据库中\n    &#125;\n    ack();\n&#125; catch (Exception e) &#123;\n    unack();\n&#125;\n\n大体思路就是，根据消息唯一编号判断消息是否被处理过，如果未被处理过，就对消息进行处理，处理成功则对消息进行ack；处理失败则将消息存入数据库中。如果存入数据库这一步操作还是失败，那么就对消息进行unack操作，将消息重新投递到消息服务器中，进而重新消费，直到数据库恢复为止。针对处理失败入库的消息，可以通过定时任务重试处理。\n该方案不仅可以解决2.2中的幂等性问题，还可以解决业务出现bug进而导致消息处理进入死循环的问题(限制重试次数)。但是该方案还是会存在一个跟本文无关的问题，那就是消息积压问题。\n2.4 结合消息ack机制 + 数据库 + 定时任务方案二try &#123;\n    // 根据消息唯一编号查询该消息是否存在，不存在则直接插入数据库中，存在则不进行处理\n    ack();\n    // 异步处理业务逻辑\n&#125; catch (Exception e) &#123;\n    unack();\n&#125;\n\n2.4与2.3优缺点对比\n\n\n\n序号\n优点\n缺点\n\n\n\n2.3\n只在业务处理失败将消息插入数据库中，消息数量不会太多\n消息处理慢会导致消息积压\n\n\n2.4\n消息异步处理，不会导致消息积压\n所有消息都存储数据库，消息数量可能会很多\n\n\n关于这两种方案可以根据实际情况进行自由选择，消息积压问题处理也可以参考：消息积压你作何处理？\n3. 接收http请求\n看到这个图你可能会想这不就是一个很简单的流程嘛，开发系统接收请求、处理请求、响应结果就可以了。如你所想，确实很简单，但是如果你的开发系统业务处理失败，就会导致外围系统进行重试，直到重试次数用完，开发系统还未恢复正常，那么此次的外围请求数据就会丢失，从而引起数据不一致性问题。\n认真分析一下该场景，你会发现造成数据不一致性问题的关键在于开发系统的业务处理。如果开发系统能在正确接收外围系统请求后立刻进行响应，那么就可以解决该问题。\n我们只需要在接收http请求后，将请求内容写入数据库，写入成功进行异步处理并返回成功；写入失败返回失败，通过外围重试来保证数据可以正常写入数据库。处理失败的内容可以结合定时任务对请求进行重试。\n\n4.内部业务处理\n开发系统某些业务在处理成功后往往需要通知某些外围系统并且还不能因为外围系统故障从而导致当前业务无法正确处理，既然不能影响当前业务，可以采用异步的方式进行处理，异步处理就会存在当前业务处理成功，通知外围失败的问题，进而引起数据不一致性问题。\n那有没有办法可以保证业务处理成功的同时，对外围的通知也一定成功呢？\n我们可以采用本地事务的方案，把通知给外围的数据放在当前业务的事务中插入到数据库，异步通知外围系统，处理失败的数据再结合定时任务进行重试。\n\n5.总结从文中我们可以看到一致性问题的解决方案都逃不开数据库 + 重试，因此在解决一致性问题的时候可以多往这方面考虑。\n","slug":"三种你需要考虑一致性问题的场景","date":"2022-12-10T13:57:00.000Z","categories_index":"微服务","tags_index":"一致性","author_index":"黑白搬砖工"},{"id":"a74d032dc9bf8ca9d012adc342b4fd84","title":"动态伸缩你的服务","content":"1.前言如你所知，服务的常规部署方式如下：\n\n对外暴露的服务都会在前面部署nginx用于提供反向代理和负载均衡能力\n下面会快速部署一套类似的服务，分析其存在的问题并给出相应解决方案\n2.应用相关2.1 启动服务使用boot-cloud-openfeign-provider启动3个服务实例，端口分别为8081、8082、8083\n2.2 服务验证在浏览器中分别输入：http://localhost:8081/index/nginx、http://localhost:8082/index/nginx、http://localhost:8083/index/nginx，保证3个服务实例均可被访问\n\n\n\n\n\n\n\n\n\n服务运行依赖5.1和5.2中的consul\n3.nginx相关3.1 安装3.1.1 搜索镜像docker search nginx\n\n3.1.2 拉取镜像docker pull nginx\n\n3.1.3 运行镜像docker run --name nginx -p 80:80 -d nginx\n\n3.1.4 拷贝镜像配置文件docker cp nginx:/etc/nginx/nginx.conf path/nginx[宿主机放配置文件路径]\n\n3.1.5 删除容器docker rm -f nginx\n\n3.1.6 指定配置文件运行镜像docker run --name nginx -p 80:80 -v [宿主机路径]/nginx/nginx.conf:/etc/nginx/nginx.conf:ro -v [宿主机路径]/nginx/conf.d:/etc/nginx/conf.d -d nginx\n\n3.2 配置3.2.1 配置负载均衡在[宿主机路径]/nginx/conf.d目录下创建load-balancer.conf文件，内容如下：\nupstream backend &#123;\n    server 10.100.40.243:8081;\n    server 10.100.40.243:8082;\n    server 10.100.40.243:8083;\n&#125;\n\nserver&#123;\n    listen 80;\n\n    location / &#123;\n        proxy_pass http://backend;\n    &#125;\n&#125;\n\n3.3 验证3.3.1 重启nginx服务docker exec -it nginx nginx -s reload\n\n3.3.2 访问nginx服务在浏览器中输入：http://localhost/index/nginx，多次请求依次可以看到`openfeign service：hello nginx from 8081、openfeign service：hello nginx from 8082、openfeign service：hello nginx from 8083&#96;结果，则说明nginx负载均衡配置正常。\n4.问题如你所见，服务的负载均衡能力是通过在load-balancer.conf配置文件中写死服务列表来实现的，同时也意味着，只要后端服务列表发生变化，就需要修改配置并通过nginx -s reload命令来重新加载。\n5.方案有了问题，既然会有对应的解决方案，本文要介绍的解决方案就是consul和consul template\n5.1 安装consul根据Install Consul官网教程安装consul\n5.2 运行consulconsul agent -dev\n\n5.3 安装consul template5.3.1 下载压缩包wget https://releases.hashicorp.com/consul-template/0.20.0/consul-template_0.20.0_linux_amd64.zip\n\n5.3.2 解压unzip consul-template_0.20.0_linux_amd64.zip\n\n\n\n\n\n\n\n\n\n\nMac os可以通过brew install consul-template进行安装，简单、方便\n5.4 创建负载均衡配置文件模板创建load-balancer.ctmpl文件，编辑如下内容：\nupstream backend &#123;\n    &#123;&#123;- range service \"openfeign-provider-service\" &#125;&#125;\n        server &#123;&#123; .Address &#125;&#125;:&#123;&#123; .Port &#125;&#125;;\n    &#123;&#123;- end &#125;&#125;\n&#125;\n\nserver &#123;\n   listen 80;\n\n   location / &#123;\n      proxy_pass http://backend;\n   &#125;\n&#125;\n\n5.5 清空配置文件清空3.2.1章节load-balancer.conf文件内容\n5.6 创建consul template配置创建consul-template-config.hcl文件，编辑如下内容：\nconsul &#123;\n  address = \"localhost:8500\"\n\n  retry &#123;\n    enabled  = true\n    attempts = 12\n    backoff  = \"250ms\"\n  &#125;\n&#125;\ntemplate &#123;\n  source      = \"[5.4章节指定的路径]/load-balancer.conf.ctmpl\"\n  destination = \"[5.5章节指定的路径]/load-balancer.conf\"\n  perms       = 0600\n  command     = \"sh -c docker exec -it nginx nginx -s reload\"\n&#125;\n\n\n\n\n\n\n\n\n\n\n\naddress：指定consul地址；source：用于指定负载均衡配置模板文件路径；destination：负载均衡配置生成文件路径；command：用于指定要执行的命令。整体流程：consul template从address地址拉取服务地址列表，根据source模板文件生成负载均衡配置到destination文件中，执行command重新加载nginx\n5.7 运行consul templateconsul-template -config=consul-template-config.hcl\n\n\n\n\n\n\n\n\n\n\n运行consul template之后，会发现load-balancer.conf文件中有了负载均衡配置\n5.8 服务关闭、启动到这里，你会发现伴随着服务关闭、服务启动，load-balancer.conf配置文件中的内容也在一直跟着改变，从而实现服务动态伸缩。\n","slug":"动态伸缩你的服务","date":"2022-12-10T13:57:00.000Z","categories_index":"微服务","tags_index":"consul,nginx,consul-template","author_index":"黑白搬砖工"}]