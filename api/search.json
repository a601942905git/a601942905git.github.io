[{"id":"c301ef2215df3326fec1a274349ffccd","title":"Redis单机、主从、哨兵、集群演进之路","content":"1.前言1.1 单机时代刚接触redis的时候，为了能快速学习和了解这门技术，我们通常会在自己的电脑上部署一个redis服务，以此来开启redis学习之路\n1.2 主从时代随着对redis的进一步深入，很快就会发现这门技术在很多场景下都能得到应用，比如：并发场景下对共享资源的控制(分布式锁)、高并发场景下对系统的保护(限流)、高并发场景下对响应时间的要求(缓存)\n在生产环境使用redis服务是否能像当初我们学习时那样，仅仅部署一个单实例redis就可以呢？如果选择单实例部署，当该实例出现故障，使用redis的业务场景都会随之受影响。为了降低单实例故障带来的影响，通常会选择冗余的方式来保证服务的高可靠性，在redis中，我们称之为主-从\n1.3 哨兵时代redis进行主-从部署后是不是就可以高枕无忧了呢？当然不是，你还需要时时刻刻监控redis主的健康状态，当其出现故障后能第一时间发现并能在从库中完成选主任务，否则同样会给相关业务带来影响。在redis中，我们通常会使用哨兵机制来帮我们完成监控、选主和通知操作，从而使我们的redis服务具备一定的高可靠性\n1.4 集群分片时代随着业务的飞速发展，redis实例中存放的数据也越来越多，当需要存储25G以上的数据时，估计你会选择一台32G的机器进行部署这个看似简单的选择题却隐藏着很严重的问题：\n\nredis中存放的数据越多，意味着宕机后的恢复时间也越长，从而导致服务长时间不可用\n\n数据越多进行rdb fork操作的时候，阻塞redis主线程的时间也越长，从而导致服务响应慢\n\n\n面对此类问题我们通常会基于大而化小、分而治之的思想进行解决，在redis中我们称之为集群分片技术\n开篇对单机、主从、哨兵、分片进行了简单介绍，下文将结合实战展开细说。\n2.redis安装磨刀不误砍柴工，在开始之前首先要保证我们的redis服务可以正常启动并能提供指令操作。\n2.1下载安装包wget https://download.redis.io/redis-stable.tar.gz\n\n2.2解压tar -zxvf redis-stable.tar.gz\n\n2.3 编译➜  redis-stable make &amp; make install\n\n2.4 启动redis-server\n\n2.5 客户端连接测试redis-cli\n127.0.0.1:6379> set name bobo\nOK\n127.0.0.1:6379> get name\n\"bobo\"\n\n完成服务的安装与测试，接下来就可以放手进行实战，实战的第一部分主从\n3.主从3.1 部署图\n主从一共由3台机器组成，演示环境通过端口号进行区分\n3.2 创建conf目录主从由3个redis服务组成，每个redis服务对应的配置都不相同，因此需要创建conf文件夹用于存放配置文件\n➜  redis-stable mkdir conf\n\n3.3 生成redis配置文件默认情况下redis.conf配置文件会有很多注释说明，为了让配置文件看上去清晰明了，使用如下命令来去除配置文件中的注释以及空行\n➜  redis-stable cat redis.conf | grep -v \"#\" | grep -v \"^$\" > ./conf/redis-6379.conf\n\n3.3 添加复制配置主从方式，从需要知道应该从哪一个主进行数据复制，因此需要在从再配置文件中添加如下配置\nreplicaof 127.0.0.1 6380\n\n3.4 拷贝redis配置文件之前也有过说明，演示环境通过端口号进行区分，因此配置文件中除了端口号以及数据保存路径不一样之外，其它的都一样，这里可以通过如下命令进行配置文件拷贝\n➜  redis-stable sed 's/6379/6380/g' conf/redis-6379.conf > conf/redis-6380.conf\n➜  redis-stable sed 's/6379/6381/g' conf/redis-6379.conf > conf/redis-6381.conf\n\n3.5 启动redis万事俱备，只欠东风，准备工作完成之后，只需要逐个启动服务即可\n3.5.1 启动主6379从在启动的时候需要和主建立连接，因此应先启动主6379\n\n通过日志文件可以了解到主6379在启动的过程中会去加载rdb文件用于数据恢复，并且从rdb文件中加载了一个key\n3.5.2 启动从6380\n从6380启动后，可以看到主6379的日志内容有所增加\n从6380向主6379请求数据同步，由于是第一次会进行全量同步，主6379 fork进程生成rdb文件，然后将生成好的rdb文件传输给6380\n\n从6380启动过程中会去连接主6379，接收主6379传过来的rdb文件，在清空旧数据之后加载rdb文件进行数据同步\n3.5.3 启动从6381\n从6381启动和从6380启动是一样的流程，不再进行细说\n3.5.4 重启6381到这里，你已经知道从在第一次连接主后会进行全量同步，估计也会好奇非第一次连接会如何进行同步。想知道结果，只需要重启其中一个从即可，这里选择重启从6381\n\n重启后，会发现从6381请求的是增量同步而非全量同步\n\n通过主6379的日志可以看到其接受了从6381增量同步的请求，并从backlog偏移量183开始发送了245字节的数据\n这里可以猜想一下，主6379若想知道应该增量同步哪些数据给从6381，那么它一定得知道从6381上一次同步到哪里，因此重启再次连接的时候，从6381应该会将上一次同步到哪了的信息发给了主6379\n关于数据同步部分，可以得出不知道从哪开始同步就选择全量同步，知道从哪开始同步就选择增量同步的结论\n3.6 主从如何保证数据一致性？前面提到过，主6379在接到从的全量同步请求后会生成rdb文件，在生成rdb文件的过程中以及将rdb文件传给从并且从使用rdb文件恢复数据的过程中都没有新命令产生，那么主从的数据就可以保持一致。\n如果这期间产生了新的命令会不会导致主从数据不一致?\n根据官方文档 How Redis replication works中的介绍，我们可以知道期间产生的新命令会被主缓存起来，在从加载完rdb文件数据之后，主会将这期间缓存的命令发送给从，从在接受并执行完这些新命令后，就可以继续保持与主数据的一致性\n\n4.哨兵redis主从固然可以提升服务的高可靠性，却依然需要人为去进行监控、选主和通知。看上去似乎不是很靠谱，因为我们不可能做到7 * 24小时盯着redis服务，在其出问题后手动进行故障转移并通知客户端新主的地址。\nredis中我们可以通过哨兵机制来实现监控、选主、通知流程自动化，一来可以减轻开发人员压力；二来可以降低人为误操作率；三来可以提升故障恢复时效性。\n接下来会展示如何去搭建哨兵集群以及如何进行选主和通知客户端新主地址\n4.1 生成sentinel配置文件➜  redis-stable cat sentinel.conf | grep -v \"#\" | grep -v \"^$\" > ./conf/sentinel-26379.conf\n\n4.2 拷贝sentinel配置文件➜  redis-stable sed 's/26379/26380/g' conf/sentinel-26379.conf > conf/sentinel-26380.conf\n➜  redis-stable sed 's/26379/26381/g' conf/sentinel-26379.conf > conf/sentinel-26381.conf\n\n4.3 启动sentinel4.3.1 启动26379\n通过日志可以看到sentinel在启动的时候会生成一个唯一id，也就是Sentinel Id，并且还打印出了redis主从中从的相关信息，可是根据sentinel配置文件中的配置sentinel monitor mymaster 127.0.0.1 6379 2，sentine是不知道从的相关信息，那么它是从哪得到这些信息的呢？\n要想获得这些信息，sentinel只需要给监控的主发送info命令即可\n\n4.3.2 启动26380\n启动sentinel26380的时候通过日志可以看到其发现了sentinel26379的存在\n查看配置文件，可以看到配置文件中新增了从和其它sentitnel相关配置\n\n4.3.3 启动26381\n\n同理sentinel26381启动的时候发现了sentinel26379和sentinel26380的存在并在配置文件中新增了相关配置\n4.3 sentinel是如何发现彼此的存在？在sentinel26381启动完成后，sentinel集群也就搭建完成了，在搭建的过程中也留下了一个疑问：sentinel是如何发现彼此的存在？\n根据官方文档High availability with Redis Sentinel中Sentinels and replicas auto discovery的介绍，可以了解到sentinel是通过Pub/Sub 机制来发现彼此的存在，当一个sentinel与主连接后，可以在__sentinel__:hello通道中发布其对应的ip、port和runid，同时订阅__sentinel__:hello通道，这样其它sentinel发布消息的时候就可以得知对应的ip和port。\n\n4.4 sentinel功能验证sentinel集群搭建完成后，我们需要停掉主来验证其是否完成监控、选主和通知任务。\n首先停掉主6379，分别观察3个sentinel的变化\n4.4.1 观察sentinel26379\n4.4.2 观察sentinel26380\n4.4.3 观察sentinel26381\n通过观察日志可以看到主6379下线后的一些变化：\n\n每个sentinel都监控到主6379下线，主观上认为其下线了(由于网络原因，可能存在误判)，对应日志中的+sdown部分\n当半数以上的sentinel认为主6379下线，则客观上认为其下线了(排除误判)，对应日志中的+odown部分\nsentinel集群通过投票方式选出sentinel26379作为leader来执行选主操作\nsentinel26379选择redis6380作为新的主，修改slave配置\n\n4.4.5 重启redis6379sentinel选出新主后，是否将新主地址通知到各个客户端。想要验证这个问题，只需要重启redis6379即可。\n\nredis6379启动后成功连接上新主redis6380并进行数据同步\n4.5 从和重启后的旧主是如何知道新主的地址？通过上面的实战演示，可以得知，主宕机选出新主后，剩余的从会自动连接上新主并进行数据同步，旧主重启后也可以正常连接上新主并进行数据同步。对于从能连接上新主还可以理解，毕竟redis主从切换的过程中，从起码是运行状态，但是旧主在redis主从切换的过程中处于宕机状态，为什么在重启后还可以正常连接上新主？\n猜想应该是：在redis主从切换完成后，sentinel leader向从发送了slaveof 新主host 新主port命令，这样从就可以与新主连接并进行数据同步。针对旧主在其重启后，sentinel与之建立连接并发送slaveof 新主host 新主port命令，这样旧主也可以与新主连接并进行数据同步。\n4.6 客户端应用是如何知道新主的地址？使用springboot集成redis sentinel，通常会在application.yml中添加如下配置：\nspring:\n  redis:\n    sentinel:\n      master: mymaster\n      nodes:\n        - 127.0.0.1:26379\n        - 127.0.0.1:26380\n        - 127.0.0.1:26381\n\n配置文件中仅仅只配置了sentinel节点信息，并没有配置redis相关地址信息，客户端是如何知道redis地址信息，当redis发送主从切换，客户端又是如何知道新主的地址信息？\n根据官方文档High availability with Redis Sentinel中Obtaining the address of the current master的介绍，客户端可以通过SENTINEL get-master-addr-by-name mymaster命令获取当前主的地址信息\n\n结合客户端源码分析\n\n\n我们可以得知客户端是通过向sentinel发送get-master-addr-by-name mymaster命令来获取redis主的连接地址\n4.7 总结\nsentinel之间通过redis的pub/sub机制发现彼此的存在\nredis主从切换后，sentinel向从和旧主发送slaveof host port命令来连接新主\n客户端通过向sentinel发送get-master-addr-by-name mymaster命令来获取redis主的连接地址\n\n5.集群redis主从模式在大多数场景下都是比较适用，在面对需要持久化大量数据的场景下会变得比之前慢。慢的主要原因是：在进行rdb持久化会fork出一个子进程，fork操作会阻塞主线程并且阻塞时间与内存中的数据成正相关。因此，在面对大量数据需要持久化的场景时，可以考虑选择redis-cluster来进行应对。\n在使用集群时，需要考虑下面问题：\n\nkey/value在集群中如何分布\n想对某个key/value进行操作时，该连接那个节点\n集群节点数量发生变化，还用原来的地址操作key/value是否可行？\n\n为了弄明白这些问题，按照惯例，先来搭建redis集群\n5.1 集群搭建5.1.1 创建conf目录➜  redis-stable mkdir conf\n\n5.1.2 创建配置文件在conf目录下创建redis-7001.conf配置文件，内容如下：\nport 7001\ncluster-enabled yes\ncluster-config-file nodes-7001.conf\ncluster-node-timeout 5000\nappendonly yes\n\n5.1.3 拷贝集群配置文件➜  redis-stable sed 's/7001/7002/g' conf/redis-7001.conf > conf/redis-7002.conf\nsed 's/7001/7003/g' conf/redis-7001.conf > conf/redis-7003.conf\nsed 's/7001/7004/g' conf/redis-7001.conf > conf/redis-7004.conf\nsed 's/7001/7005/g' conf/redis-7001.conf > conf/redis-7005.conf\nsed 's/7001/7006/g' conf/redis-7001.conf > conf/redis-7006.conf\n\n5.1.4 分别启动redis服务➜  redis-stable redis-server conf/redis-7001.conf\n➜  redis-stable redis-server conf/redis-7002.conf\n➜  redis-stable redis-server conf/redis-7003.conf\n➜  redis-stable redis-server conf/redis-7004.conf\n➜  redis-stable redis-server conf/redis-7005.conf\n➜  redis-stable redis-server conf/redis-7006.conf\n\n5.1.5 创建集群redis-cli --cluster create 127.0.0.1:7001 127.0.0.1:7002 \\\n127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005 127.0.0.1:7006 \\\n--cluster-replicas 1\n\n输入完命令后，可以看到如下分配方案，针对分配方案输入yes即可\n\n为了方便理解，可以看下图\n\n到此，redis集群就搭建完成了\n5.2 客户端连接集群5.2.1 缓存slot与实例映射关系搭建完redis集群后，可以通过客户端来连接集群，为了能更清晰了解客户端连接集群都做了些什么，这里选择使用spring boot应用作为客户端。通过源码调试，可以看到如下结果：\n\n客户端应用在启动的时候会向redis集群实例发送cluster slots命令获取slot分配信息，然后在本地缓存slot与redis实例的映射关系\n5.2.2 根据key计算对应slot\n客户端在对key/value进行操作时，会对key进行CRC计算并和cluster slot数量进行与运算得到最终的slot值\n5.2.3 根据slot获取实例\n5.2.1中提到客户端会缓存slot与实例映射关系，5.2.2根据计算得到对应的slot，若想与redis实例交互，此时只需要从映射缓存中获取对应的实例既可\n5.2.4 刷新slot与实例映射缓存关系集群环境往往会伴随着实例的上线与下线，不管是上线还是下线，都会使得slot重新分配，原本在某个实例上的slot会被分配到新的实例上，针对这种情况，客户端如果还是请求旧实例会发生什么？客户端又如何知道slot对应的实例发生了变化？\n为了演示该场景，会再运行两台redis实例，分别为:redis7007和redis7008，redis实例启动完成之后，通过redis-cli --cluster add-node 127.0.0.1:7007 127.0.0.1:7001命令将redis7007以master的方式加入到已存在的集群中，再通过redis-cli --cluster add-node 127.0.0.1:7008 127.0.0.1:7000 --cluster-slave命令将redis7008以slave的方式加入到已存在的集群中。\nredis实例加入集群后，可以通过redis-cli --cluster check 127.0.0.1:7001命令来查看集群的当前情况\n\n根据输出内容可以看到新加入的redis7007实例还没有分配slot，可以通过redis-cli --cluster reshard 127.0.0.1:7001命令来分配slot，分配成功之后原本在redis7001、redis7002、redis7003实例上的slot会移动到redis7007上。\n找一个原本在redis7001实例上后来被移动到redis7007实例上的key，执行get命令\n\n执行完get命令后，可以看到服务端返回了错误，并告诉我们当前key已经移动到redis7007实例上，这是redis-cli客户端执行后的结果，应用客户端会如何处理呢？\n\n通过源码分析，可以得知当出现JedisMovedDataException异常后，应用客户端会重新发送cluster slots命令来刷新本地缓存\n5.3 总结\n集群部署后，16384个slot会散落在各个redis master实例上\n客户端通过发送cluster slots命令在本地缓存slot与redis实例的映射关系\n对key/value进行操作时，key通过CRC计算并和slot数量 - 1进行与运算后得到对应slot，得到slot，又知道slot与redis实例的映射关系，就可以对redis实例进行访问\nslot迁移后，客户端还用原来的实例执行命令，会出现异常，针对异常客户端会通过发送cluster slots命令来刷新本地slot与redis实例的映射关系\n\n","slug":"Redis单机、主从、哨兵、集群演进之路","date":"2023-01-04T01:49:00.000Z","categories_index":"redis","tags_index":"redis,master-slave,sentinel,cluster","author_index":"黑白搬砖工"},{"id":"077c9e1306e9e7f4038efb1171146e9a","title":"一文读懂Redis持久化机制","content":"1.前言redis作为内存数据库，最常见的使用场景就是当缓存用。浏览器访问后端应用，后端应用先访问redis，如果redis中有数据，直接返回；否则就去查询数据库。\n\n如你所知，redis作为内存数据库，数据存放在内存中，如果宕机，会导致数据全部丢失。数据丢失，来自浏览器的所有请求都会去查询数据库，一方面会给数据库带来巨大压力；另一方面由于这些请求需要访问数据库，相比访问redis会导致响应时间变长。\n假如redis在宕机恢复后数据不丢失，就可以避免给数据库造成巨大压力和导致响应时间变长问题。要想实现数据恢复，可以使用redis持久化方案aof和rdb\n2.AOF2.1 官方说明\n1.redis官方认为RDB快照方式可以满足大多数应用，因此默认情况下只会开启RDB快照方式进行持久化。\n2.RDB快照方式在服务停电时会造成数分钟写丢失，为了保证数据少丢，可以选择使用AOF持久化\n3.AOF和RDB可以同时开启，如果AOF功能开启，Redis在启动时会加载AOF文件\n2.2 开启AOF功能将appendonly后面的no改成yes即可开启AOF功能\n2.3 验证AOF功能通过客户端执行set k1 v1，在AOF文件中可以看到如下内容\n\n2.3 写入时机redis为了避免检查写命令开销，选择了先写命令，再记日志\n\n2.3 写入策略redis记日志是在主线程中完成，为了平衡高可靠性与高性能，提供了三种写入策略\n\n\n\n\n写入策略\n优点\n缺点\n\n\n\nno(操作系统写回)\n性能最高\n数据丢的最多\n\n\nalways(同步写回)\n性能最差\n可以保证几乎不丢数据\n\n\neverysec(每秒写回)\n性能适中\n最多丢1s数据\n\n\nredis默认情况下选择每秒写回策略，丢失1s数据一般还是比较能接受；如果选择操作系统写回说明能够接受大量数据丢失，此时可以考虑只使用RDB,不开启AOF\n2.4 aof文件过大怎么办当redis开启aof持久化功能后，客户端的写命令会被持续记录在aof日志文件中，时间越长，aof日志文件就会越大，那么文件过大会有什么影响呢？\n\n文件系统对文件大小有限制，无法保存过大文件\n如果文件过大，之后再往里面追加日志记录，效率也会变低\nredis宕机，重启过程中会执行aof文件中的记录，如果文件过大，会导致重启时间过长，服务长时间不可用\n\n为了避免这些影响，aof文件在达到一定大小后就需要对其重写\n2.5 重写依据\naof文件大小达到64M并且当前大小相比上一次重写后aof文件的大小增长100%就会触发重写操作\n2.4 aof重写过程aof重写可以选择在主线程中执行，主线程读取redis中的数据以命令的方式记录在重写aof文件中，读取完成后，用重写aof文件替换旧aof文件即可完成重写。方案简单易用，问题同样也很明显，那就是在重写完成之前无法对外提供服务。此时你大概会很好奇为什么redis在提供重写能力的同时还能响应客户端的请求，它是如何实现的呢？答案就是fork()，我们来看看fork()究竟干了什么\n\naof文件达到重写要求后，redis会通过fork()系统调用创建出一个子进程(用来执行重写操作)，子进程会拷贝主进程的虚拟地址空间，这样主进程和子进程就可以共享内存中的数据。\n子进程重写过程中只需要读取内存中的数据以命令方式追加到重写aof文件中即可，重写过程中产生的新的写命令会被主进程记录在重写aof缓冲中，内存中的数据重写完成后再将重写aof缓冲中的命令追加到重写aof文件中，最后用重写aof文件替换原aof文件就完成了整个重写操作。\n为了引出下一个知识点，先带你分析一种场景：重新过程中redis中有这样一条数据k1 =1,客户端执行了incr k1命令后k1就等于2，主进程会在重新aof缓冲中记录incr k1命令，由于k1在内存中的数据等于2，子线程重写追加到文件中的命令应该是set k1 = 2，等重写完内存中的数据后，再追加重写aof缓冲中的incr k1命令，就会导致k1实际的值变成3而不是2，产生数据不一致性问题。\n如何避免在重写的过程中产生数据不一致性的问题呢？答案就是：写时复制（copy on write（COW）），主进程有写操作发生时，需要将内存中的数据复制一份再进行修改，而不是在原来的数据上进行修改，主进程虚拟地址指向复制后的物理地址；子进程还是指向原来的物理地址，aof重写操作完成后k1的值是2而不是3，与预期结果一致。\n\n3.RDB3.1 aof持久化存在的问题由于aof日志文件中记录的是操作命令，在进行数据恢复时，需要将日志文件中的命令逐条执行，一旦日志文件过大，就会导致redis数据恢复时间变长。那么有没有一种方式既可以实现内存数据持久化，又能快速恢复内存数据的解决方案呢？\nrdb(redis database)：存放的是内存中的数据而非执行的命令，因此在满足内存数据持久化的同时又达到能快速恢复数据的目的。\n3.2 rdb流程同aof日志文件重写一样，执行rdb操作的时候主进程也会fork出一个子进程，用于将内存的数据写入到文件中。此时你可能会有疑问，如果子进程在持久过的过程中，内存中的数据发生变化了怎么办？\n事实上，子进程持久化的是内存中的快照数据，那么什么是快照数据呢？\n快照数据就是某一时刻内存中的数据，子进程在持久化的过程中，如果有写入操作，会通过cow(copy on wirte)技术来保证快照数据不被修改。\n3.3 rdb写入频率\n根据配置文件说明，使用者可以自定义rdb写入频率，但是设置多少合适呢？频率太高，会造成磁盘写入压力过大，上次一次写入任务还未执行完成下一次写入任务又开启了，从而进入恶性循环；频率太低，会造成数据丢失过多的问题发生，因此写入频率的设置是一个不太好衡量的操作。\n3.4 aof + rdb既然rdb写入频率不太好去衡量，那么我们可以结合aof来解决rbd周期之间数据丢失的问题。使用rdb + aof结合方式既可以满足内存数据的快速恢复，又可以实现内存数据的持久化。\n\n","slug":"一文读懂Redis持久化机制","date":"2022-12-18T07:32:00.000Z","categories_index":"redis","tags_index":"redis,aof,rdb","author_index":"黑白搬砖工"},{"id":"a74d032dc9bf8ca9d012adc342b4fd84","title":"动态伸缩你的服务","content":"1.前言如你所知，服务的常规部署方式如下：\n\n对外暴露的服务都会在前面部署nginx用于提供反向代理和负载均衡能力\n下面会快速部署一套类似的服务，分析其存在的问题并给出相应解决方案\n2.应用相关2.1 启动服务使用boot-cloud-openfeign-provider启动3个服务实例，端口分别为8081、8082、8083\n2.2 服务验证在浏览器中分别输入：http://localhost:8081/index/nginx、http://localhost:8082/index/nginx、http://localhost:8083/index/nginx，保证3个服务实例均可被访问\n\n\n\n\n\n\n\n\n\n服务运行依赖5.1和5.2中的consul\n3.nginx相关3.1 安装3.1.1 搜索镜像docker search nginx\n\n3.1.2 拉取镜像docker pull nginx\n\n3.1.3 运行镜像docker run --name nginx -p 80:80 -d nginx\n\n3.1.4 拷贝镜像配置文件docker cp nginx:/etc/nginx/nginx.conf path/nginx[宿主机放配置文件路径]\n\n3.1.5 删除容器docker rm -f nginx\n\n3.1.6 指定配置文件运行镜像docker run --name nginx -p 80:80 -v [宿主机路径]/nginx/nginx.conf:/etc/nginx/nginx.conf:ro -v [宿主机路径]/nginx/conf.d:/etc/nginx/conf.d -d nginx\n\n3.2 配置3.2.1 配置负载均衡在[宿主机路径]/nginx/conf.d目录下创建load-balancer.conf文件，内容如下：\nupstream backend &#123;\n    server 10.100.40.243:8081;\n    server 10.100.40.243:8082;\n    server 10.100.40.243:8083;\n&#125;\n\nserver&#123;\n    listen 80;\n\n    location / &#123;\n        proxy_pass http://backend;\n    &#125;\n&#125;\n\n3.3 验证3.3.1 重启nginx服务docker exec -it nginx nginx -s reload\n\n3.3.2 访问nginx服务在浏览器中输入：http://localhost/index/nginx，多次请求依次可以看到`openfeign service：hello nginx from 8081、openfeign service：hello nginx from 8082、openfeign service：hello nginx from 8083&#96;结果，则说明nginx负载均衡配置正常。\n4.问题如你所见，服务的负载均衡能力是通过在load-balancer.conf配置文件中写死服务列表来实现的，同时也意味着，只要后端服务列表发生变化，就需要修改配置并通过nginx -s reload命令来重新加载。\n5.方案有了问题，既然会有对应的解决方案，本文要介绍的解决方案就是consul和consul template\n5.1 安装consul根据Install Consul官网教程安装consul\n5.2 运行consulconsul agent -dev\n\n5.3 安装consul template5.3.1 下载压缩包wget https://releases.hashicorp.com/consul-template/0.20.0/consul-template_0.20.0_linux_amd64.zip\n\n5.3.2 解压unzip consul-template_0.20.0_linux_amd64.zip\n\n\n\n\n\n\n\n\n\n\nMac os可以通过brew install consul-template进行安装，简单、方便\n5.4 创建负载均衡配置文件模板创建load-balancer.ctmpl文件，编辑如下内容：\nupstream backend &#123;\n    &#123;&#123;- range service \"openfeign-provider-service\" &#125;&#125;\n        server &#123;&#123; .Address &#125;&#125;:&#123;&#123; .Port &#125;&#125;;\n    &#123;&#123;- end &#125;&#125;\n&#125;\n\nserver &#123;\n   listen 80;\n\n   location / &#123;\n      proxy_pass http://backend;\n   &#125;\n&#125;\n\n5.5 清空配置文件清空3.2.1章节load-balancer.conf文件内容\n5.6 创建consul template配置创建consul-template-config.hcl文件，编辑如下内容：\nconsul &#123;\n  address = \"localhost:8500\"\n\n  retry &#123;\n    enabled  = true\n    attempts = 12\n    backoff  = \"250ms\"\n  &#125;\n&#125;\ntemplate &#123;\n  source      = \"[5.4章节指定的路径]/load-balancer.conf.ctmpl\"\n  destination = \"[5.5章节指定的路径]/load-balancer.conf\"\n  perms       = 0600\n  command     = \"sh -c docker exec -it nginx nginx -s reload\"\n&#125;\n\n\n\n\n\n\n\n\n\n\n\naddress：指定consul地址；source：用于指定负载均衡配置模板文件路径；destination：负载均衡配置生成文件路径；command：用于指定要执行的命令。整体流程：consul template从address地址拉取服务地址列表，根据source模板文件生成负载均衡配置到destination文件中，执行command重新加载nginx\n5.7 运行consul templateconsul-template -config=consul-template-config.hcl\n\n\n\n\n\n\n\n\n\n\n运行consul template之后，会发现load-balancer.conf文件中有了负载均衡配置\n5.8 服务关闭、启动到这里，你会发现伴随着服务关闭、服务启动，load-balancer.conf配置文件中的内容也在一直跟着改变，从而实现服务动态伸缩。\n","slug":"动态伸缩你的服务","date":"2022-12-10T13:57:00.000Z","categories_index":"微服务","tags_index":"consul,nginx,consul-template","author_index":"黑白搬砖工"},{"id":"6c37772ad206371cddc6299a58095fb8","title":"带你看懂rabbitmq集群","content":"1. 如何实现rabbitmq高可用服务高可用实现准则服务尽可能少中断、数据尽可能少丢，因此为了达到这一目标，可以通过集群部署来实现\n2. rabbitmq集群搭建完成，队列数据是否就高可用了？2.1 官方描述\n\n\n\n\n\n\n\n\nBy default, contents of a queue within a RabbitMQ cluster are located on a single node (the node on which the queue was declared). This is in contrast to exchanges and bindings, which can always be considered to be on all nodes. Queues can optionally run mirrors (additional replicas) on other cluster nodes.\n2.2 翻译\n\n\n\n\n\n\n\n\n默认情况下，队列中的数据只在创建队列的节点上存在，并不是集群中所有节点都存在。如果希望队列数据在集群中所有节点都存在，需要配置镜像队列\n3. 什么是镜像队列镜像队列就是队列在集群其它节点存在一个或多个镜像副本\n\n4.集群中leader和mirrors是否都可以处理消息？4.1 官方描述\n\n\n\n\n\n\n\n\nAll operations for a given queue are first applied on the queue’s leader node and then propagated to mirrors. This involves enqueueing publishes, delivering messages to consumers, tracking acknowledgements from consumers and so on.\n4.2 翻译\n\n\n\n\n\n\n\n\n队列的所有操作都在leader节点上完成，然后将操作同步给镜像节点，也就是说镜像节点只做数据备份，因此可以得出镜像队列并不能提高rabbitmq性能结论。\n5.如何配置镜像队列？5.1 官方描述\n\n\n\n\n\n\n\n\nTo make the classic queues mirrored, create a policy which matches them and sets policy keys ha-mode and (optionally) ha-params.\n5.2 翻译使用ha-mode和ha-params创建策略并应用到所有队列\n\n\n\n\n\n\n\n\n\n\nha-mode和ha-params其它参数值配置可参考Queue Arguments that Control Mirroring\n6.如何检测镜像队列配置成功？6.1 官方描述\n\n\n\n\n\n\n\n\nMirrored queues will have a policy name and the number of additional replicas (mirrors) next to it on the queue page in the management UI.\n6.2 翻译队列界面Node列可以看到镜像节点数量，Features列可以看到使用策略名称，则说明镜像队列配置成功\n\n镜像队列配置成功，队列详情界面会列举出镜像节点\n\n7.如何选择镜像节点数量在创建镜像策略的时候，是不是应该把所有节点都作为镜像节点？来看看官方说明\n7.1 官方描述\n\n\n\n\n\n\n\n\nMirroring to all nodes is the most conservative option. It will put additional strain on all cluster nodes, including network I&#x2F;O, disk I&#x2F;O and disk space usage. Having a replica on every node is unnecessary in most cases.\nFor clusters of 3 and more nodes it is recommended to replicate to a quorum (the majority) of nodes, e.g. 2 nodes in a 3 node cluster or 3 nodes in a 5 node cluster.\nSince some data can be inherently transient or very time sensitive, it can be perfectly reasonable to use a lower number of mirrors for some queues (or even not use any mirroring).\n7.2 翻译通过官方描述可以了解到：镜像到所有节点是最保守的选择，这将会给集群中所有节点带来网络I/O、磁盘I/O、磁盘空间使用压力\n官方建议：镜像节点数量推荐集群节点半数以上，比如3个节点集群，镜像节点数量推荐2；比如5个节点集群，镜像节点数量推荐3\n当然并不是所有场景都对数据可靠性有要求，针对可靠性要求不高的场景甚至可以选择不设置镜像队列\n8. 如何平衡集群节点负载从第四章节可以了解到镜像队列中只有leader可以处理消息，这同时也就带来一个问题，如果所有的leader都在一个节点中，那么该节点就将承载所有的压力，有没有一种可以平衡集群节点负载的方法？\n8.1 官方描述\n\n\n\n\n\n\n\n\nEvery queue in RabbitMQ has a primary replica. That replica is called queue leader (originally “queue master”). All queue operations go through the leader replica first and then are replicated to followers (mirrors). This is necessary to guarantee FIFO ordering of messages.\nTo avoid some nodes in a cluster hosting the majority of queue leader replicas and thus handling most of the load, queue leaders should be reasonably evenly distributed across cluster nodes.\nQueue leaders can be distributed between nodes using several strategies. Which strategy is used is controlled in three ways, namely, using the x-queue-master-locator optional queue argument, setting the queue-master-locator policy key or by defining the queue_master_locator key in the configuration file. Here are the possible strategies and how to set them:\n\nPick the node hosting the minimum number of leaders: min-masters\nPick the node the client that declares the queue is connected to: client-local\nPick a random node: random\n\n8.2 翻译\n\n\n\n\n\n\n\n\n添加名称：queue-master-locator，值：min-masters策略，可以将新增的队列添加到leader数最少的节点上，从而达到平衡集群负载效果\n9. 镜像策略失效9.1 新增平衡集群节点负载策略\n9.2 新增队列\n\n\n\n\n\n\n\n\n\n新增完平衡集群节点负载策略后，会发现队列没有镜像功能了，既不显示镜像节点数量也不显示镜像策略名称，为什么会出现这种现象呢？难道镜像策略和平衡集群节点负载策略只能使用一个，没法同时使用？\n9.3 官方描述\n\n\n\n\n\n\n\n\nAt most one policy matches a queue or exchange. Since multiple policies can match a single name, a mechanism is needed to resolve such policy conflicts. This mechanism is called policy priorities. Every policy has a a numeric priority associated with it. This priority can be specified when declaring a policy. If not explicitly provided, the priority of 1 will be used.\nMatching policies are then sorted by priority and the one with the highest priority will take effect.\n9.4 翻译\n\n\n\n\n\n\n\n\n队列至多只能使用一种策略，当队列匹配到多种策略时，最终只会匹配优先级最高的策略。\n根据官方描述得知队列只能使用一种策略，要么使用镜像策略要么使用平衡节点负载策略，无法同时使用两种策略。\n如果相同时拥有这两种功能，似乎是件不可能实现的事。\n9.5 一种策略多种定义9.5.1 官方描述\n\n\n\n\n\n\n\n\nIn some cases we might want to apply more than one policy definition to a resource. For example we might need a queue to be federated and has message TTL. At most one policy will apply to a resource at any given time, but we can apply multiple definitions in that policy.\n9.5.2 翻译\n\n\n\n\n\n\n\n\n队列虽然无法同时使用两种策略，但是可以在一种策略中应用多个definitions\n9.5.3 定义多个definitions\n9.5.4 新增队列查看效果\n\n\n\n\n\n\n\n\n\n新增队列后可以看到队列既具有镜像功能并且还均匀分布在集群的各个节点上\n10.集群disc和ram节点区别\n搭建完rabbitmq集群后，可以看到集群节点既有disc类型又有RAM类型，是不是说disc类型节点支持将数据持久化到硬盘，RAM&#96;类型节点不支持将数据持久化到硬盘，只支持数据持久化到内存的话就会存在数据丢失的问题\n10.1 官方描述\n\n\n\n\n\n\n\n\nA node can be a disk node or a RAM node. (Note: disk and disc are used interchangeably). RAM nodes store internal database tables in RAM only. This does not include messages, message store indices, queue indices and other node state.\nIn the vast majority of cases you want all your nodes to be disk nodes; RAM nodes are a special case that can be used to improve the performance clusters with high queue, exchange, or binding churn. RAM nodes do not provide higher message rates. When in doubt, use disk nodes only.\nSince RAM nodes store internal database tables in RAM only, they must sync them from a peer node on startup. This means that a cluster must contain at least one disk node. It is therefore not possible to manually remove the last remaining disk node in a cluster.\n10.2 翻译RAN类型节点只是将内部数据库表存储在内存中，消息并不在内存中存储，因此不用担心消息在内存中存在丢失的情况发生\n11. 生产者11.1 生产者如何保证消息可靠性11.1.1 官方描述\n\n\n\n\n\n\n\n\n In order to guarantee persistence, a client should use confirms. If the publisher’s channel had been in confirm mode, the publisher would not have received an ack for the lost message (since the message hadn’t been written to disk yet).\n11.1.2 翻译\n\n\n\n\n\n\n\n\n为了保证消息持久化，客户端需要使用confirms机制。如果生产者channel使用了confirm模式，消息丢失情况下生产者不会收到ack通知\n11.2 broker发送ack时机11.2.1 官方描述\n\n\n\n\n\n\n\n\nFor routable messages, the basic.ack is sent when a message has been accepted by all the queues. For persistent messages routed to durable queues, this means persisting to disk. For quorum queues, this means that a quorum replicas have accepted and confirmed the message to the elected leader.\n11.2.2 翻译\n\n\n\n\n\n\n\n\n针对路由到持久化队列的消息在所有队列将消息持久化到磁盘后broker会向生产者发送basic.ack\n11.2.3 ack延迟11.2.3.1 官方描述\n\n\n\n\n\n\n\n\nbasic.ack for a persistent message routed to a durable queue will be sent after persisting the message to disk. The RabbitMQ message store persists messages to disk in batches after an interval (a few hundred milliseconds) to minimise the number of fsync(2) calls, or when a queue is idle.\nThis means that under a constant load, latency for basic.ack can reach a few hundred milliseconds. To improve throughput, applications are strongly advised to process acknowledgements asynchronously (as a stream) or publish batches of messages and wait for outstanding confirms. The exact API for this varies between client libraries.\n11.2.3.2 翻译\n\n\n\n\n\n\n\n\nrabbitmq采用周期性的方式对消息进行持久化，因此应用程序接收basic.ack会存在几百毫秒延迟\n12.参考文献Classic Queue Mirroring\nDisk and RAM Nodes\nPublisher Confirms\n","slug":"带你看懂rabbitmq集群","date":"2022-12-11T05:52:18.000Z","categories_index":"消息中间件","tags_index":"rabbit","author_index":"黑白搬砖工"},{"id":"4d529d29bdaac9a514904201dfd2325c","title":"带你看懂kafka","content":"1.前言kafka服务端存在相当多的术语，只有了解这些术语具体的含义，我们才能对kafka有一个粗略的认识。本文将带着你去了解以及理清相关术语，让你不再对其感到陌生以及恐惧。\n2.集群搭建工欲善其事必先利其器，在正式开始前，我们需要搭建一个kafka集群\n集群搭建选择使用docker-compose方式，使用该方式的好处就是：在docker-compose.yml文件编写完成的情况下，只需要一条命令即可实现集群的运行与停止\n2.1 编写docker-compose.ymlversion: \"3\"\nservices:\n  zookeeper:\n    image: 'zookeeper:latest'\n    ports:\n      - '2181:2181'\n    environment:\n      - ALLOW_ANONYMOUS_LOGIN=yes\n  kafka0:\n    image: 'bitnami/kafka:latest'\n    ports:\n      - '9092:9092'\n    environment:\n      - KAFKA_BROKER_ID=0\n      - KAFKA_CFG_LISTENERS=PLAINTEXT://0.0.0.0:9092\n      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://192.168.0.101:9092\n      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181\n      - ALLOW_PLAINTEXT_LISTENER=yes\n    volumes:\n      - 宿主机日志目录:/bitnami/kafka\n    depends_on:\n      - zookeeper\n  kafka1:\n    image: 'bitnami/kafka:latest'\n    ports:\n      - '9093:9093'\n    environment:\n      - KAFKA_BROKER_ID=1\n      - KAFKA_CFG_LISTENERS=PLAINTEXT://0.0.0.0:9093\n      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://192.168.0.101:9093\n      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181\n      - ALLOW_PLAINTEXT_LISTENER=yes\n    volumes:\n      - 宿主机日志目录:/bitnami/kafka\n    depends_on:\n      - zookeeper\n  kafka2:\n    image: 'bitnami/kafka:latest'\n    ports:\n      - '9094:9094'\n    environment:\n      - KAFKA_BROKER_ID=2\n      - KAFKA_CFG_LISTENERS=PLAINTEXT://0.0.0.0:9094\n      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://192.168.0.101:9094\n      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181\n      - ALLOW_PLAINTEXT_LISTENER=yes\n    volumes:\n      - 宿主机日志目录:/bitnami/kafka\n    depends_on:\n      - zookeeper\n\n\n\n\n\n\n\n\n\n\n注意：如果希望通过宿主机连接kafka集群，需要添加KAFKA_CFG_LISTENERS和KAFKA_CFG_ADVERTISED_LISTENERS配置并且KAFKA_CFG_ADVERTISED_LISTENERS配置地址为宿主机对应ip\nKAFKA_CFG_ADVERTISED_LISTENERS为暴露给客户端的监听地址\n2.2 运行集群在编写完docker-compose.yml文件后，只需下面的一条命令即可完成集群的运行\ndocker-compose up -d\n\n集群运行正常，你将看到如下内容输出\nCreating network \"kafka_default\" with the default driver\nCreating kafka_zookeeper_1 ... done\nCreating kafka_kafka0_1    ... done\nCreating kafka_kafka2_1    ... done\nCreating kafka_kafka1_1    ... done\n\n2.3 停止集群如果你想停止集群，那么同样可以使用一条命令来实现\ndocker-compose down\n\n3.概念3.1 主题主题(topic)作为一个逻辑概念，在系统中主要用于区分业务场景，比如可以为下单场景创建一个topic：create-order，为加入购物车场景创建一个topic：add-cart\n3.1.1 创建主题bin/kafka-topics.sh --create --topic create-order --bootstrap-server 127.0.0.1:9092 --partitions 3 --replication-factor 3\n\n\n\n\n\n\n\n\n\n\n–partitions：用于指定主题对应分区数量\n–replication-factor：用于指定分区对应副本数量\n3.1.2 查看主题信息bin/kafka-topics.sh --describe --topic create-order --bootstrap-server localhost:9092\n\n\n\n\n\n\n\n\n\n\n\n通过如上命令可以查看主题详细信息，包括：分区数量、分区leader、分区副本数、AR以及ISR\n3.2 分区在3.1创建主题部分我们通过--partitions参数指定了主题对应分区数\n分区作为一个物理概念，分散在各个broker中，可以提升并发能力以及服务性能\n\n3.3 副本副本的存在是为了提升分区数据可靠性，副本分为leader副本和flower副本，leader副本负责处理客户端的读写请求，flower副本只负责从leader副本同步数据\n\n3.4 ARAR(Assigned Replicas)：分区所有副本集合\ntopic：create-order，partition0对应AR：1，2，0\ntopic：create-order，partition1对应AR：0，1，2\ntopic：create-order，partition2对应AR：2，0，1\n3.5 ISRISR(In-Sync Replicas)：保持正常同步的副本集合(包含leader副本)\ntopic：create-order，partition0对应ISR：1，2，0\ntopic：create-order，partition1对应ISR：0，1，2\ntopic：create-order，partition2对应ISR：2，0，1\n3.6 OSROSR(Out-Of-Sync Replicas)：与leader副本同步滞后过多的flower副本集合\n\n\n\n\n\n\n\n\n\n当OSR为空时，AR &#x3D; ISR，否则，AR &#x3D; ISR + OSR\n3.7 LE0LEO(Log End Offset)：分区待写入消息对应偏移量\n\n3.8 HWHW(High Water)：分区所有副本都会维护自己的LEO，副本中最小的LEO即为HW\n\n\n\n\n\n\n\n\n\n\n注意：消费者只能拉取到HW之前的消息进行消费\n3.9 ControllerController：集群协调器，负责\n\n创建、删除主题，增加分区并分配leader分区；\n集群Broker管理（新增 Broker、Broker 主动关闭、Broker 故障)\npreferred leader选举\n分区重分配\n\n从Controller作用可以看到其重要性，那么Controller是如何选举出来的呢？\n在集群启动的过程中，节点会在zookeeper中创建controller节点，谁先创建成功谁就是Controller\n\n\n\n\n\n\n\n\n\n\n通过重启集群，可以看到controller节点下brokerid对应的值也在变化\n\n3.10 leader选举\ntopic：create-order，partition：0在未停掉brokerid:2的情况下，分区leader分配在brokerid为2的机器上\ntopic：create-order，partition：0在停掉brokerid:2的情况下，分区leader分配在brokerid为1的机器上\n\n\ntopic：create-order，partition：1在未停掉brokerid:0的情况下，分区leader分配在brokerid为0的机器上\ntopic：create-order，partition：1在停掉brokerid:0的情况下，分区leader分配在brokerid为2的机器上\n\n\n\n\n\n\n\n\n\n由此可见，分区的leader的选举首选要保证在Isr列表中，按照Replicas列表的顺序选举\n4.zookeeper中存储的信息\n","slug":"带你看懂kafka","date":"2022-12-11T05:51:06.000Z","categories_index":"消息中间件","tags_index":"kafka","author_index":"黑白搬砖工"},{"id":"9f635fa5cf662529fd1247abdd3830ea","title":"kafka存储","content":"1.前言系统仅仅拥有高并发、高可用往往是不够的，在此基础上还需要具备高可靠性。那么什么是高可靠性呢？高可靠性就是系统在宕机恢复后数据不丢失，仍然可以保证业务的正常运行。\n作为业务系统的开发人员，提到数据持久化，基于本能反应首选想到的一定是数据库。kafka作为消息中间件，会选择什么方式来进行数据的持久化呢？答案是：日志文件\n2.日志文件内容既然知道kafka采用日志文件的方式来对数据进行持久化，想你一定会好奇，日志文件中到底存储的是什么内容？\n2.1 如何找到日志文件位置想看日志文件内容，首先你得知道日志文件路径，通过config目录下server.properties配置文件中的log.dirs=配置项可以确定日志文件存放路径\n确定配置文件路径后，切换到对应目录，依据topic + partition来确定目标目录，比如想查看topic 为product，0号分区对应的日志文件，需要找到product-0 目录即可\n进入目标目录，查看目录下的文件信息\n-rw-r--r--  1   1137481367        24  9  3 16:02 00000000000000000000.index\n-rw-r--r--  1   1137481367     16375  9  3 16:02 00000000000000000000.log\n-rw-r--r--  1   1137481367        48  9  3 16:02 00000000000000000000.timeindex\n-rw-r--r--  1   1137481367        24  9  3 16:02 00000000000000002186.index\n-rw-r--r--  1   1137481367     16224  9  3 16:02 00000000000000002186.log\n-rw-r--r--  1   1137481367        48  9  3 16:02 00000000000000002186.timeindex\n-rw-r--r--  1   1137481367        24  9  3 16:02 00000000000000004415.index\n-rw-r--r--  1   1137481367     16275  9  3 16:02 00000000000000004415.log\n-rw-r--r--  1   1137481367        48  9  3 16:02 00000000000000004415.timeindex\n-rw-r--r--  1   1137481367        24  9  3 16:02 00000000000000006647.index\n-rw-r--r--  1   1137481367     16278  9  3 16:02 00000000000000006647.log\n-rw-r--r--  1   1137481367        48  9  3 16:02 00000000000000006647.timeindex\n-rw-r--r--  1   1137481367  10485760  9  3 16:02 00000000000000008879.index\n-rw-r--r--  1   1137481367      8594  9  3 16:02 00000000000000008879.log\n-rw-r--r--  1   1137481367  10485756  9  3 16:02 00000000000000008879.timeindex\n\n通过文件信息展示可看出kafka日志中会有三种文件，分别以.index、.log、.timeindex结尾，从名字上可以看出.index、.timeindex应该与索引相关,.log文件才是真正存储数据的地方\n2.2 查看日志文件内容明确存储数据文件后，你就可以通过vi命令打开.log文件查看具体内容，不出意外，映入眼帘的是满屏看不懂、不明所以的玩意\n\n看不懂，那一定是打开的方式不对，千万不要怀疑是自己能力的问题，你可以尝试使用如下方式打开文件\nbin&#x2F;kafka-run-class.sh kafka.tools.DumpLogSegments --deep-iteration --print-data-log --files &#x2F;xxxxx&#x2F;kafka&#x2F;kafka_2.12-3.2.1&#x2F;logs&#x2F;product-0&#x2F;00000000000000000000.log\n\n使用正确方式打开文件内容\nbaseOffset: 0 lastOffset: 121 count: 122 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: false deleteHorizonMs: OptionalLong.empty position: 0 CreateTime: 1662192149996 size: 966 magic: 2 compresscodec: gzip crc: 3650920144 isvalid: true\n| offset: 0 CreateTime: 1662192149978 keySize: -1 valueSize: 88 sequence: -1 headerKeys: [__TypeId__] payload: \"&#123;\\\"producerId\\\":\\\"strimzi-canary-client\\\",\\\"messageId\\\":1,\\\"timestamp\\\":1662192149651&#125;\"\n...\n| offset: 121 CreateTime: 1662192149996 keySize: -1 valueSize: 90 sequence: -1 headerKeys: [__TypeId__] payload: \"&#123;\\\"producerId\\\":\\\"strimzi-canary-client\\\",\\\"messageId\\\":122,\\\"timestamp\\\":1662192149996&#125;\"\n\n\nbaseOffset: 122 lastOffset: 243 count: 122 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: false deleteHorizonMs: OptionalLong.empty position: 966 CreateTime: 1662192150002 size: 936 magic: 2 compresscodec: gzip crc: 635681227 isvalid: true\n| offset: 122 CreateTime: 1662192149996 keySize: -1 valueSize: 90 sequence: -1 headerKeys: [__TypeId__] payload: \"&#123;\\\"producerId\\\":\\\"strimzi-canary-client\\\",\\\"messageId\\\":123,\\\"timestamp\\\":1662192149996&#125;\"\n...\n| offset: 243 CreateTime: 1662192150002 keySize: -1 valueSize: 90 sequence: -1 headerKeys: [__TypeId__] payload: \"&#123;\\\"producerId\\\":\\\"strimzi-canary-client\\\",\\\"messageId\\\":244,\\\"timestamp\\\":1662192150002&#125;\"\n\n\nbaseOffset: 244 lastOffset: 365 count: 122 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: false deleteHorizonMs: OptionalLong.empty position: 1902 CreateTime: 1662192150007 size: 905 magic: 2 compresscodec: gzip crc: 2330978284 isvalid: true\n| offset: 244 CreateTime: 1662192150002 keySize: -1 valueSize: 90 sequence: -1 headerKeys: [__TypeId__] payload: \"&#123;\\\"producerId\\\":\\\"strimzi-canary-client\\\",\\\"messageId\\\":245,\\\"timestamp\\\":1662192150002&#125;\"\n...\n| offset: 365 CreateTime: 1662192150007 keySize: -1 valueSize: 90 sequence: -1 headerKeys: [__TypeId__] payload: \"&#123;\\\"producerId\\\":\\\"strimzi-canary-client\\\",\\\"messageId\\\":366,\\\"timestamp\\\":1662192150007&#125;\"\n\n通过内容展示方式可以看到日志文件中记录了每条消息对应的offset、创建时间以及消息内容\n再打开00000000000000002186.log查看，可以得出一个结论，那就是文件名对应了该文件中第一条消息的offset\n3.性能3.1 顺序写数据写入磁盘是一种比较耗时操作，以至于看到该类操作的第一反应就是慢，能避免就尽量避免，但是为了保证数据的可靠性，这种操作往往又是不可或缺的\n其实这里往往有一个误区，所有的IO操作都比较慢嘛？当然不是，顺序IO虽没有内存操作快，但是也不差\nkafka也是通过采用顺序写IO方式来实现数据写盘，从而提升写性能\n\n3.2 分而治之不知你是否有过类似的经历，打开一个数兆文件只需要几秒时间，但是打开一个数百兆或者更大文件却需要几分钟甚至更长时间\n由此可以想象一下，如果不断往.log文件中追加消息，那么随着时间的推移.log文件会变得越来越大，大到打开该文件需要花费分钟级别的耗时，这对高性能kafka是完全不能接受的一件事情，那么如何解决大文件带来的耗时问题？\n分而治之是一种很好的思想，我们可以将一个大文件拆分成由多个小文件组成，从而解决大文件带来的性能问题\n\n3.3 索引查询现在你已经知道kafka会将客户端推送过来的消息存放在.log文件中，那么kafka是如何检索指定offset的消息给消费者进行消费的呢？\n不知你是否会想到前文提到的.index文件，通过如下命令来看看文件中存储的内容\nbin/kafka-run-class.sh kafka.tools.DumpLogSegments --print-data-log --files /xxx/kafka/kafka_2.12-3.2.1/logs/product-0/00000000000000000000.index\n\noffset: 731 position: 4623\noffset: 1338 position: 9121\noffset: 1943 position: 13634\n\n该文件存储了offset与position映射关系，到此可以得出根据offset就能找到对应的position的结论\n看完.index文件后，再来看看主角.log文件\nbaseOffset: 0 lastOffset: 121 count: 122 position: 0 CreateTime: 1662192149996 size: 966 magic: 2 compresscodec: gzip crc: 3650920144 isvalid: true\n| offset: 0 CreateTime: 1662192149978 keySize: -1 valueSize: 88 sequence: -1 headerKeys: [__TypeId__] payload: \"&#123;\\\"producerId\\\":\\\"strimzi-canary-client\\\",\\\"messageId\\\":1,\\\"timestamp\\\":1662192149651&#125;\"\n......\n| offset: 121 CreateTime: 1662192149996 keySize: -1 valueSize: 90 sequence: -1 headerKeys: [__TypeId__] payload: \"&#123;\\\"producerId\\\":\\\"strimzi-canary-client\\\",\\\"messageId\\\":122,\\\"timestamp\\\":1662192149996&#125;\"\nbaseOffset: 122 lastOffset: 243 count: 122 position: 966 CreateTime: 1662192150002 size: 936 magic: 2 compresscodec: gzip crc: 635681227 isvalid: true\nbaseOffset: 244 lastOffset: 365 count: 122 position: 1902 CreateTime: 1662192150007 size: 905 magic: 2 compresscodec: gzip crc: 2330978284 isvalid: true\nbaseOffset: 366 lastOffset: 487 count: 122 position: 2807 CreateTime: 1662192150011 size: 903 magic: 2 compresscodec: gzip crc: 1412054061 isvalid: true\nbaseOffset: 488 lastOffset: 609 count: 122 position: 3710 CreateTime: 1662192150015 size: 913 magic: 2 compresscodec: gzip crc: 3677057634 isvalid: true\nbaseOffset: 610 lastOffset: 731 count: 122 position: 4623 CreateTime: 1662192150019 size: 903 magic: 2 compresscodec: gzip crc: 1937572371 isvalid: true\n| offset: 610 CreateTime: 1662192150015 keySize: -1 valueSize: 90 sequence: -1 headerKeys: [__TypeId__] payload: \"&#123;\\\"producerId\\\":\\\"strimzi-canary-client\\\",\\\"messageId\\\":611,\\\"timestamp\\\":1662192150015&#125;\"\n......\n| offset: 731 CreateTime: 1662192150019 keySize: -1 valueSize: 90 sequence: -1 headerKeys: [__TypeId__] payload: \"&#123;\\\"producerId\\\":\\\"strimzi-canary-client\\\",\\\"messageId\\\":732,\\\"timestamp\\\":1662192150019&#125;\"\n\nbaseOffset: 732 lastOffset: 853 count: 122 position: 5526 CreateTime: 1662192150022 size: 902 magic: 2 compresscodec: gzip crc: 677681648 isvalid: true\nbaseOffset: 854 lastOffset: 975 count: 122 position: 6428 CreateTime: 1662192150025 size: 880 magic: 2 compresscodec: gzip crc: 1374850199 isvalid: true\nbaseOffset: 976 lastOffset: 1096 count: 121 position: 7308 CreateTime: 1662192150027 size: 909 magic: 2 compresscodec: gzip crc: 2771471302 isvalid: true\nbaseOffset: 1097 lastOffset: 1217 count: 121 position: 8217 CreateTime: 1662192150030 size: 904 magic: 2 compresscodec: gzip crc: 438650712 isvalid: true\nbaseOffset: 1218 lastOffset: 1338 count: 121 position: 9121 CreateTime: 1662192150033 size: 898 magic: 2 compresscodec: gzip crc: 983033759 isvalid: true\n\nbaseOffset: 1339 lastOffset: 1459 count: 121 position: 10019 CreateTime: 1662192150035 size: 895 magic: 2 compresscodec: gzip crc: 964946264 isvalid: true\nbaseOffset: 1460 lastOffset: 1580 count: 121 position: 10914 CreateTime: 1662192150040 size: 941 magic: 2 compresscodec: gzip crc: 2189697033 isvalid: true\nbaseOffset: 1581 lastOffset: 1701 count: 121 position: 11855 CreateTime: 1662192150043 size: 903 magic: 2 compresscodec: gzip crc: 2220582279 isvalid: true\nbaseOffset: 1702 lastOffset: 1822 count: 121 position: 12758 CreateTime: 1662192150045 size: 876 magic: 2 compresscodec: gzip crc: 3714367387 isvalid: true\nbaseOffset: 1823 lastOffset: 1943 count: 121 position: 13634 CreateTime: 1662192150048 size: 901 magic: 2 compresscodec: gzip crc: 531545191 isvalid: true\n\nbaseOffset: 1944 lastOffset: 2064 count: 121 position: 14535 CreateTime: 1662192150050 size: 896 magic: 2 compresscodec: gzip crc: 3782783873 isvalid: true\nbaseOffset: 2065 lastOffset: 2185 count: 121 position: 15431 CreateTime: 1662192150055 size: 944 magic: 2 compresscodec: gzip crc: 198975981 isvalid: true\n\n根据.log文件内容，可以画出下图\n结论：\n1.根据指定的offset在.index文件中找到对应的position\n2.根据position在.log文件中找到目标位置，如果查找目标offset大于position对应的offset，那么向后查找，否则向前查找，直到找到目标offset对应消息为止\n4.总结在了解完kafka消息存放形式以及查找方式后，我们能从中学到哪些内容呢？\n1.针对大文件带来的读写问题，可以通过拆分文件的方式进行解决，类比elasticsearch以及redis分片，分而治之思想\n2.针对查询慢的问题，可以通过创建索引方式进行解决，类比mysql索引、书本目录\n","slug":"kafka存储","date":"2022-12-11T05:50:10.000Z","categories_index":"消息中间件","tags_index":"kafka","author_index":"黑白搬砖工"},{"id":"5ce0c663a19d8e8ee699635ec101be20","title":"带你看懂redis IO模型","content":"1.前言如你所知，高性能web服务器Nginx选择使用IO多路复用技术处理客户端请求，高性能内存数据库Redis同样也选择使用IO多路复用技术处理客户端请求，这足以说明IO多路复用是非常优秀的IO模型，毕竟人以类聚、物以群分。\n接下来会从传统阻塞IO模型说起，聊到传统非阻塞IO模型，再到IO多路复用\n2. IO模型2.1 传统阻塞IO模型\n传统阻塞IO模型中，当客户端与服务器端建立连接创建socket后，服务端需要从socket中读取数据，如果socket数据不可读，就会阻塞当前线程，又由于是单线程，就会导致服务端无法对外提供任何服务\n2.2 传统非阻塞IO模型通过对传统阻塞IO模型进行分析后，可得知问题的关键在单线程上，知道问题所在，对应解决方案也就有了，那就是针对每个client创建一个线程进行socket数据读取\n\n通过多线程解决了单线程阻塞导致服务不可用的问题，随之而来也带了一个新的问题，那就是线程过多会造成资源浪费(每个线程拥有1M栈空间)、上下文切换问题\n2.3 多路复用单线程和多线程都有对应的问题存在，那么还有其它更好的解决方案嘛？方案当然是有的，那就是IO多路复用。IO多路复用的实现有select()、poll()、epoll(),分别来看一下\n2.3.1 术语介绍文件描述符：客户端与服务端建立的socket连接称作一个文件描述符（File Descriptor 以下简称 FD）\n2.3.2 select()2.3.2.1 描述\n通过select()描述可以得知该函数允许应用程序同时监听多个fd，在fd可读可写之前会一直阻塞，同时还有一个很关键的点，那就是select()同时监听的文件描述符数量不能大于1024\n2.3.2.2 返回值\nselect()调用成功后会返回就绪fd个数\n2.3.2.3 流程应用程序调用select()函数将fd传给内核空间\n\n内核空间会对fd进行循环遍历，当有fd变得就绪后，应用程序的select()调用会返回就绪fd个数，此时应用程序再通过循环遍历方式读取就绪fd数据\n2.3.2.4 存在的问题\n内核不知道fd何时就绪，只能通过循环遍历的方式得知，会造成CPU资源浪费\n内核 只会返回就绪fd的个数，应用程序并不知道具体哪个fd是就绪状态，只能再次循环系统调用才可得知，会造成无效系统调用\n同时监听文件描述符数不能超过1024\n\n2.3.2 epoll2.3.3.1 描述\nepollAPI核心概念就是epoll instance，epoll instance是一个内核数据结构。从用户空间角度来看，epoll instance是一个包含进程注册的fd列表和就绪fd列表的容器\nepoll提供了3个系统调用用于创建和管理epoll实例\n\nepoll_create：创建一个epoll实例，并返回一个fd\nepoll_ctl：对fd进行增、删、改\nepoll_wait：阻塞等待IO事件\n\n2.3.3.2 流程\n先通过epoll_create创建一个epoll instance，再通过epoll_ctl往注册列表中添加fd并监听对应事件(比如读事件、写事件)，最后通过epoll_wait阻塞等待，直到就绪列表中有fd为止，期间如果某个fd就绪，会从注册列表中移动到就绪列表中，epoll_wait返回就绪fd个数\n通过流程可以看到：\n\n应用程序每次都是增量往注册列表中添加fd，而不像select那样每次都传所有fd\n\n内核空间通过事件驱动方式得知fd就绪，而不像select那样需要循环遍历\n\nepoll_wait返回的后，应用程序知道具体哪一个fd就绪，而不像select那样循环遍历所有fd才知道哪些处于就绪状态\n\n\n2.3.3.3 示例分析\n3. 查看redis的IO多路复用实现3.1 追踪redisstrace -ff -o ./redis.out redis-6.2.6/src/redis-server /opt/redis/redis-6.2.6/redis.conf\n\n3.2 查看追踪文件\nvi redis.out.5444\n\n\n可以看到redis通过epoll实现了IO多路复用\n","slug":"IO模型","date":"2022-12-11T05:49:28.000Z","categories_index":"redis","tags_index":"多路复用","author_index":"黑白搬砖工"},{"id":"f8c1d4f8bf32cb91acc7b2a551ae4194","title":"消息积压你作何处理？","content":"1.前言当我们使用@KafkaListener注解声明一个消费者时，该消费者就会轮询去拉取对应分区消息记录，消费消息记录，正如你所知道的那样，正常场景下会执行ack操作，提交offset到kafka服务器。但是异常场景下会如何执行，不知你是否也了解？在了解之前，先一起来看下异常处理器，看完之后想必会有所收获。\n2.异常处理器2.1 创建异常处理器在实例化org.springframework.kafka.listener.KafkaMessageListenerContainer.ListenerConsumer#ListenerConsumer的时候如果没有自定义异常处理器，会去创建SeekToCurrentErrorHandler作为默认异常处理器使用\nprotected ErrorHandler determineErrorHandler(GenericErrorHandler&lt;?> errHandler) &#123;\n    return errHandler != null ? (ErrorHandler) errHandler\n            : this.transactionManager != null ? null : new SeekToCurrentErrorHandler();\n&#125;\n\n2.2 声明补偿策略在构建默认异常处理器SeekToCurrentErrorHandler时会指定对应的补偿策略\n/**\n  * Construct an instance with the default recoverer which simply logs the record after\n  * &#123;@value SeekUtils#DEFAULT_MAX_FAILURES&#125; (maxFailures) have occurred for a\n  * topic/partition/offset, with the default back off (9 retries, no delay).\n  * @since 2.2\n  */\npublic SeekToCurrentErrorHandler() &#123;\n    this(null, SeekUtils.DEFAULT_BACK_OFF);\n&#125;\n\n\n\n\n\n\n\n\n\n\n从代码注释上我们可以了解到该补偿策略会进行9次无时间间隔重试\n2.3 调用异常处理器@Nullable\nprivate RuntimeException doInvokeRecordListener(final ConsumerRecord&lt;K, V> record, // NOSONAR\n        Iterator&lt;ConsumerRecord&lt;K, V>> iterator) &#123;\n\n    Object sample = startMicrometerSample();\n\n    try &#123;\n        // 1.消费消息\n        invokeOnMessage(record);\n        successTimer(sample);\n        recordInterceptAfter(record, null);\n    &#125; catch (RuntimeException e) &#123;\n        try &#123;\n            // 2.执行异常处理器\n            invokeErrorHandler(record, iterator, e);\n            // 3.提交offset\n            commitOffsetsIfNeeded(record);\n        &#125; catch (KafkaException ke) &#123;\n           \n        &#125;\n    &#125;\n    return null;\n&#125;\n\n\n\n\n\n\n\n\n\n\n这里可以看到当消息消费异常后，会调用默认异常处理器SeekToCurrentErrorHandler\n2.4 执行异常处理器public static boolean doSeeks(List&lt;ConsumerRecord&lt;?, ?>> records, Consumer&lt;?, ?> consumer, Exception exception,\n\t\t\tboolean recoverable, RecoveryStrategy recovery, @Nullable MessageListenerContainer container,\n\t\t\tLogAccessor logger) &#123;\n\n    Map&lt;TopicPartition, Long> partitions = new LinkedHashMap&lt;>();\n    AtomicBoolean first = new AtomicBoolean(true);\n    AtomicBoolean skipped = new AtomicBoolean();\n    records.forEach(record -> &#123;\n        if (recoverable &amp;&amp; first.get()) &#123;\n            try &#123;\n                // 1.判断该消息是否可重试\n                boolean test = recovery.recovered(record, exception, container, consumer);\n                skipped.set(test);\n            &#125;\n            catch (Exception ex) &#123;\n            &#125;\n        &#125;\n        if (!recoverable || !first.get() || !skipped.get()) &#123;\n            partitions.computeIfAbsent(new TopicPartition(record.topic(), record.partition()),\n                    offset -> record.offset());\n        &#125;\n        first.set(false);\n    &#125;);\n    // 2.重置分区偏移量，以便可以重复拉取异常消息\n    seekPartitions(consumer, partitions, logger);\n    return skipped.get();\n&#125;\n\n\n\n\n\n\n\n\n\n\n异常处理器执行过程：\n判断当前消息是否可重试\n如果当前消息可以重试，会将该消息对应offset存储在partitions中，紧接着通过seekPartitions方法来将当前分区offset重置为当前消息offset，以至在下一次拉取消息的时候，仍然可以拉取到该异常消息。\n如果当前消息不可以重试，判断此次拉取的消息是否只有一条，如果是，不做处理；如果不是，则通过partitions.computeIfAbsent方法设置分区offset为异常消息下一条消息对应offset，以至在下一次拉取的时候可以拉取到异常消息后的其它消息。\n2.5 提交偏移量@Nullable\nprivate RuntimeException doInvokeRecordListener(final ConsumerRecord&lt;K, V> record, // NOSONAR\n        Iterator&lt;ConsumerRecord&lt;K, V>> iterator) &#123;\n\n    Object sample = startMicrometerSample();\n\n    try &#123;\n        invokeOnMessage(record);\n    &#125;\n    catch (RuntimeException e) &#123;\n        try &#123;\n            invokeErrorHandler(record, iterator, e);\n            // 提交分区offset\n            commitOffsetsIfNeeded(record);\n        &#125; catch (KafkaException ke) &#123;\n        &#125;\n    &#125;\n    return null;\n&#125;\n\n\n\n\n\n\n\n\n\n\n当默认异常处理器重试达到最大次数9次后，会执行commitOffsetsIfNeeded方法，手动提交分区offset，从而可以避免再次消费到异常消息\n2.6 总结当消费消息异常，在没有声明异常处理器的前提下会选择使用默认异常处理器SeekToCurrentErrorHandler，默认异常处理器会对异常消息进行重试，在达到最大重试次数9次后，会手动提交异常消息offset，然后继续消费异常消息之后的其它消息。\n至此想必对消息消费异常有了一个大致认识，如有疑问，欢迎留言讨论。\n","slug":"kafka消费异常会如何处理？","date":"2022-12-11T05:48:32.000Z","categories_index":"消息中间件","tags_index":"kafka","author_index":"黑白搬砖工"},{"id":"f8c1d4f8bf32cb91acc7b2a551ae4194","title":"消息积压你作何处理？","content":"1.前言当被通知消费的队列存在消息积压并呈一个持续上升趋势，需要紧急处理一下，你会怎么办？能怎么办呢？先内心慌张一会，战斗一番，然后打开著名的搜索引擎进行搜索，在搜索结果中寻找答案。本文也即将成为你搜索引擎中出现的一个搜索结果，为你提供解决方案。\n2.解决方案以下解决方案都是针对rabbitmq\n2.1 加机器最简单最方便的处理方案就是让运维加机器，这样消息队列中的消息可以通过负载的方式将消息均摊到更多的机器上，消费者多了，会慢慢处理掉队列中积压的消息。\n2.2 加消费者当然加机器方案并不是万能的，如果你的系统不够格，是加不了加机器(成本过高)，那么就得另辟蹊径了，增加并发消费线程数。\n2.2.1 固定并发数\n&#96;&#96;&#96;java\n&#x2F;**\n\t * Set the concurrency of the listener container for this listener. Overrides the\n\t * default set by the listener container factory. Maps to the concurrency setting of\n\t * the container type.\n\t * &lt;p&gt;For a\n\t * &#123;@link org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer\n\t * SimpleMessageListenerContainer&#125; if this value is a simple integer, it sets a fixed\n\t * number of consumers in the &#123;@code concurrentConsumers&#125; property. If it is a string\n\t * with the form &#123;@code &quot;m-n&quot;&#125;, the &#123;@code concurrentConsumers&#125; is set to &#123;@code m&#125;\n\t * and the &#123;@code maxConcurrentConsumers&#125; is set to &#123;@code n&#125;.\n\t * &lt;p&gt;For a\n\t * &#123;@link org.springframework.amqp.rabbit.listener.DirectMessageListenerContainer\n\t * DirectMessageListenerContainer&#125; it sets the &#123;@code consumersPerQueue&#125; property.\n\t * @return the concurrency.\n\t * @since 2.0\n\t *&#x2F;\n\n\n\n\n\n\n\n\n\n\n翻译过来就是为监听器设置并发数，如果属性值是一个int类型，那么设置的就是固定并发数；如果属性值是一个string类型(m-n)，那么设置的就是动态并发数，最小并发数为m，最大并发数为n\n基于文档注释，可以得出一个结论，那就是如果想要设置并发，那么只需要在@RabbitListener注解中添加concurrency属性并指定一个数值即可\n@RabbitListener(queues = \"test3\", containerFactory = \"rabbitListenerContainerFactory\", concurrency = \"10\")\n\n2.2.2 动态并发数设置消费者并发数后消息消费快了，也不积压了，问题被完美解决。由于存在动态并发数这一章节，说明事情并没有这么简单。设置消费者并发数的初衷是想解决消息积压问题，但是消息并非时时刻刻都会积压，大部分情况下队列中可能都不存在可以消费的消息，此时消费者并发数的设置无疑会给系统带来一定的资源消耗。好在spring框架相关大佬已经给出了解决方案:动态并发数\n动态并发数设置形式：\n@RabbitListener(queues = \"test3\", containerFactory = \"rabbitListenerContainerFactory\", concurrency = \"1-10\")\n\n\n\n\n\n\n\n\n\n\nconcurrency设置为1-10则表示消费者并发数最小为1个，当有更多消息需要处理时，会逐渐增大消费者并发数，最大值为10，当没有消息需要处理是，会逐渐减少消费者并发数，减到1为止\n看完上面的描述，你可能认为我是在胡扯，且等看看如下代码实现再下结论\nprivate void checkAdjust(boolean receivedOk) &#123;\n    if (receivedOk) &#123;\n        // 1.当前消费者处于激活状态\n        if (isActive(this.consumer)) &#123;\n            this.consecutiveIdles = 0;\n            // 1.1 如果连续消息数超过默认值10，则考虑新增一个消费者\n            if (this.consecutiveMessages++ > SimpleMessageListenerContainer.this.consecutiveActiveTrigger) &#123;\n                considerAddingAConsumer();\n                this.consecutiveMessages = 0;\n            &#125;\n        &#125;\n    &#125;\n    else &#123;\n        this.consecutiveMessages = 0;\n        // 2.如果连续超过10次都没有消费到消息，则考虑终止当前消费者\n        if (this.consecutiveIdles++ > SimpleMessageListenerContainer.this.consecutiveIdleTrigger) &#123;\n            considerStoppingAConsumer(this.consumer);\n            this.consecutiveIdles = 0;\n        &#125;\n    &#125;\n&#125;\n\n3. 总结消息队列中出现消息积压，首先不要慌，其实是能加机器就加机器，不能加机器，说明你的系统不太重要，就得老老实实增加消费者并发数，增加消费者并发数基于上述认知，你应该设置动态并发数\n","slug":"消息积压你作何处理？","date":"2022-12-11T05:48:32.000Z","categories_index":"消息中间件","tags_index":"消息积压","author_index":"黑白搬砖工"},{"id":"9553bf6744373965b077add4719345f2","title":"探秘kafka消费者流程","content":"1.前言为什么会想着去探秘kafka消费者流程呢？在回答这个问题之前，先带你看两个示例，看过之后想必你也就知道其中的原因了\n2.示例2.1 示例一@KafkaListener(topics = \"product\", groupId = \"product1\")\npublic void consumeMessage1(ConsumerRecord&lt;String, String> consumerRecord, Acknowledgment ack) &#123;\n    log.info(\"receive message topic：&#123;&#125;, partition：&#123;&#125;，offset：&#123;&#125;，message：&#123;&#125;\", consumerRecord.topic(),\n            consumerRecord.partition(), consumerRecord.offset(), consumerRecord.value());\n    if (\"test\".equals(consumerRecord.value())) &#123;\n        throw new IllegalArgumentException(\"message content is empty\");\n    &#125;\n    ack.acknowledge();\n&#125;\n\n\n\n\n\n\n\n\n\n\n示例很简单，就是正常消费消息，在业务处理完成之后执行ack操作进行消息确认。也就是说如果不执行ack操作，那么这条消息应该可以一直消费，直到处理成功为止。事实上真的是这样嘛？\n2.2 测试方案通过kafka客户端发送如下消息：\naa\nbb\ntest\ncc\ndd\nee\n\n应用控制台会输出如下内容：\nreceive message topic：product, partition：0，offset：5，message：aa\nreceive message topic：product, partition：0，offset：6，message：bb\nreceive message topic：product, partition：0，offset：7，message：test\nreceive message topic：product, partition：0，offset：7，message：test\nreceive message topic：product, partition：0，offset：7，message：test\nreceive message topic：product, partition：0，offset：7，message：test\nreceive message topic：product, partition：0，offset：7，message：test\nreceive message topic：product, partition：0，offset：7，message：test\nreceive message topic：product, partition：0，offset：7，message：test\nreceive message topic：product, partition：0，offset：7，message：test\nreceive message topic：product, partition：0，offset：7，message：test\nreceive message topic：product, partition：0，offset：7，message：test\nreceive message topic：product, partition：0，offset：8，message：cc\nreceive message topic：product, partition：0，offset：9，message：dd\nreceive message topic：product, partition：0，offset：10，message：ee\n\n\n\n\n\n\n\n\n\n\n从输出内容来看，test这条消息在执行10后就丢失了，因为当执行到ee这条消息的时候会进行ack操作，一旦ack成功，下一次就会从ee这条消息对应offset  + 1位置开始拉取消息，如果不重置offset，永远无法再消费到test这条消息。想一想如果这种丢消息的情况发生在核心交易链路会造成什么影响？还有一点与认知不同的是：”消息没有被ack为什么会丢？”\n2.3 示例二@KafkaListener(topics = \"product\", groupId = \"product2\")\npublic void consumeMessage2(ConsumerRecord&lt;String, String> consumerRecord, Acknowledgment ack) &#123;\n    log.info(\"receive message topic：&#123;&#125;, partition：&#123;&#125;，offset：&#123;&#125;，message：&#123;&#125;\", consumerRecord.topic(),\n            consumerRecord.partition(), consumerRecord.offset(), consumerRecord.value());\n    try &#123;\n        if (\"test\".equals(consumerRecord.value())) &#123;\n            throw new IllegalArgumentException(\"message content is empty\");\n        &#125;\n        ack.acknowledge();\n    &#125; catch (Exception e) &#123;\n        log.error(\"consume message error：\", e);\n    &#125;\n&#125;\n\n\n\n\n\n\n\n\n\n\n示例2与示例1的区别就是多了异常捕获，接下来通过测试方案来看结果，可能也会超出你的认知。\n2.4 测试方案通过kafka客户端发送如下消息：\ntest\n\n\n\n\n\n\n\n\n\n\n应用程序控制台会输入异常和error日志，由于没有执行ack操作，理论上消息可以再次被拉取，事实上真是如此嘛？此时，只需要重启应用程序即可验证，在重启程序之后，可以看到消息再次被消费，这与我们的认知是一样的，即消息没有ack，就可以再次被消费。\n接下来再做一次测试，通过kafka客户端发送如下消息：\naa\ntest\nbb\n\n\n\n\n\n\n\n\n\n\n在揭晓之前，你可以先猜一猜最终的结果会如何呈现？\n重启应用后可以看到控制台输入如下内容：\nreceive message topic：product, partition：0，offset：76，message：aa\nreceive message topic：product, partition：0，offset：77，message：test\nconsume message error：\nreceive message topic：product, partition：0，offset：78，message：bb\n\n\n\n\n\n\n\n\n\n\n为了验证test这条消息没有被ack的消息能否再次被消费到，只需要重启应用即可。重启应用后想必你多少会有点迷茫，test这条消息最终并没有并消费，这条消息丢了，出现了和示例一同样诡异的现象：”未ack的消息丢了”\n通过对这两个示例的测试，我有了如下的疑惑：\n\n为什么在不捕获异常的情况下会对消息进行重试，在捕获异常的情况下不会对消息进行重试？\n为什么在捕获异常的情况下，异常消息之后不发送其它消息，异常消息在应用重启后可以继续被消费，发送其它消息后异常消息却丢了？\n为什么未ack的消息也会丢？\n\n看到这些疑惑，想必你也就知道为什么我会去探秘kafka消费者流程了，只有了解其中的原理，才能解开心中的这些疑惑。\n3.探秘kafka消费者流程3.1 寻找消费者组对应的Coordinator\n消费者与kafka交互，都是与消费者组对应的Coordinator进行通信，因此消费者首要任务就是寻找自己所在组对应的Coordinator\n对应源码可查看org.apache.kafka.clients.consumer.internals.ConsumerCoordinator#poll(org.apache.kafka.common.utils.Timer, boolean)中的ensureCoordinatorReady，该方法会向kafka中的一个broker发送请求，请求的响应结果会返回对应Coordinator的host、port、节点id\nprivate class FindCoordinatorResponseHandler extends RequestFutureAdapter&lt;ClientResponse, Void> &#123;\n    @Override\n    public void onSuccess(ClientResponse resp, RequestFuture&lt;Void> future) &#123;\n        log.debug(\"Received FindCoordinator response &#123;&#125;\", resp);\n        FindCoordinatorResponse findCoordinatorResponse = (FindCoordinatorResponse) resp.responseBody();\n        Errors error = findCoordinatorResponse.error();\n        if (error == Errors.NONE) &#123;\n            synchronized (AbstractCoordinator.this) &#123;\n                int coordinatorConnectionId = Integer.MAX_VALUE - findCoordinatorResponse.data().nodeId();\n                // 设置Coordinator\n                AbstractCoordinator.this.coordinator = new Node(\n                        coordinatorConnectionId,\n                        findCoordinatorResponse.data().host(),\n                        findCoordinatorResponse.data().port());\n                log.info(\"Discovered group coordinator &#123;&#125;\", coordinator);\n                client.tryConnect(coordinator);\n                heartbeat.resetSessionTimeout();\n            &#125;\n            future.complete(null);\n        &#125;\n    &#125;\n&#125;\n\n3.2 加入消费者组\n在寻找到Coordinator后，消费者会向对应的Coordinator发送加入消费者组请求\n对应源码可查看org.apache.kafka.clients.consumer.internals.ConsumerCoordinator#poll(org.apache.kafka.common.utils.Timer, boolean)中的ensureActiveGroup，该方法会向Coordinator发送加入消费者组请求\nRequestFuture&lt;ByteBuffer> sendJoinGroupRequest() &#123;\n    if (coordinatorUnknown())\n        return RequestFuture.coordinatorNotAvailable();\n    // send a join group request to the coordinator\n    log.info(\"(Re-)joining group\");\n    JoinGroupRequest.Builder requestBuilder = new JoinGroupRequest.Builder(\n            new JoinGroupRequestData()\n                    .setGroupId(rebalanceConfig.groupId)\n                    .setSessionTimeoutMs(this.rebalanceConfig.sessionTimeoutMs)\n                    .setMemberId(this.generation.memberId)\n                    .setGroupInstanceId(this.rebalanceConfig.groupInstanceId.orElse(null))\n                    .setProtocolType(protocolType())\n                    .setProtocols(metadata())\n                    .setRebalanceTimeoutMs(this.rebalanceConfig.rebalanceTimeoutMs)\n    );\n    log.debug(\"Sending JoinGroup (&#123;&#125;) to coordinator &#123;&#125;\", requestBuilder, this.coordinator);\n    int joinGroupTimeoutMs = Math.max(client.defaultRequestTimeoutMs(),\n        rebalanceConfig.rebalanceTimeoutMs + JOIN_GROUP_TIMEOUT_LAPSE);\n    // 消费者发送加入消费者组请求，请求响应处理可查看JoinGroupResponseHandler\n    return client.send(coordinator, requestBuilder, joinGroupTimeoutMs)\n            .compose(new JoinGroupResponseHandler(generation));\n&#125;\n\n3.3 Coordinator响应加入消费者请求\nCoordinator在收到消费者加入消费者组请求后，会从同一个消费者组中选择一个消费者作为leader，其余消费者作为flower\nprivate class JoinGroupResponseHandler extends CoordinatorResponseHandler&lt;JoinGroupResponse, ByteBuffer> &#123;\n    @Override\n    public void handle(JoinGroupResponse joinResponse, RequestFuture&lt;ByteBuffer> future) &#123;\n        Errors error = joinResponse.error();\n        if (error == Errors.NONE) &#123;\n            if (isProtocolTypeInconsistent(joinResponse.data().protocolType())) &#123;\n            &#125; else &#123;\n                synchronized (AbstractCoordinator.this) &#123;\n                    if (state != MemberState.PREPARING_REBALANCE) &#123;\n                    &#125; else &#123;\n                        // 如果当前消费者被选为leader\n                        if (joinResponse.isLeader()) &#123;\n                            onJoinLeader(joinResponse).chain(future);                          \n                        &#125; else &#123;\n                            onJoinFollower().chain(future);\n                        &#125;\n                    &#125;\n                &#125;\n            &#125;\n        &#125;\n    &#125;\n&#125;\n\n3.4 消费者leader制定分区分配方案如果某一消费者被Coordinator选为leader后，那么就需要负责制定该分组分区分配方案\n对应源码可查看org.apache.kafka.clients.consumer.internals.AbstractCoordinator#onJoinLeader中的performAssignment，该方法先寻找对应的分区分配器，根据分配器来给消费者组中的消费者分配分区\n@Override\nprotected Map&lt;String, ByteBuffer> performAssignment(String leaderId,\n                                                    String assignmentStrategy,\n                                                    List&lt;JoinGroupResponseData.JoinGroupResponseMember> allSubscriptions) &#123;\n    // 1.寻找分区分配器\n    ConsumerPartitionAssignor assignor = lookupAssignor(assignmentStrategy);\n    Set&lt;String> allSubscribedTopics = new HashSet&lt;>();\n    Map&lt;String, Subscription> subscriptions = new HashMap&lt;>();\n\n    // collect all the owned partitions\n    Map&lt;String, List&lt;TopicPartition>> ownedPartitions = new HashMap&lt;>();\n\n    for (JoinGroupResponseData.JoinGroupResponseMember memberSubscription : allSubscriptions) &#123;\n        Subscription subscription = ConsumerProtocol.deserializeSubscription(ByteBuffer.wrap(memberSubscription.metadata()));\n        subscription.setGroupInstanceId(Optional.ofNullable(memberSubscription.groupInstanceId()));\n        subscriptions.put(memberSubscription.memberId(), subscription);\n        allSubscribedTopics.addAll(subscription.topics());\n        ownedPartitions.put(memberSubscription.memberId(), subscription.ownedPartitions());\n    &#125;\n\n    // the leader will begin watching for changes to any of the topics the group is interested in,\n    // which ensures that all metadata changes will eventually be seen\n    updateGroupSubscription(allSubscribedTopics);\n\n    isLeader = true;\n\n    log.debug(\"Performing assignment using strategy &#123;&#125; with subscriptions &#123;&#125;\", assignor.name(), subscriptions);\n    // 2.根据分区分配器制定消费者组分区分配方案\n    Map&lt;String, Assignment> assignments = assignor.assign(metadata.fetch(), new GroupSubscription(subscriptions)).groupAssignment();\n\n    log.info(\"Finished assignment for group at generation &#123;&#125;: &#123;&#125;\", generation().generationId, assignments);\n\n    Map&lt;String, ByteBuffer> groupAssignment = new HashMap&lt;>();\n    for (Map.Entry&lt;String, Assignment> assignmentEntry : assignments.entrySet()) &#123;\n        ByteBuffer buffer = ConsumerProtocol.serializeAssignment(assignmentEntry.getValue());\n        groupAssignment.put(assignmentEntry.getKey(), buffer);\n    &#125;\n\n    return groupAssignment;\n&#125;\n\n3.5 发送同步组请求\n消费者在收到加入消费者组的响应后，如果被选为leader，那么该消费者负责制定分区分配方案，其它消费者不需要，之后所有消费者再次向Coordinator发送同步组请求\n3.6 Coordinator通知分区分配方案\nCoordinator在收到消费者leader制定的分区分配方案后，会将该方案通知到各个消费者，告诉每个消费者应该消费哪些分区\n对应源码可查看org.apache.kafka.clients.consumer.internals.AbstractCoordinator#joinGroupIfNeeded中的onJoinComplete，该方法会设置消费分区信息\n@Override\nprotected void onJoinComplete(int generation,\n                                String memberId,\n                                String assignmentStrategy,\n                                ByteBuffer assignmentBuffer) &#123;\n    log.debug(\"Executing onJoinComplete with generation &#123;&#125; and memberId &#123;&#125;\", generation, memberId);\n    ConsumerPartitionAssignor assignor = lookupAssignor(assignmentStrategy);\n    // Give the assignor a chance to update internal state based on the received assignment\n    groupMetadata = new ConsumerGroupMetadata(rebalanceConfig.groupId, generation, memberId, rebalanceConfig.groupInstanceId);\n\n    Set&lt;TopicPartition> ownedPartitions = new HashSet&lt;>(subscriptions.assignedPartitions());\n    // 1.反序列化分区信息\n    Assignment assignment = ConsumerProtocol.deserializeAssignment(assignmentBuffer);\n    Set&lt;TopicPartition> assignedPartitions = new HashSet&lt;>(assignment.partitions());\n    // 2.设置分区信息\n    subscriptions.assignFromSubscribed(assignedPartitions);\n&#125;\n\n3.6.1 反序列化分区信息\n\n\n\n\n\n\n\n\n\n通过调试可以看到当前消费者应该消费topic&#x3D;product的0号分区\n3.6.2 设置分区信息\n\n\n\n\n\n\n\n\n\n在反序列化得到分区信息后，会将分区信息设置在SubscriptionState对象的assignment属性中，后续消费者拉取消息的时候会用到\n3.7 小结到此，我们对消费者流程有了一个大致的认识，稍微总结下，内容如下：\n\n寻找后续交互的Coordinator\n消费者请求加入消费者组，Coordinator从消费者中选择一个作为leader\nleader消费者制定分区分配方案并同步Coordinator\nCoordinator向消费者下发分区分配方案\n消费者反序列分区信息并在本地设置分区信息\n\n3.8 初始化分区偏移量\n消费者有了分区信息后就可以拉取该分区存储的消息记录，在拉取消息记录之前，必须要明确从什么位置开始拉取，因此需要初始化分区偏移量。\n对应源码查看org.apache.kafka.clients.consumer.KafkaConsumer#updateAssignmentMetadataIfNeeded(org.apache.kafka.common.utils.Timer, boolean)中的updateFetchPositions\nprivate boolean updateFetchPositions(final Timer timer) &#123;\n    // If any partitions have been truncated due to a leader change, we need to validate the offsets\n    fetcher.validateOffsetsIfNeeded();\n    // 1.如果设置过offset直接返回\n    cachedSubscriptionHashAllFetchPositions = subscriptions.hasAllFetchPositions();\n    if (cachedSubscriptionHashAllFetchPositions) return true;\n    // 2.向kafka询问分区对应offset，如果存在则设置分区offset\n    if (coordinator != null &amp;&amp; !coordinator.refreshCommittedOffsetsIfNeeded(timer)) return false;\n    // 3.kafka不存在分区对应offset，需要将分区状态设置为AWAIT_RESET\n    subscriptions.resetInitializingPositions();\n    // 4.根据分区offset重置策略进行offset重置\n    fetcher.resetOffsetsIfNeeded();\n    return true;\n&#125;\n\n\n\n\n\n\n\n\n\n\n从源码分析中可以了解到这里存在两种情况，一种是kafka服务器中存在分区对应偏移量，另一种是kafka服务器中不存在分区对应偏移量，这里以第一种情况进行分析\npublic boolean refreshCommittedOffsetsIfNeeded(Timer timer) &#123;\n    final Set&lt;TopicPartition> initializingPartitions = subscriptions.initializingPartitions();\n    // 1.向Coordinator发送请求获取当前分区offset\n    final Map&lt;TopicPartition, OffsetAndMetadata> offsets = fetchCommittedOffsets(initializingPartitions, timer);\n    // 2.kafka服务器不存在分区对应offset，直接返回\n    if (offsets == null) return false;\n\n    for (final Map.Entry&lt;TopicPartition, OffsetAndMetadata> entry : offsets.entrySet()) &#123;\n        final TopicPartition tp = entry.getKey();\n        final OffsetAndMetadata offsetAndMetadata = entry.getValue();\n        if (offsetAndMetadata != null) &#123;\n            // first update the epoch if necessary\n            entry.getValue().leaderEpoch().ifPresent(epoch -> this.metadata.updateLastSeenEpochIfNewer(entry.getKey(), epoch));\n            // it's possible that the partition is no longer assigned when the response is received,\n            // so we need to ignore seeking if that's the case\n            if (this.subscriptions.isAssigned(tp)) &#123;\n                final ConsumerMetadata.LeaderAndEpoch leaderAndEpoch = metadata.currentLeader(tp);\n                final SubscriptionState.FetchPosition position = new SubscriptionState.FetchPosition(\n                        offsetAndMetadata.offset(), offsetAndMetadata.leaderEpoch(),\n                        leaderAndEpoch);\n                // 设置分区对应偏移量\n                this.subscriptions.seekUnvalidated(tp, position);\n                log.info(\"Setting offset for partition &#123;&#125; to the committed offset &#123;&#125;\", tp, position);\n            &#125; \n        &#125;\n    &#125;\n    return true;\n&#125;\n\n\n\n\n\n\n\n\n\n\n获取所有初始化状态分区列表，向Coordinator询问对应的offset，得到偏移量后进行本地初始化\n\n\n\n\n\n\n\n\n\n\n\n对比3.6.2中的截图，可以看到此处的position对象被赋值并且明确了分区offset，有了分区offset就可以拉取该分区对应的消息记录了\n3.9 拉取消息记录\n万事俱备，只欠东风。有了分区offset，只需要将topic、partition、offset告诉kafka服务器就可以获取到消息记录。\n3.9.1 发送拉取请求拉取消息分为两部分，第一步部分发送拉取请求，第二部分拉取消息记录，先来看看第一部分发送拉取请求\n对应源码查看org.apache.kafka.clients.consumer.KafkaConsumer#pollForFetches中的fetcher.sendFetches()\npublic synchronized int sendFetches() &#123;\n    // 1.准备请求参数\n    Map&lt;Node, FetchSessionHandler.FetchRequestData> fetchRequestMap = prepareFetchRequests();\n    for (Map.Entry&lt;Node, FetchSessionHandler.FetchRequestData> entry : fetchRequestMap.entrySet()) &#123;\n        final Node fetchTarget = entry.getKey();\n        final FetchSessionHandler.FetchRequestData data = entry.getValue();\n        // 2.构建请求对象\n        final FetchRequest.Builder request = FetchRequest.Builder\n                .forConsumer(this.maxWaitMs, this.minBytes, data.toSend())\n                .isolationLevel(isolationLevel)\n                .setMaxBytes(this.maxBytes)\n                .metadata(data.metadata())\n                .toForget(data.toForget())\n                .rackId(clientRackId);\n\n        if (log.isDebugEnabled()) &#123;\n            log.debug(\"Sending &#123;&#125; &#123;&#125; to broker &#123;&#125;\", isolationLevel, data.toString(), fetchTarget);\n        &#125;\n        // 3.发送拉取消息记录请求\n        RequestFuture&lt;ClientResponse> future = client.send(fetchTarget, request);\n        // We add the node to the set of nodes with pending fetch requests before adding the\n        // listener because the future may have been fulfilled on another thread (e.g. during a\n        // disconnection being handled by the heartbeat thread) which will mean the listener\n        // will be invoked synchronously.\n        this.nodesWithPendingFetchRequests.add(entry.getKey().id());\n        future.addListener(new RequestFutureListener&lt;ClientResponse>() &#123;\n            @Override\n            public void onSuccess(ClientResponse resp) &#123;\n                synchronized (Fetcher.this) &#123;\n                    try &#123;\n                        FetchResponse&lt;Records> response = (FetchResponse&lt;Records>) resp.responseBody();\n                        Set&lt;TopicPartition> partitions = new HashSet&lt;>(response.responseData().keySet());\n                        FetchResponseMetricAggregator metricAggregator = new FetchResponseMetricAggregator(sensors, partitions);\n                        for (Map.Entry&lt;TopicPartition, FetchResponse.PartitionData&lt;Records>> entry : response.responseData().entrySet()) &#123;\n                            TopicPartition partition = entry.getKey();\n                            FetchRequest.PartitionData requestData = data.sessionPartitions().get(partition);\n                            if (requestData == null) &#123;\n                               \n                            &#125; else &#123;\n                                long fetchOffset = requestData.fetchOffset;\n                                FetchResponse.PartitionData&lt;Records> partitionData = entry.getValue();\n\n                                log.debug(\"Fetch &#123;&#125; at offset &#123;&#125; for partition &#123;&#125; returned fetch data &#123;&#125;\",\n                                        isolationLevel, fetchOffset, partition, partitionData);\n\n                                Iterator&lt;? extends RecordBatch> batches = partitionData.records().batches().iterator();\n                                short responseVersion = resp.requestHeader().apiVersion();\n                                // 4.将消息记录放入本地队列中\n                                completedFetches.add(new CompletedFetch(partition, partitionData,\n                                        metricAggregator, batches, fetchOffset, responseVersion));\n                            &#125;\n                        &#125;\n                    &#125;\n                &#125;\n            &#125;\n    &#125;\n    return fetchRequestMap.size();\n&#125;\n\n\n\n\n\n\n\n\n\n\n通过分析可以得知，发送完拉取请求后会将拉取到的消息放入本地队列completedFetches中\n3.9.2 拉取消息记录对应源码查看org.apache.kafka.clients.consumer.internals.Fetcher.CompletedFetch#fetchRecords\nprivate List&lt;ConsumerRecord&lt;K, V>> fetchRecords(int maxRecords) &#123;\n    List&lt;ConsumerRecord&lt;K, V>> records = new ArrayList&lt;>();\n    try &#123;\n        for (int i = 0; i &lt; maxRecords; i++) &#123;\n            if (cachedRecordException == null) &#123;\n                corruptLastRecord = true;\n                lastRecord = nextFetchedRecord();\n                corruptLastRecord = false;\n            &#125;\n            if (lastRecord == null)\n                break;\n            // 1.解析消息记录并放入集合中\n            records.add(parseRecord(partition, currentBatch, lastRecord));\n            recordsRead++;\n            bytesRead += lastRecord.sizeInBytes();\n            // 2.nextFetchOffset = 最后一条消息记录offset + 1，比如最后一条消息offset = 81，那么nextFetchOffset=82\n            nextFetchOffset = lastRecord.offset() + 1;\n            // In some cases, the deserialization may have thrown an exception and the retry may succeed,\n            // we allow user to move forward in this case.\n            cachedRecordException = null;\n        &#125;\n    &#125;\n    return records;\n&#125;\n\n\n\n\n\n\n\n\n\n\n至此终于看到我们比较熟悉的内容ConsumerRecord&lt;K, V&gt;，也就是消费者方法参数中的ConsumerRecord&lt;String, String&gt; consumerRecord。在源码分析中特地强调了nextFetchOffset，那么它有什么用呢？一起来分析下如下代码，你就能明白了。\nprivate List&lt;ConsumerRecord&lt;K, V>> fetchRecords(CompletedFetch completedFetch, int maxRecords) &#123;\n    if (!subscriptions.isAssigned(completedFetch.partition)) &#123;\n    &#125; else if (!subscriptions.isFetchable(completedFetch.partition)) &#123;\n    &#125; else &#123;\n        FetchPosition position = subscriptions.position(completedFetch.partition);\n        if (completedFetch.nextFetchOffset == position.offset) &#123;\n            List&lt;ConsumerRecord&lt;K, V>> partRecords = completedFetch.fetchRecords(maxRecords);\n\n            log.trace(\"Returning &#123;&#125; fetched records at offset &#123;&#125; for assigned partition &#123;&#125;\",\n                    partRecords.size(), position, completedFetch.partition);\n            // 假设position.offset=81，拉取到的是offset=81位置对应的消息\n            // 通过控制台发送一条消息\n            // 从上面的分析可以知道completedFetch.nextFetchOffset = 最后一条消息offset + 1也就是82\n            if (completedFetch.nextFetchOffset > position.offset) &#123;\n                FetchPosition nextPosition = new FetchPosition(\n                        completedFetch.nextFetchOffset,\n                        completedFetch.lastEpoch,\n                        position.currentLeader);\n                log.trace(\"Update fetching position to &#123;&#125; for partition &#123;&#125;\", nextPosition, completedFetch.partition);\n                // 将分区偏移量设置为最后一条消息offset + 1，也就是82\n                subscriptions.position(completedFetch.partition, nextPosition);\n            &#125;\n            return partRecords;\n        &#125; else &#123;\n        &#125;\n    &#125;\n    return emptyList();\n&#125;\n\n\n\n\n\n\n\n\n\n\n\n通过调试也可以证明我们的结论是正确的。这里再次强调一下，每次拉取完消息后，消费者会将分区本地offset设置为最后一条消息对应offset + 1\n3.10 消费消息在拉取到消息后，就需要对消息记录进行消费\nprivate void doInvokeWithRecords(final ConsumerRecords&lt;K, V> records) &#123;\n    Iterator&lt;ConsumerRecord&lt;K, V>> iterator = records.iterator();\n    // 1.遍历消息记录\n    while (iterator.hasNext()) &#123;\n        if (this.stopImmediate &amp;&amp; !isRunning()) &#123;\n            break;\n        &#125;\n        final ConsumerRecord&lt;K, V> record = checkEarlyIntercept(iterator.next());\n        if (record == null) &#123;\n            continue;\n        &#125;\n        this.logger.trace(() -> \"Processing \" + ListenerUtils.recordToString(record));\n        // 2.调用监听器方法\n        doInvokeRecordListener(record, iterator);\n        if (this.nackSleep >= 0) &#123;\n            handleNack(records, record);\n            break;\n        &#125;\n    &#125;\n&#125;\n\n\n\n\n\n\n\n\n\n\n消费的过程也就是遍历消息记录，然后调用对应被@KafkaListener注解标注的方法\n4.解惑4.1 示例二解惑在解释疑惑之前，这里再贴一下对应内容，方便查看\n\n\n\n\n\n\n\n\n\n为什么在捕获异常的情况下，异常消息之后不发送其它消息，异常消息在应用重启后可以继续被消费，发送其它消息后异常消息却丢了？\n不知当你再次看到这个疑惑的时候心中是否已经有了答案，不管有没有答案，一起来分析下其中的缘由吧。\n假设消费者消费的最后一条消息对应的offset &#x3D; 81，那么下一次就应该拉取offset &#x3D; 82位置对应的消息。\n此时通过kafka客户端发送一条消息，消息内容为”test”，消费者就会拉取到该消息，紧接着修改本地分区offset &#x3D; 82 + 1 &#x3D; 83，消费该消息，业务报错，未执行ack操作，重启应用，消费者得到的分区offset仍然是82，所以可以继续消费test这条消息。\n此时通过kafka客户端再发送一条消息，消息内容为”bb”，由于在拉取”test”消息后会将本地分区offset修改为83，那么消费者就可以正常拉取到”bb”这条消息，成功消费，进而执行ack，应用重启，消费者得到的分区offset是84，所以”test”这条消息就丢了。\n\n\n\n\n\n\n\n\n\n一句话总结该疑惑的”罪魁祸首”就是在拉取完消息记录后，会将分区本地offset设置为最后一条消息对应offset + 1\n4.2 示例一解惑同样为了方便，这里再贴一下对应内容\n\n\n\n\n\n\n\n\n\n为什么在不捕获异常的情况下会对消息进行重试，重试9次之后消息丢失\n在解惑前先来看一下org.springframework.kafka.listener.KafkaMessageListenerContainer.ListenerConsumer中的一段代码\nprotected ErrorHandler determineErrorHandler(GenericErrorHandler&lt;?> errHandler) &#123;\n    return errHandler != null ? (ErrorHandler) errHandler\n            : this.transactionManager != null ? null : new SeekToCurrentErrorHandler();\n&#125;\n\n\n\n\n\n\n\n\n\n\n通过这段代码可以了解到在没有定义异常处理器的情况下会默认使用SeekToCurrentErrorHandler异常处理器\n既然知道会使用SeekToCurrentErrorHandler异常处理器，不妨点进去看看\n/**\n\t * Construct an instance with the default recoverer which simply logs the record after\n\t * &#123;@value SeekUtils#DEFAULT_MAX_FAILURES&#125; (maxFailures) have occurred for a\n\t * topic/partition/offset, with the default back off (9 retries, no delay).\n\t * @since 2.2\n\t */\n\tpublic SeekToCurrentErrorHandler() &#123;\n\t\tthis(null, SeekUtils.DEFAULT_BACK_OFF);\n\t&#125;\n\n\n\n\n\n\n\n\n\n\n通过注释我们可以了解到该异常处理器会进行无时间间隔的9次重试\n有了如上知识后，再来看下org.springframework.kafka.listener.KafkaMessageListenerContainer.ListenerConsumer#doInvokeRecordListener中的一段代码\nprivate RuntimeException doInvokeRecordListener(final ConsumerRecord&lt;K, V> record, // NOSONAR\n\t\t\t\tIterator&lt;ConsumerRecord&lt;K, V>> iterator) &#123;\n\n    Object sample = startMicrometerSample();\n\n    try &#123;\n        // 1.执行业务方法\n        invokeOnMessage(record);\n        successTimer(sample);\n        recordInterceptAfter(record, null);\n    &#125;\n    catch (RuntimeException e) &#123;\n        try &#123;\n            // 2.业务执行异常，执行默认异常处理器\n            invokeErrorHandler(record, iterator, e);\n            // 3.提交偏移量\n            commitOffsetsIfNeeded(record);\n        &#125;\n        catch (KafkaException ke) &#123;\n        &#125;\n        catch (RuntimeException ee) &#123;\n        &#125;\n        catch (Error er) &#123; // NOSONAR\n        &#125;\n    &#125;\n    return null;\n&#125;\n\n\n\n\n\n\n\n\n\n\n通过这段代码我们可以了解到，如果业务执行异常，底层会进行异常捕获并使用默认异常处理器SeekToCurrentErrorHandler进行9次重试，9次重试后还是失败，就会主动提交offset，因此异常消息会在9次重试后丢失。\n5. 源码入口org.apache.kafka.clients.consumer.KafkaConsumer#poll(org.apache.kafka.common.utils.Timer, boolean)\n其中org.apache.kafka.clients.consumer.KafkaConsumer#updateAssignmentMetadataIfNeeded(org.apache.kafka.common.utils.Timer, boolean)涉及3.1-3.6章节\norg.apache.kafka.clients.consumer.KafkaConsumer#updateFetchPositions涉及3.8章节\norg.apache.kafka.clients.consumer.KafkaConsumer#pollForFetches涉及3.9章节\n","slug":"探秘kafka消费者流程","date":"2022-12-11T05:47:00.000Z","categories_index":"消息中间件","tags_index":"kafka","author_index":"黑白搬砖工"},{"id":"a7366034e22a497a711893a0546ee23b","title":"系统重启你的应用会怎么样？","content":"1.前言近日就系统重启引发了一些思考，在系统重启过程中，正在进行的请求会如何被处理？正在消费的消息会不会丢失？异步执行的任务会不会被中断？既然存在这些问题，那我们的应用程序是不是就不能重启？但是，我们的应用程序随着版本迭代也在不断重启为什么这些问题没有出现呢？还是应用做了额外处理？带着这些疑问，结合场景模拟，看看实际情况怎么处理。\n2. 场景2.1 http请求2.1.1 创建请求@RestController\npublic class ShutDownController &#123;\n\n    @RequestMapping(\"shut/down\")\n    public String shutDown() throws InterruptedException &#123;\n        TimeUnit.SECONDS.sleep(20);\n        return \"hello\";\n    &#125;\n&#125;\n\n2.1.2 调用请求http://localhost:8080/shut/down\n2.1.3 模拟重启kill -2 应用pid\n\n2.1.4 现象\n2.1.5 结论请求执行过程中，关闭应用程序出现无法访问提示\n2.1.6 开启优雅关机如上出现的现象对用户来说很不友好，会造成用户一脸懵逼，那么有没有什么措施可以避免这种现象的出现呢？是否可以在应用关闭前执行完已经接受的请求，拒绝新的请求呢？答案可以的，只需要在配置文件中新增优雅关机配置\nserver:\n  shutdown: graceful # 设置优雅关闭，该功能在Spring Boot2.3版本中才有。注意：需要使用Kill -2 触发来关闭应用，该命令会触发shutdownHook\n\nspring:\n  lifecycle:\n    timeout-per-shutdown-phase: 30s # 设置缓冲时间，注意需要带上时间单位(该时间用于等待任务执行完成)\n\n添加完配置后，再次执行2.1.2和2.1.3流程，就会看到如下效果\n\n\n\n\n\n\n\n\n\n\n可以看到，即便在请求执行过程中关闭应用，已接收的请求依然会执行下去\n2.2 消息消费在前言提到过，消息消费过程中，关闭应用，消息是会丢失还是会被重新放入消息队列中呢？\n2.2.1 创建生产者@RestController\npublic class RabbitMqController &#123;\n\n    @Autowired\n    private RabbitTemplate rabbitTemplate;\n\n    @GetMapping(\"/sendBusinessMessage\")\n    public void sendBusinessMessage() throws InterruptedException &#123;\n        rabbitTemplate.convertAndSend(RabbitmqConfig.BUSINESS_EXCHANGE, RabbitmqConfig.BUSINESS_ROUTING_KEY, \"send message\");\n        TimeUnit.SECONDS.sleep(10000);\n    &#125;\n&#125;\n\n2.2.2 创建消费者@Component\n@RabbitListener(queues = RabbitmqConfig.BUSINESS_QUEUE_NAME)\n@Slf4j\npublic class BusinessConsumer &#123;\n\n    /**\n     * 操作场景：\n     * 1.通过RabbitmqApplication启动类启动应用程序\n     * 2.调用/sendBusinessMessage接口发送消息\n     * 3.RabbitMQ broker将消息发送给消费者\n     * 4.消费者收到消息后进行消费\n     * 5.消费者消费消息过程中，应用程序关闭，断开channel，断开connection，未ack的消息会被重新放入broker中\n     *\n     * @param content 消息内容\n     * @param channel channel通道\n     * @param message message对象\n     */\n    @RabbitHandler\n    public void helloConsumer(String content, Channel channel, Message message) &#123;\n        log.info(\"business consumer receive message：&#123;&#125;\", content);\n        try &#123;\n            // 模拟业务执行耗时\n            TimeUnit.SECONDS.sleep(10000);\n        &#125; catch (InterruptedException e) &#123;\n            e.printStackTrace();\n        &#125;\n    &#125;\n&#125;\n\n2.2.3 调用请求http://localhost:8080/sendBusinessMessage\n2.2.4 未关闭应用前\n2.2.5 关闭应用后\n2.2.6 结论消息消费过程中，关闭应用，未ack的消息会被重新放入消息队列中，以此来保证消息一定会被消费\n2.3 异步任务2.3.1 线程池配置@Component\npublic class ThreadPoolConfig &#123;\n\n    @Bean\n    public ThreadPoolTaskExecutor threadPoolTaskExecutor() &#123;\n        ThreadPoolTaskExecutor threadPoolTaskExecutor = new ThreadPoolTaskExecutor();\n        threadPoolTaskExecutor.setThreadNamePrefix(\"test-\");\n        threadPoolTaskExecutor.setCorePoolSize(3);\n        threadPoolTaskExecutor.setMaxPoolSize(3);\n        threadPoolTaskExecutor.setQueueCapacity(100);\n        return threadPoolTaskExecutor;\n    &#125;\n&#125;\n\n2.3.2 异步任务请求@Autowired\nprivate ThreadPoolTaskExecutor threadPoolTaskExecutor;\n\n@RequestMapping(\"async/task\")\npublic void asyncTask() throws InterruptedException &#123;\n  for (int i = 0; i &lt; 10; i++) &#123;\n    threadPoolTaskExecutor.execute(() -> &#123;\n      try &#123;\n        TimeUnit.SECONDS.sleep(10);\n      &#125; catch (InterruptedException e) &#123;\n        throw new RuntimeException();\n      &#125;\n      log.info(\"task execute complete...\");\n    &#125;);\n  &#125;\n&#125;\n\n2.3.3 调用请求http://localhost:8080/async/task\n2.3.4 模拟重启kill -2 应用pid\n\n2.3.5 现象Exception in thread \"test-2\" Exception in thread \"test-1\" Exception in thread \"test-3\" java.lang.RuntimeException\n\tat com.boot.example.ShutDownController.lambda$asyncTask$0(ShutDownController.java:37)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\njava.lang.RuntimeException\n\tat com.boot.example.ShutDownController.lambda$asyncTask$0(ShutDownController.java:37)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\njava.lang.RuntimeException\n\tat com.boot.example.ShutDownController.lambda$asyncTask$0(ShutDownController.java:37)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\n2.3.6 修改线程池配置在线程池配置中添加如下配置：\nthreadPoolTaskExecutor.setWaitForTasksToCompleteOnShutdown(true);\nthreadPoolTaskExecutor.setAwaitTerminationSeconds(120);\n\n2.3.7 修改配置后现象2021-12-09 17:09:40.054  INFO 22383 --- [         test-1] com.boot.example.ShutDownController      : task execute complete...\n2021-12-09 17:09:40.055  INFO 22383 --- [         test-3] com.boot.example.ShutDownController      : task execute complete...\n2021-12-09 17:09:40.055  INFO 22383 --- [         test-2] com.boot.example.ShutDownController      : task execute complete...\n2021-12-09 17:09:50.059  INFO 22383 --- [         test-3] com.boot.example.ShutDownController      : task execute complete...\n2021-12-09 17:09:50.059  INFO 22383 --- [         test-1] com.boot.example.ShutDownController      : task execute complete...\n2021-12-09 17:09:50.060  INFO 22383 --- [         test-2] com.boot.example.ShutDownController      : task execute complete...\n2021-12-09 17:10:00.062  INFO 22383 --- [         test-2] com.boot.example.ShutDownController      : task execute complete...\n2021-12-09 17:10:00.062  INFO 22383 --- [         test-1] com.boot.example.ShutDownController      : task execute complete...\n2021-12-09 17:10:00.065  INFO 22383 --- [         test-3] com.boot.example.ShutDownController      : task execute complete...\n2021-12-09 17:10:10.066  INFO 22383 --- [         test-1] com.boot.example.ShutDownController      : task execute complete...\n\n2.3.8 结论使用线程池执行异步任务，在没有添加配置的情况下，任务无法执行完成，在添加配置的情况下，任务依然可以执行完成。\n3. 总结为了保证在应用程序重启过程中任务仍然可以执行完成，需要开启优雅关机配置并对线程池添加等待任务执行完成以及等待时间配置\n","slug":"系统重启你的应用会怎么样？","date":"2022-12-10T13:58:00.000Z","categories_index":"微服务","tags_index":"优雅停机","author_index":"黑白搬砖工"},{"id":"24fad9e0b20684c1cf6b335f2dccd634","title":"三种你需要考虑的一致性问题的场景","content":"1.前言分布式系统开发过程中往往会涉及到很多需要保证数据一致性问题的场景，比如接收mq消息、接收http请求、内部业务处理。如果你还不了解这些场景或者不知道如何处理，请继续往下阅读。\n2.接收mq消息\n接收mq消息场景在分布式系统开发过程中想必是一种比较常见的场景，具体过程就是，外围系统推送mq消息到开发系统，开发系统接收消息后进行业务逻辑处理。从表面上看是一个十分简单的流程，但是如果涉及数据一致性问题，就不那么简单了。\n为什么说不那么简单呢？先来看看以下场景：\n2.1 先ack消息再处理业务ack();\n// 处理业务逻辑\n\n先ack消息再处理业务在理想场景下是不会有什么问题，看到这里你可能会有疑惑，没有问题那不就得了，还需要考虑什么呢？请注意这种没问题是建立在理想的前提下，如果业务处理过程中调用外部接口异常或者数据库宕机，就会导致消息丢失，进而出现数据不一致性的问题。\n2.2 先处理业务再ack消息// 处理业务逻辑\nack();\n\n既然先ack消息再处理业务这条路走不通，那么就先处理业务再ack消息总没问题了吧。是的，即便业务处理失败消息没有被ack，消息还会被重新消费，不会出现数据不一致性问题。但是这会涉及到另外一个问题，那就是幂等性问题，幂等问题处理不好，还是会引起数据不一致性问题。\n其实先处理业务再ack消息还会引起另一个问题，如果业务系统有bug会造成消息一直无法被ack，进而会导致消息处理进入死循环。\n这样也不行，那样也不行，就没有解决方案了？当然不是，方案还是有的，且听我慢慢跟你说\n2.3  结合消息ack机制 + 数据库 + 定时任务方案一try &#123;\n    try&#123;\n        // 根据消息唯一编号查询该消息是否已处理过，如果没有处理过，进行处理业务；如果处理过，则说明都不做\n    &#125; catch (Exception e) &#123;\n       // 将处理异常的消息插入数据库中\n    &#125;\n    ack();\n&#125; catch (Exception e) &#123;\n    unack();\n&#125;\n\n大体思路就是，根据消息唯一编号判断消息是否被处理过，如果未被处理过，就对消息进行处理，处理成功则对消息进行ack；处理失败则将消息存入数据库中。如果存入数据库这一步操作还是失败，那么就对消息进行unack操作，将消息重新投递到消息服务器中，进而重新消费，直到数据库恢复为止。针对处理失败入库的消息，可以通过定时任务重试处理。\n该方案不仅可以解决2.2中的幂等性问题，还可以解决业务出现bug进而导致消息处理进入死循环的问题(限制重试次数)。但是该方案还是会存在一个跟本文无关的问题，那就是消息积压问题。\n2.4 结合消息ack机制 + 数据库 + 定时任务方案二try &#123;\n    // 根据消息唯一编号查询该消息是否存在，不存在则直接插入数据库中，存在则不进行处理\n    ack();\n    // 异步处理业务逻辑\n&#125; catch (Exception e) &#123;\n    unack();\n&#125;\n\n2.4与2.3优缺点对比\n\n\n\n序号\n优点\n缺点\n\n\n\n2.3\n只在业务处理失败将消息插入数据库中，消息数量不会太多\n消息处理慢会导致消息积压\n\n\n2.4\n消息异步处理，不会导致消息积压\n所有消息都存储数据库，消息数量可能会很多\n\n\n关于这两种方案可以根据实际情况进行自由选择，消息积压问题处理也可以参考：消息积压你作何处理？\n3. 接收http请求\n看到这个图你可能会想这不就是一个很简单的流程嘛，开发系统接收请求、处理请求、响应结果就可以了。如你所想，确实很简单，但是如果你的开发系统业务处理失败，就会导致外围系统进行重试，直到重试次数用完，开发系统还未恢复正常，那么此次的外围请求数据就会丢失，从而引起数据不一致性问题。\n认真分析一下该场景，你会发现造成数据不一致性问题的关键在于开发系统的业务处理。如果开发系统能在正确接收外围系统请求后立刻进行响应，那么就可以解决该问题。\n我们只需要在接收http请求后，将请求内容写入数据库，写入成功进行异步处理并返回成功；写入失败返回失败，通过外围重试来保证数据可以正常写入数据库。处理失败的内容可以结合定时任务对请求进行重试。\n\n4.内部业务处理\n开发系统某些业务在处理成功后往往需要通知某些外围系统并且还不能因为外围系统故障从而导致当前业务无法正确处理，既然不能影响当前业务，可以采用异步的方式进行处理，异步处理就会存在当前业务处理成功，通知外围失败的问题，进而引起数据不一致性问题。\n那有没有办法可以保证业务处理成功的同时，对外围的通知也一定成功呢？\n我们可以采用本地事务的方案，把通知给外围的数据放在当前业务的事务中插入到数据库，异步通知外围系统，处理失败的数据再结合定时任务进行重试。\n\n5.总结从文中我们可以看到一致性问题的解决方案都逃不开数据库 + 重试，因此在解决一致性问题的时候可以多往这方面考虑。\n","slug":"三种你需要考虑一致性问题的场景","date":"2022-12-10T13:57:00.000Z","categories_index":"微服务","tags_index":"一致性","author_index":"黑白搬砖工"}]