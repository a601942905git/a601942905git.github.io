[{"id":"6c37772ad206371cddc6299a58095fb8","title":"带你看懂rabbitmq集群","content":"1. 如何实现rabbitmq高可用服务高可用实现准则服务尽可能少中断、数据尽可能少丢，因此为了达到这一目标，可以通过集群部署来实现\n2. rabbitmq集群搭建完成，队列数据是否就高可用了？2.1 官方描述\n\n\n\n\n\n\n\n\nBy default, contents of a queue within a RabbitMQ cluster are located on a single node (the node on which the queue was declared). This is in contrast to exchanges and bindings, which can always be considered to be on all nodes. Queues can optionally run mirrors (additional replicas) on other cluster nodes.\n2.2 翻译\n\n\n\n\n\n\n\n\n默认情况下，队列中的数据只在创建队列的节点上存在，并不是集群中所有节点都存在。如果希望队列数据在集群中所有节点都存在，需要配置镜像队列\n3. 什么是镜像队列镜像队列就是队列在集群其它节点存在一个或多个镜像副本\n\n4.集群中leader和mirrors是否都可以处理消息？4.1 官方描述\n\n\n\n\n\n\n\n\nAll operations for a given queue are first applied on the queue’s leader node and then propagated to mirrors. This involves enqueueing publishes, delivering messages to consumers, tracking acknowledgements from consumers and so on.\n4.2 翻译\n\n\n\n\n\n\n\n\n队列的所有操作都在leader节点上完成，然后将操作同步给镜像节点，也就是说镜像节点只做数据备份，因此可以得出镜像队列并不能提高rabbitmq性能结论。\n5.如何配置镜像队列？5.1 官方描述\n\n\n\n\n\n\n\n\nTo make the classic queues mirrored, create a policy which matches them and sets policy keys ha-mode and (optionally) ha-params.\n5.2 翻译使用ha-mode和ha-params创建策略并应用到所有队列\n\n\n\n\n\n\n\n\n\n\nha-mode和ha-params其它参数值配置可参考Queue Arguments that Control Mirroring\n6.如何检测镜像队列配置成功？6.1 官方描述\n\n\n\n\n\n\n\n\nMirrored queues will have a policy name and the number of additional replicas (mirrors) next to it on the queue page in the management UI.\n6.2 翻译队列界面Node列可以看到镜像节点数量，Features列可以看到使用策略名称，则说明镜像队列配置成功\n\n镜像队列配置成功，队列详情界面会列举出镜像节点\n\n7.如何选择镜像节点数量在创建镜像策略的时候，是不是应该把所有节点都作为镜像节点？来看看官方说明\n7.1 官方描述\n\n\n\n\n\n\n\n\nMirroring to all nodes is the most conservative option. It will put additional strain on all cluster nodes, including network I&#x2F;O, disk I&#x2F;O and disk space usage. Having a replica on every node is unnecessary in most cases.\nFor clusters of 3 and more nodes it is recommended to replicate to a quorum (the majority) of nodes, e.g. 2 nodes in a 3 node cluster or 3 nodes in a 5 node cluster.\nSince some data can be inherently transient or very time sensitive, it can be perfectly reasonable to use a lower number of mirrors for some queues (or even not use any mirroring).\n7.2 翻译通过官方描述可以了解到：镜像到所有节点是最保守的选择，这将会给集群中所有节点带来网络I/O、磁盘I/O、磁盘空间使用压力\n官方建议：镜像节点数量推荐集群节点半数以上，比如3个节点集群，镜像节点数量推荐2；比如5个节点集群，镜像节点数量推荐3\n当然并不是所有场景都对数据可靠性有要求，针对可靠性要求不高的场景甚至可以选择不设置镜像队列\n8. 如何平衡集群节点负载从第四章节可以了解到镜像队列中只有leader可以处理消息，这同时也就带来一个问题，如果所有的leader都在一个节点中，那么该节点就将承载所有的压力，有没有一种可以平衡集群节点负载的方法？\n8.1 官方描述\n\n\n\n\n\n\n\n\nEvery queue in RabbitMQ has a primary replica. That replica is called queue leader (originally “queue master”). All queue operations go through the leader replica first and then are replicated to followers (mirrors). This is necessary to guarantee FIFO ordering of messages.\nTo avoid some nodes in a cluster hosting the majority of queue leader replicas and thus handling most of the load, queue leaders should be reasonably evenly distributed across cluster nodes.\nQueue leaders can be distributed between nodes using several strategies. Which strategy is used is controlled in three ways, namely, using the x-queue-master-locator optional queue argument, setting the queue-master-locator policy key or by defining the queue_master_locator key in the configuration file. Here are the possible strategies and how to set them:\n\nPick the node hosting the minimum number of leaders: min-masters\nPick the node the client that declares the queue is connected to: client-local\nPick a random node: random\n\n8.2 翻译\n\n\n\n\n\n\n\n\n添加名称：queue-master-locator，值：min-masters策略，可以将新增的队列添加到leader数最少的节点上，从而达到平衡集群负载效果\n9. 镜像策略失效9.1 新增平衡集群节点负载策略\n9.2 新增队列\n\n\n\n\n\n\n\n\n\n新增完平衡集群节点负载策略后，会发现队列没有镜像功能了，既不显示镜像节点数量也不显示镜像策略名称，为什么会出现这种现象呢？难道镜像策略和平衡集群节点负载策略只能使用一个，没法同时使用？\n9.3 官方描述\n\n\n\n\n\n\n\n\nAt most one policy matches a queue or exchange. Since multiple policies can match a single name, a mechanism is needed to resolve such policy conflicts. This mechanism is called policy priorities. Every policy has a a numeric priority associated with it. This priority can be specified when declaring a policy. If not explicitly provided, the priority of 1 will be used.\nMatching policies are then sorted by priority and the one with the highest priority will take effect.\n9.4 翻译\n\n\n\n\n\n\n\n\n队列至多只能使用一种策略，当队列匹配到多种策略时，最终只会匹配优先级最高的策略。\n根据官方描述得知队列只能使用一种策略，要么使用镜像策略要么使用平衡节点负载策略，无法同时使用两种策略。\n如果相同时拥有这两种功能，似乎是件不可能实现的事。\n9.5 一种策略多种定义9.5.1 官方描述\n\n\n\n\n\n\n\n\nIn some cases we might want to apply more than one policy definition to a resource. For example we might need a queue to be federated and has message TTL. At most one policy will apply to a resource at any given time, but we can apply multiple definitions in that policy.\n9.5.2 翻译\n\n\n\n\n\n\n\n\n队列虽然无法同时使用两种策略，但是可以在一种策略中应用多个definitions\n9.5.3 定义多个definitions\n9.5.4 新增队列查看效果\n\n\n\n\n\n\n\n\n\n新增队列后可以看到队列既具有镜像功能并且还均匀分布在集群的各个节点上\n10.集群disc和ram节点区别\n搭建完rabbitmq集群后，可以看到集群节点既有disc类型又有RAM类型，是不是说disc类型节点支持将数据持久化到硬盘，RAM&#96;类型节点不支持将数据持久化到硬盘，只支持数据持久化到内存的话就会存在数据丢失的问题\n10.1 官方描述\n\n\n\n\n\n\n\n\nA node can be a disk node or a RAM node. (Note: disk and disc are used interchangeably). RAM nodes store internal database tables in RAM only. This does not include messages, message store indices, queue indices and other node state.\nIn the vast majority of cases you want all your nodes to be disk nodes; RAM nodes are a special case that can be used to improve the performance clusters with high queue, exchange, or binding churn. RAM nodes do not provide higher message rates. When in doubt, use disk nodes only.\nSince RAM nodes store internal database tables in RAM only, they must sync them from a peer node on startup. This means that a cluster must contain at least one disk node. It is therefore not possible to manually remove the last remaining disk node in a cluster.\n10.2 翻译RAN类型节点只是将内部数据库表存储在内存中，消息并不在内存中存储，因此不用担心消息在内存中存在丢失的情况发生\n11. 生产者11.1 生产者如何保证消息可靠性11.1.1 官方描述\n\n\n\n\n\n\n\n\n In order to guarantee persistence, a client should use confirms. If the publisher’s channel had been in confirm mode, the publisher would not have received an ack for the lost message (since the message hadn’t been written to disk yet).\n11.1.2 翻译\n\n\n\n\n\n\n\n\n为了保证消息持久化，客户端需要使用confirms机制。如果生产者channel使用了confirm模式，消息丢失情况下生产者不会收到ack通知\n11.2 broker发送ack时机11.2.1 官方描述\n\n\n\n\n\n\n\n\nFor routable messages, the basic.ack is sent when a message has been accepted by all the queues. For persistent messages routed to durable queues, this means persisting to disk. For quorum queues, this means that a quorum replicas have accepted and confirmed the message to the elected leader.\n11.2.2 翻译\n\n\n\n\n\n\n\n\n针对路由到持久化队列的消息在所有队列将消息持久化到磁盘后broker会向生产者发送basic.ack\n11.2.3 ack延迟11.2.3.1 官方描述\n\n\n\n\n\n\n\n\nbasic.ack for a persistent message routed to a durable queue will be sent after persisting the message to disk. The RabbitMQ message store persists messages to disk in batches after an interval (a few hundred milliseconds) to minimise the number of fsync(2) calls, or when a queue is idle.\nThis means that under a constant load, latency for basic.ack can reach a few hundred milliseconds. To improve throughput, applications are strongly advised to process acknowledgements asynchronously (as a stream) or publish batches of messages and wait for outstanding confirms. The exact API for this varies between client libraries.\n11.2.3.2 翻译\n\n\n\n\n\n\n\n\nrabbitmq采用周期性的方式对消息进行持久化，因此应用程序接收basic.ack会存在几百毫秒延迟\n12.参考文献Classic Queue Mirroring\nDisk and RAM Nodes\nPublisher Confirms\n","slug":"带你看懂rabbitmq集群","date":"2022-12-11T05:52:18.000Z","categories_index":"消息中间件","tags_index":"rabbit","author_index":"黑白搬砖工"},{"id":"4d529d29bdaac9a514904201dfd2325c","title":"带你看懂kafka","content":"1.前言kafka服务端存在相当多的术语，只有了解这些术语具体的含义，我们才能对kafka有一个粗略的认识。本文将带着你去了解以及理清相关术语，让你不再对其感到陌生以及恐惧。\n2.集群搭建工欲善其事必先利其器，在正式开始前，我们需要搭建一个kafka集群\n集群搭建选择使用docker-compose方式，使用该方式的好处就是：在docker-compose.yml文件编写完成的情况下，只需要一条命令即可实现集群的运行与停止\n2.1 编写docker-compose.ymlversion: \"3\"\nservices:\n  zookeeper:\n    image: 'zookeeper:latest'\n    ports:\n      - '2181:2181'\n    environment:\n      - ALLOW_ANONYMOUS_LOGIN=yes\n  kafka0:\n    image: 'bitnami/kafka:latest'\n    ports:\n      - '9092:9092'\n    environment:\n      - KAFKA_BROKER_ID=0\n      - KAFKA_CFG_LISTENERS=PLAINTEXT://0.0.0.0:9092\n      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://192.168.0.101:9092\n      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181\n      - ALLOW_PLAINTEXT_LISTENER=yes\n    volumes:\n      - 宿主机日志目录:/bitnami/kafka\n    depends_on:\n      - zookeeper\n  kafka1:\n    image: 'bitnami/kafka:latest'\n    ports:\n      - '9093:9093'\n    environment:\n      - KAFKA_BROKER_ID=1\n      - KAFKA_CFG_LISTENERS=PLAINTEXT://0.0.0.0:9093\n      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://192.168.0.101:9093\n      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181\n      - ALLOW_PLAINTEXT_LISTENER=yes\n    volumes:\n      - 宿主机日志目录:/bitnami/kafka\n    depends_on:\n      - zookeeper\n  kafka2:\n    image: 'bitnami/kafka:latest'\n    ports:\n      - '9094:9094'\n    environment:\n      - KAFKA_BROKER_ID=2\n      - KAFKA_CFG_LISTENERS=PLAINTEXT://0.0.0.0:9094\n      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://192.168.0.101:9094\n      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181\n      - ALLOW_PLAINTEXT_LISTENER=yes\n    volumes:\n      - 宿主机日志目录:/bitnami/kafka\n    depends_on:\n      - zookeeper\n\n\n\n\n\n\n\n\n\n\n注意：如果希望通过宿主机连接kafka集群，需要添加KAFKA_CFG_LISTENERS和KAFKA_CFG_ADVERTISED_LISTENERS配置并且KAFKA_CFG_ADVERTISED_LISTENERS配置地址为宿主机对应ip\nKAFKA_CFG_ADVERTISED_LISTENERS为暴露给客户端的监听地址\n2.2 运行集群在编写完docker-compose.yml文件后，只需下面的一条命令即可完成集群的运行\ndocker-compose up -d\n\n集群运行正常，你将看到如下内容输出\nCreating network \"kafka_default\" with the default driver\nCreating kafka_zookeeper_1 ... done\nCreating kafka_kafka0_1    ... done\nCreating kafka_kafka2_1    ... done\nCreating kafka_kafka1_1    ... done\n\n2.3 停止集群如果你想停止集群，那么同样可以使用一条命令来实现\ndocker-compose down\n\n3.概念3.1 主题主题(topic)作为一个逻辑概念，在系统中主要用于区分业务场景，比如可以为下单场景创建一个topic：create-order，为加入购物车场景创建一个topic：add-cart\n3.1.1 创建主题bin/kafka-topics.sh --create --topic create-order --bootstrap-server 127.0.0.1:9092 --partitions 3 --replication-factor 3\n\n\n\n\n\n\n\n\n\n\n–partitions：用于指定主题对应分区数量\n–replication-factor：用于指定分区对应副本数量\n3.1.2 查看主题信息bin/kafka-topics.sh --describe --topic create-order --bootstrap-server localhost:9092\n\n\n\n\n\n\n\n\n\n\n\n通过如上命令可以查看主题详细信息，包括：分区数量、分区leader、分区副本数、AR以及ISR\n3.2 分区在3.1创建主题部分我们通过--partitions参数指定了主题对应分区数\n分区作为一个物理概念，分散在各个broker中，可以提升并发能力以及服务性能\n\n3.3 副本副本的存在是为了提升分区数据可靠性，副本分为leader副本和flower副本，leader副本负责处理客户端的读写请求，flower副本只负责从leader副本同步数据\n\n3.4 ARAR(Assigned Replicas)：分区所有副本集合\ntopic：create-order，partition0对应AR：1，2，0\ntopic：create-order，partition1对应AR：0，1，2\ntopic：create-order，partition2对应AR：2，0，1\n3.5 ISRISR(In-Sync Replicas)：保持正常同步的副本集合(包含leader副本)\ntopic：create-order，partition0对应ISR：1，2，0\ntopic：create-order，partition1对应ISR：0，1，2\ntopic：create-order，partition2对应ISR：2，0，1\n3.6 OSROSR(Out-Of-Sync Replicas)：与leader副本同步滞后过多的flower副本集合\n\n\n\n\n\n\n\n\n\n当OSR为空时，AR &#x3D; ISR，否则，AR &#x3D; ISR + OSR\n3.7 LE0LEO(Log End Offset)：分区待写入消息对应偏移量\n\n3.8 HWHW(High Water)：分区所有副本都会维护自己的LEO，副本中最小的LEO即为HW\n\n\n\n\n\n\n\n\n\n\n注意：消费者只能拉取到HW之前的消息进行消费\n3.9 ControllerController：集群协调器，负责\n\n创建、删除主题，增加分区并分配leader分区；\n集群Broker管理（新增 Broker、Broker 主动关闭、Broker 故障)\npreferred leader选举\n分区重分配\n\n从Controller作用可以看到其重要性，那么Controller是如何选举出来的呢？\n在集群启动的过程中，节点会在zookeeper中创建controller节点，谁先创建成功谁就是Controller\n\n\n\n\n\n\n\n\n\n\n通过重启集群，可以看到controller节点下brokerid对应的值也在变化\n\n3.10 leader选举\ntopic：create-order，partition：0在未停掉brokerid:2的情况下，分区leader分配在brokerid为2的机器上\ntopic：create-order，partition：0在停掉brokerid:2的情况下，分区leader分配在brokerid为1的机器上\n\n\ntopic：create-order，partition：1在未停掉brokerid:0的情况下，分区leader分配在brokerid为0的机器上\ntopic：create-order，partition：1在停掉brokerid:0的情况下，分区leader分配在brokerid为2的机器上\n\n\n\n\n\n\n\n\n\n由此可见，分区的leader的选举首选要保证在Isr列表中，按照Replicas列表的顺序选举\n4.zookeeper中存储的信息\n","slug":"带你看懂kafka","date":"2022-12-11T05:51:06.000Z","categories_index":"消息中间件","tags_index":"kafka","author_index":"黑白搬砖工"},{"id":"a74d032dc9bf8ca9d012adc342b4fd84","title":"动态伸缩你的服务","content":"1.前言如你所知，服务的常规部署方式如下：\n\n对外暴露的服务都会在前面部署nginx用于提供反向代理和负载均衡能力\n下面会快速部署一套类似的服务，分析其存在的问题并给出相应解决方案\n2.应用相关2.1 启动服务使用boot-cloud-openfeign-provider启动3个服务实例，端口分别为8081、8082、8083\n2.2 服务验证在浏览器中分别输入：http://localhost:8081/index/nginx、http://localhost:8082/index/nginx、http://localhost:8083/index/nginx，保证3个服务实例均可被访问\n\n\n\n\n\n\n\n\n\n服务运行依赖5.1和5.2中的consul\n3.nginx相关3.1 安装3.1.1 搜索镜像docker search nginx\n\n3.1.2 拉取镜像docker pull nginx\n\n3.1.3 运行镜像docker run --name nginx -p 80:80 -d nginx\n\n3.1.4 拷贝镜像配置文件docker cp nginx:/etc/nginx/nginx.conf path/nginx[宿主机放配置文件路径]\n\n3.1.5 删除容器docker rm -f nginx\n\n3.1.6 指定配置文件运行镜像docker run --name nginx -p 80:80 -v [宿主机路径]/nginx/nginx.conf:/etc/nginx/nginx.conf:ro -v [宿主机路径]/nginx/conf.d:/etc/nginx/conf.d -d nginx\n\n3.2 配置3.2.1 配置负载均衡在[宿主机路径]/nginx/conf.d目录下创建load-balancer.conf文件，内容如下：\nupstream backend &#123;\n    server 10.100.40.243:8081;\n    server 10.100.40.243:8082;\n    server 10.100.40.243:8083;\n&#125;\n\nserver&#123;\n    listen 80;\n\n    location / &#123;\n        proxy_pass http://backend;\n    &#125;\n&#125;\n\n3.3 验证3.3.1 重启nginx服务docker exec -it nginx nginx -s reload\n\n3.3.2 访问nginx服务在浏览器中输入：http://localhost/index/nginx，多次请求依次可以看到`openfeign service：hello nginx from 8081、openfeign service：hello nginx from 8082、openfeign service：hello nginx from 8083&#96;结果，则说明nginx负载均衡配置正常。\n4.问题如你所见，服务的负载均衡能力是通过在load-balancer.conf配置文件中写死服务列表来实现的，同时也意味着，只要后端服务列表发生变化，就需要修改配置并通过nginx -s reload命令来重新加载。\n5.方案有了问题，既然会有对应的解决方案，本文要介绍的解决方案就是consul和consul template\n5.1 安装consul根据Install Consul官网教程安装consul\n5.2 运行consulconsul agent -dev\n\n5.3 安装consul template5.3.1 下载压缩包wget https://releases.hashicorp.com/consul-template/0.20.0/consul-template_0.20.0_linux_amd64.zip\n\n5.3.2 解压unzip consul-template_0.20.0_linux_amd64.zip\n\n\n\n\n\n\n\n\n\n\nMac os可以通过brew install consul-template进行安装，简单、方便\n5.4 创建负载均衡配置文件模板创建load-balancer.ctmpl文件，编辑如下内容：\nupstream backend &#123;\n    &#123;&#123;- range service \"openfeign-provider-service\" &#125;&#125;\n        server &#123;&#123; .Address &#125;&#125;:&#123;&#123; .Port &#125;&#125;;\n    &#123;&#123;- end &#125;&#125;\n&#125;\n\nserver &#123;\n   listen 80;\n\n   location / &#123;\n      proxy_pass http://backend;\n   &#125;\n&#125;\n\n5.5 清空配置文件清空3.2.1章节load-balancer.conf文件内容\n5.6 创建consul template配置创建consul-template-config.hcl文件，编辑如下内容：\nconsul &#123;\n  address = \"localhost:8500\"\n\n  retry &#123;\n    enabled  = true\n    attempts = 12\n    backoff  = \"250ms\"\n  &#125;\n&#125;\ntemplate &#123;\n  source      = \"[5.4章节指定的路径]/load-balancer.conf.ctmpl\"\n  destination = \"[5.5章节指定的路径]/load-balancer.conf\"\n  perms       = 0600\n  command     = \"sh -c docker exec -it nginx nginx -s reload\"\n&#125;\n\n\n\n\n\n\n\n\n\n\n\naddress：指定consul地址；source：用于指定负载均衡配置模板文件路径；destination：负载均衡配置生成文件路径；command：用于指定要执行的命令。整体流程：consul template从address地址拉取服务地址列表，根据source模板文件生成负载均衡配置到destination文件中，执行command重新加载nginx\n5.7 运行consul templateconsul-template -config=consul-template-config.hcl\n\n\n\n\n\n\n\n\n\n\n运行consul template之后，会发现load-balancer.conf文件中有了负载均衡配置\n5.8 服务关闭、启动到这里，你会发现伴随着服务关闭、服务启动，load-balancer.conf配置文件中的内容也在一直跟着改变，从而实现服务动态伸缩。\n","slug":"动态伸缩你的服务","date":"2022-12-10T13:57:00.000Z","categories_index":"微服务","tags_index":"consul,nginx,consul-template","author_index":"黑白搬砖工"},{"id":"9f635fa5cf662529fd1247abdd3830ea","title":"kafka存储","content":"1.前言系统仅仅拥有高并发、高可用往往是不够的，在此基础上还需要具备高可靠性。那么什么是高可靠性呢？高可靠性就是系统在宕机恢复后数据不丢失，仍然可以保证业务的正常运行。\n作为业务系统的开发人员，提到数据持久化，基于本能反应首选想到的一定是数据库。kafka作为消息中间件，会选择什么方式来进行数据的持久化呢？答案是：日志文件\n2.日志文件内容既然知道kafka采用日志文件的方式来对数据进行持久化，想你一定会好奇，日志文件中到底存储的是什么内容？\n2.1 如何找到日志文件位置想看日志文件内容，首先你得知道日志文件路径，通过config目录下server.properties配置文件中的log.dirs=配置项可以确定日志文件存放路径\n确定配置文件路径后，切换到对应目录，依据topic + partition来确定目标目录，比如想查看topic 为product，0号分区对应的日志文件，需要找到product-0 目录即可\n进入目标目录，查看目录下的文件信息\n-rw-r--r--  1   1137481367        24  9  3 16:02 00000000000000000000.index\n-rw-r--r--  1   1137481367     16375  9  3 16:02 00000000000000000000.log\n-rw-r--r--  1   1137481367        48  9  3 16:02 00000000000000000000.timeindex\n-rw-r--r--  1   1137481367        24  9  3 16:02 00000000000000002186.index\n-rw-r--r--  1   1137481367     16224  9  3 16:02 00000000000000002186.log\n-rw-r--r--  1   1137481367        48  9  3 16:02 00000000000000002186.timeindex\n-rw-r--r--  1   1137481367        24  9  3 16:02 00000000000000004415.index\n-rw-r--r--  1   1137481367     16275  9  3 16:02 00000000000000004415.log\n-rw-r--r--  1   1137481367        48  9  3 16:02 00000000000000004415.timeindex\n-rw-r--r--  1   1137481367        24  9  3 16:02 00000000000000006647.index\n-rw-r--r--  1   1137481367     16278  9  3 16:02 00000000000000006647.log\n-rw-r--r--  1   1137481367        48  9  3 16:02 00000000000000006647.timeindex\n-rw-r--r--  1   1137481367  10485760  9  3 16:02 00000000000000008879.index\n-rw-r--r--  1   1137481367      8594  9  3 16:02 00000000000000008879.log\n-rw-r--r--  1   1137481367  10485756  9  3 16:02 00000000000000008879.timeindex\n\n通过文件信息展示可看出kafka日志中会有三种文件，分别以.index、.log、.timeindex结尾，从名字上可以看出.index、.timeindex应该与索引相关,.log文件才是真正存储数据的地方\n2.2 查看日志文件内容明确存储数据文件后，你就可以通过vi命令打开.log文件查看具体内容，不出意外，映入眼帘的是满屏看不懂、不明所以的玩意\n\n看不懂，那一定是打开的方式不对，千万不要怀疑是自己能力的问题，你可以尝试使用如下方式打开文件\nbin&#x2F;kafka-run-class.sh kafka.tools.DumpLogSegments --deep-iteration --print-data-log --files &#x2F;xxxxx&#x2F;kafka&#x2F;kafka_2.12-3.2.1&#x2F;logs&#x2F;product-0&#x2F;00000000000000000000.log\n\n使用正确方式打开文件内容\nbaseOffset: 0 lastOffset: 121 count: 122 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: false deleteHorizonMs: OptionalLong.empty position: 0 CreateTime: 1662192149996 size: 966 magic: 2 compresscodec: gzip crc: 3650920144 isvalid: true\n| offset: 0 CreateTime: 1662192149978 keySize: -1 valueSize: 88 sequence: -1 headerKeys: [__TypeId__] payload: \"&#123;\\\"producerId\\\":\\\"strimzi-canary-client\\\",\\\"messageId\\\":1,\\\"timestamp\\\":1662192149651&#125;\"\n...\n| offset: 121 CreateTime: 1662192149996 keySize: -1 valueSize: 90 sequence: -1 headerKeys: [__TypeId__] payload: \"&#123;\\\"producerId\\\":\\\"strimzi-canary-client\\\",\\\"messageId\\\":122,\\\"timestamp\\\":1662192149996&#125;\"\n\n\nbaseOffset: 122 lastOffset: 243 count: 122 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: false deleteHorizonMs: OptionalLong.empty position: 966 CreateTime: 1662192150002 size: 936 magic: 2 compresscodec: gzip crc: 635681227 isvalid: true\n| offset: 122 CreateTime: 1662192149996 keySize: -1 valueSize: 90 sequence: -1 headerKeys: [__TypeId__] payload: \"&#123;\\\"producerId\\\":\\\"strimzi-canary-client\\\",\\\"messageId\\\":123,\\\"timestamp\\\":1662192149996&#125;\"\n...\n| offset: 243 CreateTime: 1662192150002 keySize: -1 valueSize: 90 sequence: -1 headerKeys: [__TypeId__] payload: \"&#123;\\\"producerId\\\":\\\"strimzi-canary-client\\\",\\\"messageId\\\":244,\\\"timestamp\\\":1662192150002&#125;\"\n\n\nbaseOffset: 244 lastOffset: 365 count: 122 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: false deleteHorizonMs: OptionalLong.empty position: 1902 CreateTime: 1662192150007 size: 905 magic: 2 compresscodec: gzip crc: 2330978284 isvalid: true\n| offset: 244 CreateTime: 1662192150002 keySize: -1 valueSize: 90 sequence: -1 headerKeys: [__TypeId__] payload: \"&#123;\\\"producerId\\\":\\\"strimzi-canary-client\\\",\\\"messageId\\\":245,\\\"timestamp\\\":1662192150002&#125;\"\n...\n| offset: 365 CreateTime: 1662192150007 keySize: -1 valueSize: 90 sequence: -1 headerKeys: [__TypeId__] payload: \"&#123;\\\"producerId\\\":\\\"strimzi-canary-client\\\",\\\"messageId\\\":366,\\\"timestamp\\\":1662192150007&#125;\"\n\n通过内容展示方式可以看到日志文件中记录了每条消息对应的offset、创建时间以及消息内容\n再打开00000000000000002186.log查看，可以得出一个结论，那就是文件名对应了该文件中第一条消息的offset\n3.性能3.1 顺序写数据写入磁盘是一种比较耗时操作，以至于看到该类操作的第一反应就是慢，能避免就尽量避免，但是为了保证数据的可靠性，这种操作往往又是不可或缺的\n其实这里往往有一个误区，所有的IO操作都比较慢嘛？当然不是，顺序IO虽没有内存操作快，但是也不差\nkafka也是通过采用顺序写IO方式来实现数据写盘，从而提升写性能\n\n3.2 分而治之不知你是否有过类似的经历，打开一个数兆文件只需要几秒时间，但是打开一个数百兆或者更大文件却需要几分钟甚至更长时间\n由此可以想象一下，如果不断往.log文件中追加消息，那么随着时间的推移.log文件会变得越来越大，大到打开该文件需要花费分钟级别的耗时，这对高性能kafka是完全不能接受的一件事情，那么如何解决大文件带来的耗时问题？\n分而治之是一种很好的思想，我们可以将一个大文件拆分成由多个小文件组成，从而解决大文件带来的性能问题\n\n3.3 索引查询现在你已经知道kafka会将客户端推送过来的消息存放在.log文件中，那么kafka是如何检索指定offset的消息给消费者进行消费的呢？\n不知你是否会想到前文提到的.index文件，通过如下命令来看看文件中存储的内容\nbin/kafka-run-class.sh kafka.tools.DumpLogSegments --print-data-log --files /xxx/kafka/kafka_2.12-3.2.1/logs/product-0/00000000000000000000.index\n\noffset: 731 position: 4623\noffset: 1338 position: 9121\noffset: 1943 position: 13634\n\n该文件存储了offset与position映射关系，到此可以得出根据offset就能找到对应的position的结论\n看完.index文件后，再来看看主角.log文件\nbaseOffset: 0 lastOffset: 121 count: 122 position: 0 CreateTime: 1662192149996 size: 966 magic: 2 compresscodec: gzip crc: 3650920144 isvalid: true\n| offset: 0 CreateTime: 1662192149978 keySize: -1 valueSize: 88 sequence: -1 headerKeys: [__TypeId__] payload: \"&#123;\\\"producerId\\\":\\\"strimzi-canary-client\\\",\\\"messageId\\\":1,\\\"timestamp\\\":1662192149651&#125;\"\n......\n| offset: 121 CreateTime: 1662192149996 keySize: -1 valueSize: 90 sequence: -1 headerKeys: [__TypeId__] payload: \"&#123;\\\"producerId\\\":\\\"strimzi-canary-client\\\",\\\"messageId\\\":122,\\\"timestamp\\\":1662192149996&#125;\"\nbaseOffset: 122 lastOffset: 243 count: 122 position: 966 CreateTime: 1662192150002 size: 936 magic: 2 compresscodec: gzip crc: 635681227 isvalid: true\nbaseOffset: 244 lastOffset: 365 count: 122 position: 1902 CreateTime: 1662192150007 size: 905 magic: 2 compresscodec: gzip crc: 2330978284 isvalid: true\nbaseOffset: 366 lastOffset: 487 count: 122 position: 2807 CreateTime: 1662192150011 size: 903 magic: 2 compresscodec: gzip crc: 1412054061 isvalid: true\nbaseOffset: 488 lastOffset: 609 count: 122 position: 3710 CreateTime: 1662192150015 size: 913 magic: 2 compresscodec: gzip crc: 3677057634 isvalid: true\nbaseOffset: 610 lastOffset: 731 count: 122 position: 4623 CreateTime: 1662192150019 size: 903 magic: 2 compresscodec: gzip crc: 1937572371 isvalid: true\n| offset: 610 CreateTime: 1662192150015 keySize: -1 valueSize: 90 sequence: -1 headerKeys: [__TypeId__] payload: \"&#123;\\\"producerId\\\":\\\"strimzi-canary-client\\\",\\\"messageId\\\":611,\\\"timestamp\\\":1662192150015&#125;\"\n......\n| offset: 731 CreateTime: 1662192150019 keySize: -1 valueSize: 90 sequence: -1 headerKeys: [__TypeId__] payload: \"&#123;\\\"producerId\\\":\\\"strimzi-canary-client\\\",\\\"messageId\\\":732,\\\"timestamp\\\":1662192150019&#125;\"\n\nbaseOffset: 732 lastOffset: 853 count: 122 position: 5526 CreateTime: 1662192150022 size: 902 magic: 2 compresscodec: gzip crc: 677681648 isvalid: true\nbaseOffset: 854 lastOffset: 975 count: 122 position: 6428 CreateTime: 1662192150025 size: 880 magic: 2 compresscodec: gzip crc: 1374850199 isvalid: true\nbaseOffset: 976 lastOffset: 1096 count: 121 position: 7308 CreateTime: 1662192150027 size: 909 magic: 2 compresscodec: gzip crc: 2771471302 isvalid: true\nbaseOffset: 1097 lastOffset: 1217 count: 121 position: 8217 CreateTime: 1662192150030 size: 904 magic: 2 compresscodec: gzip crc: 438650712 isvalid: true\nbaseOffset: 1218 lastOffset: 1338 count: 121 position: 9121 CreateTime: 1662192150033 size: 898 magic: 2 compresscodec: gzip crc: 983033759 isvalid: true\n\nbaseOffset: 1339 lastOffset: 1459 count: 121 position: 10019 CreateTime: 1662192150035 size: 895 magic: 2 compresscodec: gzip crc: 964946264 isvalid: true\nbaseOffset: 1460 lastOffset: 1580 count: 121 position: 10914 CreateTime: 1662192150040 size: 941 magic: 2 compresscodec: gzip crc: 2189697033 isvalid: true\nbaseOffset: 1581 lastOffset: 1701 count: 121 position: 11855 CreateTime: 1662192150043 size: 903 magic: 2 compresscodec: gzip crc: 2220582279 isvalid: true\nbaseOffset: 1702 lastOffset: 1822 count: 121 position: 12758 CreateTime: 1662192150045 size: 876 magic: 2 compresscodec: gzip crc: 3714367387 isvalid: true\nbaseOffset: 1823 lastOffset: 1943 count: 121 position: 13634 CreateTime: 1662192150048 size: 901 magic: 2 compresscodec: gzip crc: 531545191 isvalid: true\n\nbaseOffset: 1944 lastOffset: 2064 count: 121 position: 14535 CreateTime: 1662192150050 size: 896 magic: 2 compresscodec: gzip crc: 3782783873 isvalid: true\nbaseOffset: 2065 lastOffset: 2185 count: 121 position: 15431 CreateTime: 1662192150055 size: 944 magic: 2 compresscodec: gzip crc: 198975981 isvalid: true\n\n根据.log文件内容，可以画出下图\n结论：\n1.根据指定的offset在.index文件中找到对应的position\n2.根据position在.log文件中找到目标位置，如果查找目标offset大于position对应的offset，那么向后查找，否则向前查找，直到找到目标offset对应消息为止\n4.总结在了解完kafka消息存放形式以及查找方式后，我们能从中学到哪些内容呢？\n1.针对大文件带来的读写问题，可以通过拆分文件的方式进行解决，类比elasticsearch以及redis分片，分而治之思想\n2.针对查询慢的问题，可以通过创建索引方式进行解决，类比mysql索引、书本目录\n","slug":"kafka存储","date":"2022-12-11T05:50:10.000Z","categories_index":"消息中间件","tags_index":"kafka","author_index":"黑白搬砖工"},{"id":"5ce0c663a19d8e8ee699635ec101be20","title":"带你看懂redis IO模型","content":"1.前言如你所知，高性能web服务器Nginx选择使用IO多路复用技术处理客户端请求，高性能内存数据库Redis同样也选择使用IO多路复用技术处理客户端请求，这足以说明IO多路复用是非常优秀的IO模型，毕竟人以类聚、物以群分。\n接下来会从传统阻塞IO模型说起，聊到传统非阻塞IO模型，再到IO多路复用\n2. IO模型2.1 传统阻塞IO模型\n传统阻塞IO模型中，当客户端与服务器端建立连接创建socket后，服务端需要从socket中读取数据，如果socket数据不可读，就会阻塞当前线程，又由于是单线程，就会导致服务端无法对外提供任何服务\n2.2 传统非阻塞IO模型通过对传统阻塞IO模型进行分析后，可得知问题的关键在单线程上，知道问题所在，对应解决方案也就有了，那就是针对每个client创建一个线程进行socket数据读取\n\n通过多线程解决了单线程阻塞导致服务不可用的问题，随之而来也带了一个新的问题，那就是线程过多会造成资源浪费(每个线程拥有1M栈空间)、上下文切换问题\n2.3 多路复用单线程和多线程都有对应的问题存在，那么还有其它更好的解决方案嘛？方案当然是有的，那就是IO多路复用。IO多路复用的实现有select()、poll()、epoll(),分别来看一下\n2.3.1 术语介绍文件描述符：客户端与服务端建立的socket连接称作一个文件描述符（File Descriptor 以下简称 FD）\n2.3.2 select()2.3.2.1 描述\n通过select()描述可以得知该函数允许应用程序同时监听多个fd，在fd可读可写之前会一直阻塞，同时还有一个很关键的点，那就是select()同时监听的文件描述符数量不能大于1024\n2.3.2.2 返回值\nselect()调用成功后会返回就绪fd个数\n2.3.2.3 流程应用程序调用select()函数将fd传给内核空间\n\n内核空间会对fd进行循环遍历，当有fd变得就绪后，应用程序的select()调用会返回就绪fd个数，此时应用程序再通过循环遍历方式读取就绪fd数据\n2.3.2.4 存在的问题\n内核不知道fd何时就绪，只能通过循环遍历的方式得知，会造成CPU资源浪费\n内核 只会返回就绪fd的个数，应用程序并不知道具体哪个fd是就绪状态，只能再次循环系统调用才可得知，会造成无效系统调用\n同时监听文件描述符数不能超过1024\n\n2.3.2 epoll2.3.3.1 描述\nepollAPI核心概念就是epoll instance，epoll instance是一个内核数据结构。从用户空间角度来看，epoll instance是一个包含进程注册的fd列表和就绪fd列表的容器\nepoll提供了3个系统调用用于创建和管理epoll实例\n\nepoll_create：创建一个epoll实例，并返回一个fd\nepoll_ctl：对fd进行增、删、改\nepoll_wait：阻塞等待IO事件\n\n2.3.3.2 流程\n先通过epoll_create创建一个epoll instance，再通过epoll_ctl往注册列表中添加fd并监听对应事件(比如读事件、写事件)，最后通过epoll_wait阻塞等待，直到就绪列表中有fd为止，期间如果某个fd就绪，会从注册列表中移动到就绪列表中，epoll_wait返回就绪fd个数\n通过流程可以看到：\n\n应用程序每次都是增量往注册列表中添加fd，而不像select那样每次都传所有fd\n\n内核空间通过事件驱动方式得知fd就绪，而不像select那样需要循环遍历\n\nepoll_wait返回的后，应用程序知道具体哪一个fd就绪，而不像select那样循环遍历所有fd才知道哪些处于就绪状态\n\n\n2.3.3.3 示例分析\n3. 查看redis的IO多路复用实现3.1 追踪redisstrace -ff -o ./redis.out redis-6.2.6/src/redis-server /opt/redis/redis-6.2.6/redis.conf\n\n3.2 查看追踪文件\nvi redis.out.5444\n\n\n可以看到redis通过epoll实现了IO多路复用\n","slug":"IO模型","date":"2022-12-11T05:49:28.000Z","categories_index":"redis","tags_index":"多路复用","author_index":"黑白搬砖工"},{"id":"f8c1d4f8bf32cb91acc7b2a551ae4194","title":"消息积压你作何处理？","content":"1.前言当我们使用@KafkaListener注解声明一个消费者时，该消费者就会轮询去拉取对应分区消息记录，消费消息记录，正如你所知道的那样，正常场景下会执行ack操作，提交offset到kafka服务器。但是异常场景下会如何执行，不知你是否也了解？在了解之前，先一起来看下异常处理器，看完之后想必会有所收获。\n2.异常处理器2.1 创建异常处理器在实例化org.springframework.kafka.listener.KafkaMessageListenerContainer.ListenerConsumer#ListenerConsumer的时候如果没有自定义异常处理器，会去创建SeekToCurrentErrorHandler作为默认异常处理器使用\nprotected ErrorHandler determineErrorHandler(GenericErrorHandler&lt;?> errHandler) &#123;\n    return errHandler != null ? (ErrorHandler) errHandler\n            : this.transactionManager != null ? null : new SeekToCurrentErrorHandler();\n&#125;\n\n2.2 声明补偿策略在构建默认异常处理器SeekToCurrentErrorHandler时会指定对应的补偿策略\n/**\n  * Construct an instance with the default recoverer which simply logs the record after\n  * &#123;@value SeekUtils#DEFAULT_MAX_FAILURES&#125; (maxFailures) have occurred for a\n  * topic/partition/offset, with the default back off (9 retries, no delay).\n  * @since 2.2\n  */\npublic SeekToCurrentErrorHandler() &#123;\n    this(null, SeekUtils.DEFAULT_BACK_OFF);\n&#125;\n\n\n\n\n\n\n\n\n\n\n从代码注释上我们可以了解到该补偿策略会进行9次无时间间隔重试\n2.3 调用异常处理器@Nullable\nprivate RuntimeException doInvokeRecordListener(final ConsumerRecord&lt;K, V> record, // NOSONAR\n        Iterator&lt;ConsumerRecord&lt;K, V>> iterator) &#123;\n\n    Object sample = startMicrometerSample();\n\n    try &#123;\n        // 1.消费消息\n        invokeOnMessage(record);\n        successTimer(sample);\n        recordInterceptAfter(record, null);\n    &#125; catch (RuntimeException e) &#123;\n        try &#123;\n            // 2.执行异常处理器\n            invokeErrorHandler(record, iterator, e);\n            // 3.提交offset\n            commitOffsetsIfNeeded(record);\n        &#125; catch (KafkaException ke) &#123;\n           \n        &#125;\n    &#125;\n    return null;\n&#125;\n\n\n\n\n\n\n\n\n\n\n这里可以看到当消息消费异常后，会调用默认异常处理器SeekToCurrentErrorHandler\n2.4 执行异常处理器public static boolean doSeeks(List&lt;ConsumerRecord&lt;?, ?>> records, Consumer&lt;?, ?> consumer, Exception exception,\n\t\t\tboolean recoverable, RecoveryStrategy recovery, @Nullable MessageListenerContainer container,\n\t\t\tLogAccessor logger) &#123;\n\n    Map&lt;TopicPartition, Long> partitions = new LinkedHashMap&lt;>();\n    AtomicBoolean first = new AtomicBoolean(true);\n    AtomicBoolean skipped = new AtomicBoolean();\n    records.forEach(record -> &#123;\n        if (recoverable &amp;&amp; first.get()) &#123;\n            try &#123;\n                // 1.判断该消息是否可重试\n                boolean test = recovery.recovered(record, exception, container, consumer);\n                skipped.set(test);\n            &#125;\n            catch (Exception ex) &#123;\n            &#125;\n        &#125;\n        if (!recoverable || !first.get() || !skipped.get()) &#123;\n            partitions.computeIfAbsent(new TopicPartition(record.topic(), record.partition()),\n                    offset -> record.offset());\n        &#125;\n        first.set(false);\n    &#125;);\n    // 2.重置分区偏移量，以便可以重复拉取异常消息\n    seekPartitions(consumer, partitions, logger);\n    return skipped.get();\n&#125;\n\n\n\n\n\n\n\n\n\n\n异常处理器执行过程：\n判断当前消息是否可重试\n如果当前消息可以重试，会将该消息对应offset存储在partitions中，紧接着通过seekPartitions方法来将当前分区offset重置为当前消息offset，以至在下一次拉取消息的时候，仍然可以拉取到该异常消息。\n如果当前消息不可以重试，判断此次拉取的消息是否只有一条，如果是，不做处理；如果不是，则通过partitions.computeIfAbsent方法设置分区offset为异常消息下一条消息对应offset，以至在下一次拉取的时候可以拉取到异常消息后的其它消息。\n2.5 提交偏移量@Nullable\nprivate RuntimeException doInvokeRecordListener(final ConsumerRecord&lt;K, V> record, // NOSONAR\n        Iterator&lt;ConsumerRecord&lt;K, V>> iterator) &#123;\n\n    Object sample = startMicrometerSample();\n\n    try &#123;\n        invokeOnMessage(record);\n    &#125;\n    catch (RuntimeException e) &#123;\n        try &#123;\n            invokeErrorHandler(record, iterator, e);\n            // 提交分区offset\n            commitOffsetsIfNeeded(record);\n        &#125; catch (KafkaException ke) &#123;\n        &#125;\n    &#125;\n    return null;\n&#125;\n\n\n\n\n\n\n\n\n\n\n当默认异常处理器重试达到最大次数9次后，会执行commitOffsetsIfNeeded方法，手动提交分区offset，从而可以避免再次消费到异常消息\n2.6 总结当消费消息异常，在没有声明异常处理器的前提下会选择使用默认异常处理器SeekToCurrentErrorHandler，默认异常处理器会对异常消息进行重试，在达到最大重试次数9次后，会手动提交异常消息offset，然后继续消费异常消息之后的其它消息。\n至此想必对消息消费异常有了一个大致认识，如有疑问，欢迎留言讨论。\n","slug":"kafka消费异常会如何处理？","date":"2022-12-11T05:48:32.000Z","categories_index":"消息中间件","tags_index":"kafka","author_index":"黑白搬砖工"},{"id":"f8c1d4f8bf32cb91acc7b2a551ae4194","title":"消息积压你作何处理？","content":"1.前言当被通知消费的队列存在消息积压并呈一个持续上升趋势，需要紧急处理一下，你会怎么办？能怎么办呢？先内心慌张一会，战斗一番，然后打开著名的搜索引擎进行搜索，在搜索结果中寻找答案。本文也即将成为你搜索引擎中出现的一个搜索结果，为你提供解决方案。\n2.解决方案以下解决方案都是针对rabbitmq\n2.1 加机器最简单最方便的处理方案就是让运维加机器，这样消息队列中的消息可以通过负载的方式将消息均摊到更多的机器上，消费者多了，会慢慢处理掉队列中积压的消息。\n2.2 加消费者当然加机器方案并不是万能的，如果你的系统不够格，是加不了加机器(成本过高)，那么就得另辟蹊径了，增加并发消费线程数。\n2.2.1 固定并发数\n&#96;&#96;&#96;java\n&#x2F;**\n\t * Set the concurrency of the listener container for this listener. Overrides the\n\t * default set by the listener container factory. Maps to the concurrency setting of\n\t * the container type.\n\t * &lt;p&gt;For a\n\t * &#123;@link org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer\n\t * SimpleMessageListenerContainer&#125; if this value is a simple integer, it sets a fixed\n\t * number of consumers in the &#123;@code concurrentConsumers&#125; property. If it is a string\n\t * with the form &#123;@code &quot;m-n&quot;&#125;, the &#123;@code concurrentConsumers&#125; is set to &#123;@code m&#125;\n\t * and the &#123;@code maxConcurrentConsumers&#125; is set to &#123;@code n&#125;.\n\t * &lt;p&gt;For a\n\t * &#123;@link org.springframework.amqp.rabbit.listener.DirectMessageListenerContainer\n\t * DirectMessageListenerContainer&#125; it sets the &#123;@code consumersPerQueue&#125; property.\n\t * @return the concurrency.\n\t * @since 2.0\n\t *&#x2F;\n\n\n\n\n\n\n\n\n\n\n翻译过来就是为监听器设置并发数，如果属性值是一个int类型，那么设置的就是固定并发数；如果属性值是一个string类型(m-n)，那么设置的就是动态并发数，最小并发数为m，最大并发数为n\n基于文档注释，可以得出一个结论，那就是如果想要设置并发，那么只需要在@RabbitListener注解中添加concurrency属性并指定一个数值即可\n@RabbitListener(queues = \"test3\", containerFactory = \"rabbitListenerContainerFactory\", concurrency = \"10\")\n\n2.2.2 动态并发数设置消费者并发数后消息消费快了，也不积压了，问题被完美解决。由于存在动态并发数这一章节，说明事情并没有这么简单。设置消费者并发数的初衷是想解决消息积压问题，但是消息并非时时刻刻都会积压，大部分情况下队列中可能都不存在可以消费的消息，此时消费者并发数的设置无疑会给系统带来一定的资源消耗。好在spring框架相关大佬已经给出了解决方案:动态并发数\n动态并发数设置形式：\n@RabbitListener(queues = \"test3\", containerFactory = \"rabbitListenerContainerFactory\", concurrency = \"1-10\")\n\n\n\n\n\n\n\n\n\n\nconcurrency设置为1-10则表示消费者并发数最小为1个，当有更多消息需要处理时，会逐渐增大消费者并发数，最大值为10，当没有消息需要处理是，会逐渐减少消费者并发数，减到1为止\n看完上面的描述，你可能认为我是在胡扯，且等看看如下代码实现再下结论\nprivate void checkAdjust(boolean receivedOk) &#123;\n    if (receivedOk) &#123;\n        // 1.当前消费者处于激活状态\n        if (isActive(this.consumer)) &#123;\n            this.consecutiveIdles = 0;\n            // 1.1 如果连续消息数超过默认值10，则考虑新增一个消费者\n            if (this.consecutiveMessages++ > SimpleMessageListenerContainer.this.consecutiveActiveTrigger) &#123;\n                considerAddingAConsumer();\n                this.consecutiveMessages = 0;\n            &#125;\n        &#125;\n    &#125;\n    else &#123;\n        this.consecutiveMessages = 0;\n        // 2.如果连续超过10次都没有消费到消息，则考虑终止当前消费者\n        if (this.consecutiveIdles++ > SimpleMessageListenerContainer.this.consecutiveIdleTrigger) &#123;\n            considerStoppingAConsumer(this.consumer);\n            this.consecutiveIdles = 0;\n        &#125;\n    &#125;\n&#125;\n\n3. 总结消息队列中出现消息积压，首先不要慌，其实是能加机器就加机器，不能加机器，说明你的系统不太重要，就得老老实实增加消费者并发数，增加消费者并发数基于上述认知，你应该设置动态并发数\n","slug":"消息积压你作何处理？","date":"2022-12-11T05:48:32.000Z","categories_index":"消息中间件","tags_index":"消息积压","author_index":"黑白搬砖工"},{"id":"9553bf6744373965b077add4719345f2","title":"探秘kafka消费者流程","content":"1.前言为什么会想着去探秘kafka消费者流程呢？在回答这个问题之前，先带你看两个示例，看过之后想必你也就知道其中的原因了\n2.示例2.1 示例一@KafkaListener(topics = \"product\", groupId = \"product1\")\npublic void consumeMessage1(ConsumerRecord&lt;String, String> consumerRecord, Acknowledgment ack) &#123;\n    log.info(\"receive message topic：&#123;&#125;, partition：&#123;&#125;，offset：&#123;&#125;，message：&#123;&#125;\", consumerRecord.topic(),\n            consumerRecord.partition(), consumerRecord.offset(), consumerRecord.value());\n    if (\"test\".equals(consumerRecord.value())) &#123;\n        throw new IllegalArgumentException(\"message content is empty\");\n    &#125;\n    ack.acknowledge();\n&#125;\n\n\n\n\n\n\n\n\n\n\n示例很简单，就是正常消费消息，在业务处理完成之后执行ack操作进行消息确认。也就是说如果不执行ack操作，那么这条消息应该可以一直消费，直到处理成功为止。事实上真的是这样嘛？\n2.2 测试方案通过kafka客户端发送如下消息：\naa\nbb\ntest\ncc\ndd\nee\n\n应用控制台会输出如下内容：\nreceive message topic：product, partition：0，offset：5，message：aa\nreceive message topic：product, partition：0，offset：6，message：bb\nreceive message topic：product, partition：0，offset：7，message：test\nreceive message topic：product, partition：0，offset：7，message：test\nreceive message topic：product, partition：0，offset：7，message：test\nreceive message topic：product, partition：0，offset：7，message：test\nreceive message topic：product, partition：0，offset：7，message：test\nreceive message topic：product, partition：0，offset：7，message：test\nreceive message topic：product, partition：0，offset：7，message：test\nreceive message topic：product, partition：0，offset：7，message：test\nreceive message topic：product, partition：0，offset：7，message：test\nreceive message topic：product, partition：0，offset：7，message：test\nreceive message topic：product, partition：0，offset：8，message：cc\nreceive message topic：product, partition：0，offset：9，message：dd\nreceive message topic：product, partition：0，offset：10，message：ee\n\n\n\n\n\n\n\n\n\n\n从输出内容来看，test这条消息在执行10后就丢失了，因为当执行到ee这条消息的时候会进行ack操作，一旦ack成功，下一次就会从ee这条消息对应offset  + 1位置开始拉取消息，如果不重置offset，永远无法再消费到test这条消息。想一想如果这种丢消息的情况发生在核心交易链路会造成什么影响？还有一点与认知不同的是：”消息没有被ack为什么会丢？”\n2.3 示例二@KafkaListener(topics = \"product\", groupId = \"product2\")\npublic void consumeMessage2(ConsumerRecord&lt;String, String> consumerRecord, Acknowledgment ack) &#123;\n    log.info(\"receive message topic：&#123;&#125;, partition：&#123;&#125;，offset：&#123;&#125;，message：&#123;&#125;\", consumerRecord.topic(),\n            consumerRecord.partition(), consumerRecord.offset(), consumerRecord.value());\n    try &#123;\n        if (\"test\".equals(consumerRecord.value())) &#123;\n            throw new IllegalArgumentException(\"message content is empty\");\n        &#125;\n        ack.acknowledge();\n    &#125; catch (Exception e) &#123;\n        log.error(\"consume message error：\", e);\n    &#125;\n&#125;\n\n\n\n\n\n\n\n\n\n\n示例2与示例1的区别就是多了异常捕获，接下来通过测试方案来看结果，可能也会超出你的认知。\n2.4 测试方案通过kafka客户端发送如下消息：\ntest\n\n\n\n\n\n\n\n\n\n\n应用程序控制台会输入异常和error日志，由于没有执行ack操作，理论上消息可以再次被拉取，事实上真是如此嘛？此时，只需要重启应用程序即可验证，在重启程序之后，可以看到消息再次被消费，这与我们的认知是一样的，即消息没有ack，就可以再次被消费。\n接下来再做一次测试，通过kafka客户端发送如下消息：\naa\ntest\nbb\n\n\n\n\n\n\n\n\n\n\n在揭晓之前，你可以先猜一猜最终的结果会如何呈现？\n重启应用后可以看到控制台输入如下内容：\nreceive message topic：product, partition：0，offset：76，message：aa\nreceive message topic：product, partition：0，offset：77，message：test\nconsume message error：\nreceive message topic：product, partition：0，offset：78，message：bb\n\n\n\n\n\n\n\n\n\n\n为了验证test这条消息没有被ack的消息能否再次被消费到，只需要重启应用即可。重启应用后想必你多少会有点迷茫，test这条消息最终并没有并消费，这条消息丢了，出现了和示例一同样诡异的现象：”未ack的消息丢了”\n通过对这两个示例的测试，我有了如下的疑惑：\n\n为什么在不捕获异常的情况下会对消息进行重试，在捕获异常的情况下不会对消息进行重试？\n为什么在捕获异常的情况下，异常消息之后不发送其它消息，异常消息在应用重启后可以继续被消费，发送其它消息后异常消息却丢了？\n为什么未ack的消息也会丢？\n\n看到这些疑惑，想必你也就知道为什么我会去探秘kafka消费者流程了，只有了解其中的原理，才能解开心中的这些疑惑。\n3.探秘kafka消费者流程3.1 寻找消费者组对应的Coordinator\n消费者与kafka交互，都是与消费者组对应的Coordinator进行通信，因此消费者首要任务就是寻找自己所在组对应的Coordinator\n对应源码可查看org.apache.kafka.clients.consumer.internals.ConsumerCoordinator#poll(org.apache.kafka.common.utils.Timer, boolean)中的ensureCoordinatorReady，该方法会向kafka中的一个broker发送请求，请求的响应结果会返回对应Coordinator的host、port、节点id\nprivate class FindCoordinatorResponseHandler extends RequestFutureAdapter&lt;ClientResponse, Void> &#123;\n    @Override\n    public void onSuccess(ClientResponse resp, RequestFuture&lt;Void> future) &#123;\n        log.debug(\"Received FindCoordinator response &#123;&#125;\", resp);\n        FindCoordinatorResponse findCoordinatorResponse = (FindCoordinatorResponse) resp.responseBody();\n        Errors error = findCoordinatorResponse.error();\n        if (error == Errors.NONE) &#123;\n            synchronized (AbstractCoordinator.this) &#123;\n                int coordinatorConnectionId = Integer.MAX_VALUE - findCoordinatorResponse.data().nodeId();\n                // 设置Coordinator\n                AbstractCoordinator.this.coordinator = new Node(\n                        coordinatorConnectionId,\n                        findCoordinatorResponse.data().host(),\n                        findCoordinatorResponse.data().port());\n                log.info(\"Discovered group coordinator &#123;&#125;\", coordinator);\n                client.tryConnect(coordinator);\n                heartbeat.resetSessionTimeout();\n            &#125;\n            future.complete(null);\n        &#125;\n    &#125;\n&#125;\n\n3.2 加入消费者组\n在寻找到Coordinator后，消费者会向对应的Coordinator发送加入消费者组请求\n对应源码可查看org.apache.kafka.clients.consumer.internals.ConsumerCoordinator#poll(org.apache.kafka.common.utils.Timer, boolean)中的ensureActiveGroup，该方法会向Coordinator发送加入消费者组请求\nRequestFuture&lt;ByteBuffer> sendJoinGroupRequest() &#123;\n    if (coordinatorUnknown())\n        return RequestFuture.coordinatorNotAvailable();\n    // send a join group request to the coordinator\n    log.info(\"(Re-)joining group\");\n    JoinGroupRequest.Builder requestBuilder = new JoinGroupRequest.Builder(\n            new JoinGroupRequestData()\n                    .setGroupId(rebalanceConfig.groupId)\n                    .setSessionTimeoutMs(this.rebalanceConfig.sessionTimeoutMs)\n                    .setMemberId(this.generation.memberId)\n                    .setGroupInstanceId(this.rebalanceConfig.groupInstanceId.orElse(null))\n                    .setProtocolType(protocolType())\n                    .setProtocols(metadata())\n                    .setRebalanceTimeoutMs(this.rebalanceConfig.rebalanceTimeoutMs)\n    );\n    log.debug(\"Sending JoinGroup (&#123;&#125;) to coordinator &#123;&#125;\", requestBuilder, this.coordinator);\n    int joinGroupTimeoutMs = Math.max(client.defaultRequestTimeoutMs(),\n        rebalanceConfig.rebalanceTimeoutMs + JOIN_GROUP_TIMEOUT_LAPSE);\n    // 消费者发送加入消费者组请求，请求响应处理可查看JoinGroupResponseHandler\n    return client.send(coordinator, requestBuilder, joinGroupTimeoutMs)\n            .compose(new JoinGroupResponseHandler(generation));\n&#125;\n\n3.3 Coordinator响应加入消费者请求\nCoordinator在收到消费者加入消费者组请求后，会从同一个消费者组中选择一个消费者作为leader，其余消费者作为flower\nprivate class JoinGroupResponseHandler extends CoordinatorResponseHandler&lt;JoinGroupResponse, ByteBuffer> &#123;\n    @Override\n    public void handle(JoinGroupResponse joinResponse, RequestFuture&lt;ByteBuffer> future) &#123;\n        Errors error = joinResponse.error();\n        if (error == Errors.NONE) &#123;\n            if (isProtocolTypeInconsistent(joinResponse.data().protocolType())) &#123;\n            &#125; else &#123;\n                synchronized (AbstractCoordinator.this) &#123;\n                    if (state != MemberState.PREPARING_REBALANCE) &#123;\n                    &#125; else &#123;\n                        // 如果当前消费者被选为leader\n                        if (joinResponse.isLeader()) &#123;\n                            onJoinLeader(joinResponse).chain(future);                          \n                        &#125; else &#123;\n                            onJoinFollower().chain(future);\n                        &#125;\n                    &#125;\n                &#125;\n            &#125;\n        &#125;\n    &#125;\n&#125;\n\n3.4 消费者leader制定分区分配方案如果某一消费者被Coordinator选为leader后，那么就需要负责制定该分组分区分配方案\n对应源码可查看org.apache.kafka.clients.consumer.internals.AbstractCoordinator#onJoinLeader中的performAssignment，该方法先寻找对应的分区分配器，根据分配器来给消费者组中的消费者分配分区\n@Override\nprotected Map&lt;String, ByteBuffer> performAssignment(String leaderId,\n                                                    String assignmentStrategy,\n                                                    List&lt;JoinGroupResponseData.JoinGroupResponseMember> allSubscriptions) &#123;\n    // 1.寻找分区分配器\n    ConsumerPartitionAssignor assignor = lookupAssignor(assignmentStrategy);\n    Set&lt;String> allSubscribedTopics = new HashSet&lt;>();\n    Map&lt;String, Subscription> subscriptions = new HashMap&lt;>();\n\n    // collect all the owned partitions\n    Map&lt;String, List&lt;TopicPartition>> ownedPartitions = new HashMap&lt;>();\n\n    for (JoinGroupResponseData.JoinGroupResponseMember memberSubscription : allSubscriptions) &#123;\n        Subscription subscription = ConsumerProtocol.deserializeSubscription(ByteBuffer.wrap(memberSubscription.metadata()));\n        subscription.setGroupInstanceId(Optional.ofNullable(memberSubscription.groupInstanceId()));\n        subscriptions.put(memberSubscription.memberId(), subscription);\n        allSubscribedTopics.addAll(subscription.topics());\n        ownedPartitions.put(memberSubscription.memberId(), subscription.ownedPartitions());\n    &#125;\n\n    // the leader will begin watching for changes to any of the topics the group is interested in,\n    // which ensures that all metadata changes will eventually be seen\n    updateGroupSubscription(allSubscribedTopics);\n\n    isLeader = true;\n\n    log.debug(\"Performing assignment using strategy &#123;&#125; with subscriptions &#123;&#125;\", assignor.name(), subscriptions);\n    // 2.根据分区分配器制定消费者组分区分配方案\n    Map&lt;String, Assignment> assignments = assignor.assign(metadata.fetch(), new GroupSubscription(subscriptions)).groupAssignment();\n\n    log.info(\"Finished assignment for group at generation &#123;&#125;: &#123;&#125;\", generation().generationId, assignments);\n\n    Map&lt;String, ByteBuffer> groupAssignment = new HashMap&lt;>();\n    for (Map.Entry&lt;String, Assignment> assignmentEntry : assignments.entrySet()) &#123;\n        ByteBuffer buffer = ConsumerProtocol.serializeAssignment(assignmentEntry.getValue());\n        groupAssignment.put(assignmentEntry.getKey(), buffer);\n    &#125;\n\n    return groupAssignment;\n&#125;\n\n3.5 发送同步组请求\n消费者在收到加入消费者组的响应后，如果被选为leader，那么该消费者负责制定分区分配方案，其它消费者不需要，之后所有消费者再次向Coordinator发送同步组请求\n3.6 Coordinator通知分区分配方案\nCoordinator在收到消费者leader制定的分区分配方案后，会将该方案通知到各个消费者，告诉每个消费者应该消费哪些分区\n对应源码可查看org.apache.kafka.clients.consumer.internals.AbstractCoordinator#joinGroupIfNeeded中的onJoinComplete，该方法会设置消费分区信息\n@Override\nprotected void onJoinComplete(int generation,\n                                String memberId,\n                                String assignmentStrategy,\n                                ByteBuffer assignmentBuffer) &#123;\n    log.debug(\"Executing onJoinComplete with generation &#123;&#125; and memberId &#123;&#125;\", generation, memberId);\n    ConsumerPartitionAssignor assignor = lookupAssignor(assignmentStrategy);\n    // Give the assignor a chance to update internal state based on the received assignment\n    groupMetadata = new ConsumerGroupMetadata(rebalanceConfig.groupId, generation, memberId, rebalanceConfig.groupInstanceId);\n\n    Set&lt;TopicPartition> ownedPartitions = new HashSet&lt;>(subscriptions.assignedPartitions());\n    // 1.反序列化分区信息\n    Assignment assignment = ConsumerProtocol.deserializeAssignment(assignmentBuffer);\n    Set&lt;TopicPartition> assignedPartitions = new HashSet&lt;>(assignment.partitions());\n    // 2.设置分区信息\n    subscriptions.assignFromSubscribed(assignedPartitions);\n&#125;\n\n3.6.1 反序列化分区信息\n\n\n\n\n\n\n\n\n\n通过调试可以看到当前消费者应该消费topic&#x3D;product的0号分区\n3.6.2 设置分区信息\n\n\n\n\n\n\n\n\n\n在反序列化得到分区信息后，会将分区信息设置在SubscriptionState对象的assignment属性中，后续消费者拉取消息的时候会用到\n3.7 小结到此，我们对消费者流程有了一个大致的认识，稍微总结下，内容如下：\n\n寻找后续交互的Coordinator\n消费者请求加入消费者组，Coordinator从消费者中选择一个作为leader\nleader消费者制定分区分配方案并同步Coordinator\nCoordinator向消费者下发分区分配方案\n消费者反序列分区信息并在本地设置分区信息\n\n3.8 初始化分区偏移量\n消费者有了分区信息后就可以拉取该分区存储的消息记录，在拉取消息记录之前，必须要明确从什么位置开始拉取，因此需要初始化分区偏移量。\n对应源码查看org.apache.kafka.clients.consumer.KafkaConsumer#updateAssignmentMetadataIfNeeded(org.apache.kafka.common.utils.Timer, boolean)中的updateFetchPositions\nprivate boolean updateFetchPositions(final Timer timer) &#123;\n    // If any partitions have been truncated due to a leader change, we need to validate the offsets\n    fetcher.validateOffsetsIfNeeded();\n    // 1.如果设置过offset直接返回\n    cachedSubscriptionHashAllFetchPositions = subscriptions.hasAllFetchPositions();\n    if (cachedSubscriptionHashAllFetchPositions) return true;\n    // 2.向kafka询问分区对应offset，如果存在则设置分区offset\n    if (coordinator != null &amp;&amp; !coordinator.refreshCommittedOffsetsIfNeeded(timer)) return false;\n    // 3.kafka不存在分区对应offset，需要将分区状态设置为AWAIT_RESET\n    subscriptions.resetInitializingPositions();\n    // 4.根据分区offset重置策略进行offset重置\n    fetcher.resetOffsetsIfNeeded();\n    return true;\n&#125;\n\n\n\n\n\n\n\n\n\n\n从源码分析中可以了解到这里存在两种情况，一种是kafka服务器中存在分区对应偏移量，另一种是kafka服务器中不存在分区对应偏移量，这里以第一种情况进行分析\npublic boolean refreshCommittedOffsetsIfNeeded(Timer timer) &#123;\n    final Set&lt;TopicPartition> initializingPartitions = subscriptions.initializingPartitions();\n    // 1.向Coordinator发送请求获取当前分区offset\n    final Map&lt;TopicPartition, OffsetAndMetadata> offsets = fetchCommittedOffsets(initializingPartitions, timer);\n    // 2.kafka服务器不存在分区对应offset，直接返回\n    if (offsets == null) return false;\n\n    for (final Map.Entry&lt;TopicPartition, OffsetAndMetadata> entry : offsets.entrySet()) &#123;\n        final TopicPartition tp = entry.getKey();\n        final OffsetAndMetadata offsetAndMetadata = entry.getValue();\n        if (offsetAndMetadata != null) &#123;\n            // first update the epoch if necessary\n            entry.getValue().leaderEpoch().ifPresent(epoch -> this.metadata.updateLastSeenEpochIfNewer(entry.getKey(), epoch));\n            // it's possible that the partition is no longer assigned when the response is received,\n            // so we need to ignore seeking if that's the case\n            if (this.subscriptions.isAssigned(tp)) &#123;\n                final ConsumerMetadata.LeaderAndEpoch leaderAndEpoch = metadata.currentLeader(tp);\n                final SubscriptionState.FetchPosition position = new SubscriptionState.FetchPosition(\n                        offsetAndMetadata.offset(), offsetAndMetadata.leaderEpoch(),\n                        leaderAndEpoch);\n                // 设置分区对应偏移量\n                this.subscriptions.seekUnvalidated(tp, position);\n                log.info(\"Setting offset for partition &#123;&#125; to the committed offset &#123;&#125;\", tp, position);\n            &#125; \n        &#125;\n    &#125;\n    return true;\n&#125;\n\n\n\n\n\n\n\n\n\n\n获取所有初始化状态分区列表，向Coordinator询问对应的offset，得到偏移量后进行本地初始化\n\n\n\n\n\n\n\n\n\n\n\n对比3.6.2中的截图，可以看到此处的position对象被赋值并且明确了分区offset，有了分区offset就可以拉取该分区对应的消息记录了\n3.9 拉取消息记录\n万事俱备，只欠东风。有了分区offset，只需要将topic、partition、offset告诉kafka服务器就可以获取到消息记录。\n3.9.1 发送拉取请求拉取消息分为两部分，第一步部分发送拉取请求，第二部分拉取消息记录，先来看看第一部分发送拉取请求\n对应源码查看org.apache.kafka.clients.consumer.KafkaConsumer#pollForFetches中的fetcher.sendFetches()\npublic synchronized int sendFetches() &#123;\n    // 1.准备请求参数\n    Map&lt;Node, FetchSessionHandler.FetchRequestData> fetchRequestMap = prepareFetchRequests();\n    for (Map.Entry&lt;Node, FetchSessionHandler.FetchRequestData> entry : fetchRequestMap.entrySet()) &#123;\n        final Node fetchTarget = entry.getKey();\n        final FetchSessionHandler.FetchRequestData data = entry.getValue();\n        // 2.构建请求对象\n        final FetchRequest.Builder request = FetchRequest.Builder\n                .forConsumer(this.maxWaitMs, this.minBytes, data.toSend())\n                .isolationLevel(isolationLevel)\n                .setMaxBytes(this.maxBytes)\n                .metadata(data.metadata())\n                .toForget(data.toForget())\n                .rackId(clientRackId);\n\n        if (log.isDebugEnabled()) &#123;\n            log.debug(\"Sending &#123;&#125; &#123;&#125; to broker &#123;&#125;\", isolationLevel, data.toString(), fetchTarget);\n        &#125;\n        // 3.发送拉取消息记录请求\n        RequestFuture&lt;ClientResponse> future = client.send(fetchTarget, request);\n        // We add the node to the set of nodes with pending fetch requests before adding the\n        // listener because the future may have been fulfilled on another thread (e.g. during a\n        // disconnection being handled by the heartbeat thread) which will mean the listener\n        // will be invoked synchronously.\n        this.nodesWithPendingFetchRequests.add(entry.getKey().id());\n        future.addListener(new RequestFutureListener&lt;ClientResponse>() &#123;\n            @Override\n            public void onSuccess(ClientResponse resp) &#123;\n                synchronized (Fetcher.this) &#123;\n                    try &#123;\n                        FetchResponse&lt;Records> response = (FetchResponse&lt;Records>) resp.responseBody();\n                        Set&lt;TopicPartition> partitions = new HashSet&lt;>(response.responseData().keySet());\n                        FetchResponseMetricAggregator metricAggregator = new FetchResponseMetricAggregator(sensors, partitions);\n                        for (Map.Entry&lt;TopicPartition, FetchResponse.PartitionData&lt;Records>> entry : response.responseData().entrySet()) &#123;\n                            TopicPartition partition = entry.getKey();\n                            FetchRequest.PartitionData requestData = data.sessionPartitions().get(partition);\n                            if (requestData == null) &#123;\n                               \n                            &#125; else &#123;\n                                long fetchOffset = requestData.fetchOffset;\n                                FetchResponse.PartitionData&lt;Records> partitionData = entry.getValue();\n\n                                log.debug(\"Fetch &#123;&#125; at offset &#123;&#125; for partition &#123;&#125; returned fetch data &#123;&#125;\",\n                                        isolationLevel, fetchOffset, partition, partitionData);\n\n                                Iterator&lt;? extends RecordBatch> batches = partitionData.records().batches().iterator();\n                                short responseVersion = resp.requestHeader().apiVersion();\n                                // 4.将消息记录放入本地队列中\n                                completedFetches.add(new CompletedFetch(partition, partitionData,\n                                        metricAggregator, batches, fetchOffset, responseVersion));\n                            &#125;\n                        &#125;\n                    &#125;\n                &#125;\n            &#125;\n    &#125;\n    return fetchRequestMap.size();\n&#125;\n\n\n\n\n\n\n\n\n\n\n通过分析可以得知，发送完拉取请求后会将拉取到的消息放入本地队列completedFetches中\n3.9.2 拉取消息记录对应源码查看org.apache.kafka.clients.consumer.internals.Fetcher.CompletedFetch#fetchRecords\nprivate List&lt;ConsumerRecord&lt;K, V>> fetchRecords(int maxRecords) &#123;\n    List&lt;ConsumerRecord&lt;K, V>> records = new ArrayList&lt;>();\n    try &#123;\n        for (int i = 0; i &lt; maxRecords; i++) &#123;\n            if (cachedRecordException == null) &#123;\n                corruptLastRecord = true;\n                lastRecord = nextFetchedRecord();\n                corruptLastRecord = false;\n            &#125;\n            if (lastRecord == null)\n                break;\n            // 1.解析消息记录并放入集合中\n            records.add(parseRecord(partition, currentBatch, lastRecord));\n            recordsRead++;\n            bytesRead += lastRecord.sizeInBytes();\n            // 2.nextFetchOffset = 最后一条消息记录offset + 1，比如最后一条消息offset = 81，那么nextFetchOffset=82\n            nextFetchOffset = lastRecord.offset() + 1;\n            // In some cases, the deserialization may have thrown an exception and the retry may succeed,\n            // we allow user to move forward in this case.\n            cachedRecordException = null;\n        &#125;\n    &#125;\n    return records;\n&#125;\n\n\n\n\n\n\n\n\n\n\n至此终于看到我们比较熟悉的内容ConsumerRecord&lt;K, V&gt;，也就是消费者方法参数中的ConsumerRecord&lt;String, String&gt; consumerRecord。在源码分析中特地强调了nextFetchOffset，那么它有什么用呢？一起来分析下如下代码，你就能明白了。\nprivate List&lt;ConsumerRecord&lt;K, V>> fetchRecords(CompletedFetch completedFetch, int maxRecords) &#123;\n    if (!subscriptions.isAssigned(completedFetch.partition)) &#123;\n    &#125; else if (!subscriptions.isFetchable(completedFetch.partition)) &#123;\n    &#125; else &#123;\n        FetchPosition position = subscriptions.position(completedFetch.partition);\n        if (completedFetch.nextFetchOffset == position.offset) &#123;\n            List&lt;ConsumerRecord&lt;K, V>> partRecords = completedFetch.fetchRecords(maxRecords);\n\n            log.trace(\"Returning &#123;&#125; fetched records at offset &#123;&#125; for assigned partition &#123;&#125;\",\n                    partRecords.size(), position, completedFetch.partition);\n            // 假设position.offset=81，拉取到的是offset=81位置对应的消息\n            // 通过控制台发送一条消息\n            // 从上面的分析可以知道completedFetch.nextFetchOffset = 最后一条消息offset + 1也就是82\n            if (completedFetch.nextFetchOffset > position.offset) &#123;\n                FetchPosition nextPosition = new FetchPosition(\n                        completedFetch.nextFetchOffset,\n                        completedFetch.lastEpoch,\n                        position.currentLeader);\n                log.trace(\"Update fetching position to &#123;&#125; for partition &#123;&#125;\", nextPosition, completedFetch.partition);\n                // 将分区偏移量设置为最后一条消息offset + 1，也就是82\n                subscriptions.position(completedFetch.partition, nextPosition);\n            &#125;\n            return partRecords;\n        &#125; else &#123;\n        &#125;\n    &#125;\n    return emptyList();\n&#125;\n\n\n\n\n\n\n\n\n\n\n\n通过调试也可以证明我们的结论是正确的。这里再次强调一下，每次拉取完消息后，消费者会将分区本地offset设置为最后一条消息对应offset + 1\n3.10 消费消息在拉取到消息后，就需要对消息记录进行消费\nprivate void doInvokeWithRecords(final ConsumerRecords&lt;K, V> records) &#123;\n    Iterator&lt;ConsumerRecord&lt;K, V>> iterator = records.iterator();\n    // 1.遍历消息记录\n    while (iterator.hasNext()) &#123;\n        if (this.stopImmediate &amp;&amp; !isRunning()) &#123;\n            break;\n        &#125;\n        final ConsumerRecord&lt;K, V> record = checkEarlyIntercept(iterator.next());\n        if (record == null) &#123;\n            continue;\n        &#125;\n        this.logger.trace(() -> \"Processing \" + ListenerUtils.recordToString(record));\n        // 2.调用监听器方法\n        doInvokeRecordListener(record, iterator);\n        if (this.nackSleep >= 0) &#123;\n            handleNack(records, record);\n            break;\n        &#125;\n    &#125;\n&#125;\n\n\n\n\n\n\n\n\n\n\n消费的过程也就是遍历消息记录，然后调用对应被@KafkaListener注解标注的方法\n4.解惑4.1 示例二解惑在解释疑惑之前，这里再贴一下对应内容，方便查看\n\n\n\n\n\n\n\n\n\n为什么在捕获异常的情况下，异常消息之后不发送其它消息，异常消息在应用重启后可以继续被消费，发送其它消息后异常消息却丢了？\n不知当你再次看到这个疑惑的时候心中是否已经有了答案，不管有没有答案，一起来分析下其中的缘由吧。\n假设消费者消费的最后一条消息对应的offset &#x3D; 81，那么下一次就应该拉取offset &#x3D; 82位置对应的消息。\n此时通过kafka客户端发送一条消息，消息内容为”test”，消费者就会拉取到该消息，紧接着修改本地分区offset &#x3D; 82 + 1 &#x3D; 83，消费该消息，业务报错，未执行ack操作，重启应用，消费者得到的分区offset仍然是82，所以可以继续消费test这条消息。\n此时通过kafka客户端再发送一条消息，消息内容为”bb”，由于在拉取”test”消息后会将本地分区offset修改为83，那么消费者就可以正常拉取到”bb”这条消息，成功消费，进而执行ack，应用重启，消费者得到的分区offset是84，所以”test”这条消息就丢了。\n\n\n\n\n\n\n\n\n\n一句话总结该疑惑的”罪魁祸首”就是在拉取完消息记录后，会将分区本地offset设置为最后一条消息对应offset + 1\n4.2 示例一解惑同样为了方便，这里再贴一下对应内容\n\n\n\n\n\n\n\n\n\n为什么在不捕获异常的情况下会对消息进行重试，重试9次之后消息丢失\n在解惑前先来看一下org.springframework.kafka.listener.KafkaMessageListenerContainer.ListenerConsumer中的一段代码\nprotected ErrorHandler determineErrorHandler(GenericErrorHandler&lt;?> errHandler) &#123;\n    return errHandler != null ? (ErrorHandler) errHandler\n            : this.transactionManager != null ? null : new SeekToCurrentErrorHandler();\n&#125;\n\n\n\n\n\n\n\n\n\n\n通过这段代码可以了解到在没有定义异常处理器的情况下会默认使用SeekToCurrentErrorHandler异常处理器\n既然知道会使用SeekToCurrentErrorHandler异常处理器，不妨点进去看看\n/**\n\t * Construct an instance with the default recoverer which simply logs the record after\n\t * &#123;@value SeekUtils#DEFAULT_MAX_FAILURES&#125; (maxFailures) have occurred for a\n\t * topic/partition/offset, with the default back off (9 retries, no delay).\n\t * @since 2.2\n\t */\n\tpublic SeekToCurrentErrorHandler() &#123;\n\t\tthis(null, SeekUtils.DEFAULT_BACK_OFF);\n\t&#125;\n\n\n\n\n\n\n\n\n\n\n通过注释我们可以了解到该异常处理器会进行无时间间隔的9次重试\n有了如上知识后，再来看下org.springframework.kafka.listener.KafkaMessageListenerContainer.ListenerConsumer#doInvokeRecordListener中的一段代码\nprivate RuntimeException doInvokeRecordListener(final ConsumerRecord&lt;K, V> record, // NOSONAR\n\t\t\t\tIterator&lt;ConsumerRecord&lt;K, V>> iterator) &#123;\n\n    Object sample = startMicrometerSample();\n\n    try &#123;\n        // 1.执行业务方法\n        invokeOnMessage(record);\n        successTimer(sample);\n        recordInterceptAfter(record, null);\n    &#125;\n    catch (RuntimeException e) &#123;\n        try &#123;\n            // 2.业务执行异常，执行默认异常处理器\n            invokeErrorHandler(record, iterator, e);\n            // 3.提交偏移量\n            commitOffsetsIfNeeded(record);\n        &#125;\n        catch (KafkaException ke) &#123;\n        &#125;\n        catch (RuntimeException ee) &#123;\n        &#125;\n        catch (Error er) &#123; // NOSONAR\n        &#125;\n    &#125;\n    return null;\n&#125;\n\n\n\n\n\n\n\n\n\n\n通过这段代码我们可以了解到，如果业务执行异常，底层会进行异常捕获并使用默认异常处理器SeekToCurrentErrorHandler进行9次重试，9次重试后还是失败，就会主动提交offset，因此异常消息会在9次重试后丢失。\n5. 源码入口org.apache.kafka.clients.consumer.KafkaConsumer#poll(org.apache.kafka.common.utils.Timer, boolean)\n其中org.apache.kafka.clients.consumer.KafkaConsumer#updateAssignmentMetadataIfNeeded(org.apache.kafka.common.utils.Timer, boolean)涉及3.1-3.6章节\norg.apache.kafka.clients.consumer.KafkaConsumer#updateFetchPositions涉及3.8章节\norg.apache.kafka.clients.consumer.KafkaConsumer#pollForFetches涉及3.9章节\n","slug":"探秘kafka消费者流程","date":"2022-12-11T05:47:00.000Z","categories_index":"消息中间件","tags_index":"kafka","author_index":"黑白搬砖工"},{"id":"a7366034e22a497a711893a0546ee23b","title":"系统重启你的应用会怎么样？","content":"1.前言近日就系统重启引发了一些思考，在系统重启过程中，正在进行的请求会如何被处理？正在消费的消息会不会丢失？异步执行的任务会不会被中断？既然存在这些问题，那我们的应用程序是不是就不能重启？但是，我们的应用程序随着版本迭代也在不断重启为什么这些问题没有出现呢？还是应用做了额外处理？带着这些疑问，结合场景模拟，看看实际情况怎么处理。\n2. 场景2.1 http请求2.1.1 创建请求@RestController\npublic class ShutDownController &#123;\n\n    @RequestMapping(\"shut/down\")\n    public String shutDown() throws InterruptedException &#123;\n        TimeUnit.SECONDS.sleep(20);\n        return \"hello\";\n    &#125;\n&#125;\n\n2.1.2 调用请求http://localhost:8080/shut/down\n2.1.3 模拟重启kill -2 应用pid\n\n2.1.4 现象\n2.1.5 结论请求执行过程中，关闭应用程序出现无法访问提示\n2.1.6 开启优雅关机如上出现的现象对用户来说很不友好，会造成用户一脸懵逼，那么有没有什么措施可以避免这种现象的出现呢？是否可以在应用关闭前执行完已经接受的请求，拒绝新的请求呢？答案可以的，只需要在配置文件中新增优雅关机配置\nserver:\n  shutdown: graceful # 设置优雅关闭，该功能在Spring Boot2.3版本中才有。注意：需要使用Kill -2 触发来关闭应用，该命令会触发shutdownHook\n\nspring:\n  lifecycle:\n    timeout-per-shutdown-phase: 30s # 设置缓冲时间，注意需要带上时间单位(该时间用于等待任务执行完成)\n\n添加完配置后，再次执行2.1.2和2.1.3流程，就会看到如下效果\n\n\n\n\n\n\n\n\n\n\n可以看到，即便在请求执行过程中关闭应用，已接收的请求依然会执行下去\n2.2 消息消费在前言提到过，消息消费过程中，关闭应用，消息是会丢失还是会被重新放入消息队列中呢？\n2.2.1 创建生产者@RestController\npublic class RabbitMqController &#123;\n\n    @Autowired\n    private RabbitTemplate rabbitTemplate;\n\n    @GetMapping(\"/sendBusinessMessage\")\n    public void sendBusinessMessage() throws InterruptedException &#123;\n        rabbitTemplate.convertAndSend(RabbitmqConfig.BUSINESS_EXCHANGE, RabbitmqConfig.BUSINESS_ROUTING_KEY, \"send message\");\n        TimeUnit.SECONDS.sleep(10000);\n    &#125;\n&#125;\n\n2.2.2 创建消费者@Component\n@RabbitListener(queues = RabbitmqConfig.BUSINESS_QUEUE_NAME)\n@Slf4j\npublic class BusinessConsumer &#123;\n\n    /**\n     * 操作场景：\n     * 1.通过RabbitmqApplication启动类启动应用程序\n     * 2.调用/sendBusinessMessage接口发送消息\n     * 3.RabbitMQ broker将消息发送给消费者\n     * 4.消费者收到消息后进行消费\n     * 5.消费者消费消息过程中，应用程序关闭，断开channel，断开connection，未ack的消息会被重新放入broker中\n     *\n     * @param content 消息内容\n     * @param channel channel通道\n     * @param message message对象\n     */\n    @RabbitHandler\n    public void helloConsumer(String content, Channel channel, Message message) &#123;\n        log.info(\"business consumer receive message：&#123;&#125;\", content);\n        try &#123;\n            // 模拟业务执行耗时\n            TimeUnit.SECONDS.sleep(10000);\n        &#125; catch (InterruptedException e) &#123;\n            e.printStackTrace();\n        &#125;\n    &#125;\n&#125;\n\n2.2.3 调用请求http://localhost:8080/sendBusinessMessage\n2.2.4 未关闭应用前\n2.2.5 关闭应用后\n2.2.6 结论消息消费过程中，关闭应用，未ack的消息会被重新放入消息队列中，以此来保证消息一定会被消费\n2.3 异步任务2.3.1 线程池配置@Component\npublic class ThreadPoolConfig &#123;\n\n    @Bean\n    public ThreadPoolTaskExecutor threadPoolTaskExecutor() &#123;\n        ThreadPoolTaskExecutor threadPoolTaskExecutor = new ThreadPoolTaskExecutor();\n        threadPoolTaskExecutor.setThreadNamePrefix(\"test-\");\n        threadPoolTaskExecutor.setCorePoolSize(3);\n        threadPoolTaskExecutor.setMaxPoolSize(3);\n        threadPoolTaskExecutor.setQueueCapacity(100);\n        return threadPoolTaskExecutor;\n    &#125;\n&#125;\n\n2.3.2 异步任务请求@Autowired\nprivate ThreadPoolTaskExecutor threadPoolTaskExecutor;\n\n@RequestMapping(\"async/task\")\npublic void asyncTask() throws InterruptedException &#123;\n  for (int i = 0; i &lt; 10; i++) &#123;\n    threadPoolTaskExecutor.execute(() -> &#123;\n      try &#123;\n        TimeUnit.SECONDS.sleep(10);\n      &#125; catch (InterruptedException e) &#123;\n        throw new RuntimeException();\n      &#125;\n      log.info(\"task execute complete...\");\n    &#125;);\n  &#125;\n&#125;\n\n2.3.3 调用请求http://localhost:8080/async/task\n2.3.4 模拟重启kill -2 应用pid\n\n2.3.5 现象Exception in thread \"test-2\" Exception in thread \"test-1\" Exception in thread \"test-3\" java.lang.RuntimeException\n\tat com.boot.example.ShutDownController.lambda$asyncTask$0(ShutDownController.java:37)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\njava.lang.RuntimeException\n\tat com.boot.example.ShutDownController.lambda$asyncTask$0(ShutDownController.java:37)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\njava.lang.RuntimeException\n\tat com.boot.example.ShutDownController.lambda$asyncTask$0(ShutDownController.java:37)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\n2.3.6 修改线程池配置在线程池配置中添加如下配置：\nthreadPoolTaskExecutor.setWaitForTasksToCompleteOnShutdown(true);\nthreadPoolTaskExecutor.setAwaitTerminationSeconds(120);\n\n2.3.7 修改配置后现象2021-12-09 17:09:40.054  INFO 22383 --- [         test-1] com.boot.example.ShutDownController      : task execute complete...\n2021-12-09 17:09:40.055  INFO 22383 --- [         test-3] com.boot.example.ShutDownController      : task execute complete...\n2021-12-09 17:09:40.055  INFO 22383 --- [         test-2] com.boot.example.ShutDownController      : task execute complete...\n2021-12-09 17:09:50.059  INFO 22383 --- [         test-3] com.boot.example.ShutDownController      : task execute complete...\n2021-12-09 17:09:50.059  INFO 22383 --- [         test-1] com.boot.example.ShutDownController      : task execute complete...\n2021-12-09 17:09:50.060  INFO 22383 --- [         test-2] com.boot.example.ShutDownController      : task execute complete...\n2021-12-09 17:10:00.062  INFO 22383 --- [         test-2] com.boot.example.ShutDownController      : task execute complete...\n2021-12-09 17:10:00.062  INFO 22383 --- [         test-1] com.boot.example.ShutDownController      : task execute complete...\n2021-12-09 17:10:00.065  INFO 22383 --- [         test-3] com.boot.example.ShutDownController      : task execute complete...\n2021-12-09 17:10:10.066  INFO 22383 --- [         test-1] com.boot.example.ShutDownController      : task execute complete...\n\n2.3.8 结论使用线程池执行异步任务，在没有添加配置的情况下，任务无法执行完成，在添加配置的情况下，任务依然可以执行完成。\n3. 总结为了保证在应用程序重启过程中任务仍然可以执行完成，需要开启优雅关机配置并对线程池添加等待任务执行完成以及等待时间配置\n","slug":"系统重启你的应用会怎么样？","date":"2022-12-10T13:58:00.000Z","categories_index":"微服务","tags_index":"优雅停机","author_index":"黑白搬砖工"},{"id":"24fad9e0b20684c1cf6b335f2dccd634","title":"三种你需要考虑的一致性问题的场景","content":"1.前言分布式系统开发过程中往往会涉及到很多需要保证数据一致性问题的场景，比如接收mq消息、接收http请求、内部业务处理。如果你还不了解这些场景或者不知道如何处理，请继续往下阅读。\n2.接收mq消息\n接收mq消息场景在分布式系统开发过程中想必是一种比较常见的场景，具体过程就是，外围系统推送mq消息到开发系统，开发系统接收消息后进行业务逻辑处理。从表面上看是一个十分简单的流程，但是如果涉及数据一致性问题，就不那么简单了。\n为什么说不那么简单呢？先来看看以下场景：\n2.1 先ack消息再处理业务ack();\n// 处理业务逻辑\n\n先ack消息再处理业务在理想场景下是不会有什么问题，看到这里你可能会有疑惑，没有问题那不就得了，还需要考虑什么呢？请注意这种没问题是建立在理想的前提下，如果业务处理过程中调用外部接口异常或者数据库宕机，就会导致消息丢失，进而出现数据不一致性的问题。\n2.2 先处理业务再ack消息// 处理业务逻辑\nack();\n\n既然先ack消息再处理业务这条路走不通，那么就先处理业务再ack消息总没问题了吧。是的，即便业务处理失败消息没有被ack，消息还会被重新消费，不会出现数据不一致性问题。但是这会涉及到另外一个问题，那就是幂等性问题，幂等问题处理不好，还是会引起数据不一致性问题。\n其实先处理业务再ack消息还会引起另一个问题，如果业务系统有bug会造成消息一直无法被ack，进而会导致消息处理进入死循环。\n这样也不行，那样也不行，就没有解决方案了？当然不是，方案还是有的，且听我慢慢跟你说\n2.3  结合消息ack机制 + 数据库 + 定时任务方案一try &#123;\n    try&#123;\n        // 根据消息唯一编号查询该消息是否已处理过，如果没有处理过，进行处理业务；如果处理过，则说明都不做\n    &#125; catch (Exception e) &#123;\n       // 将处理异常的消息插入数据库中\n    &#125;\n    ack();\n&#125; catch (Exception e) &#123;\n    unack();\n&#125;\n\n大体思路就是，根据消息唯一编号判断消息是否被处理过，如果未被处理过，就对消息进行处理，处理成功则对消息进行ack；处理失败则将消息存入数据库中。如果存入数据库这一步操作还是失败，那么就对消息进行unack操作，将消息重新投递到消息服务器中，进而重新消费，直到数据库恢复为止。针对处理失败入库的消息，可以通过定时任务重试处理。\n该方案不仅可以解决2.2中的幂等性问题，还可以解决业务出现bug进而导致消息处理进入死循环的问题(限制重试次数)。但是该方案还是会存在一个跟本文无关的问题，那就是消息积压问题。\n2.4 结合消息ack机制 + 数据库 + 定时任务方案二try &#123;\n    // 根据消息唯一编号查询该消息是否存在，不存在则直接插入数据库中，存在则不进行处理\n    ack();\n    // 异步处理业务逻辑\n&#125; catch (Exception e) &#123;\n    unack();\n&#125;\n\n2.4与2.3优缺点对比\n\n\n\n序号\n优点\n缺点\n\n\n\n2.3\n只在业务处理失败将消息插入数据库中，消息数量不会太多\n消息处理慢会导致消息积压\n\n\n2.4\n消息异步处理，不会导致消息积压\n所有消息都存储数据库，消息数量可能会很多\n\n\n关于这两种方案可以根据实际情况进行自由选择，消息积压问题处理也可以参考：消息积压你作何处理？\n3. 接收http请求\n看到这个图你可能会想这不就是一个很简单的流程嘛，开发系统接收请求、处理请求、响应结果就可以了。如你所想，确实很简单，但是如果你的开发系统业务处理失败，就会导致外围系统进行重试，直到重试次数用完，开发系统还未恢复正常，那么此次的外围请求数据就会丢失，从而引起数据不一致性问题。\n认真分析一下该场景，你会发现造成数据不一致性问题的关键在于开发系统的业务处理。如果开发系统能在正确接收外围系统请求后立刻进行响应，那么就可以解决该问题。\n我们只需要在接收http请求后，将请求内容写入数据库，写入成功进行异步处理并返回成功；写入失败返回失败，通过外围重试来保证数据可以正常写入数据库。处理失败的内容可以结合定时任务对请求进行重试。\n\n4.内部业务处理\n开发系统某些业务在处理成功后往往需要通知某些外围系统并且还不能因为外围系统故障从而导致当前业务无法正确处理，既然不能影响当前业务，可以采用异步的方式进行处理，异步处理就会存在当前业务处理成功，通知外围失败的问题，进而引起数据不一致性问题。\n那有没有办法可以保证业务处理成功的同时，对外围的通知也一定成功呢？\n我们可以采用本地事务的方案，把通知给外围的数据放在当前业务的事务中插入到数据库，异步通知外围系统，处理失败的数据再结合定时任务进行重试。\n\n5.总结从文中我们可以看到一致性问题的解决方案都逃不开数据库 + 重试，因此在解决一致性问题的时候可以多往这方面考虑。\n","slug":"三种你需要考虑一致性问题的场景","date":"2022-12-10T13:57:00.000Z","categories_index":"微服务","tags_index":"一致性","author_index":"黑白搬砖工"}]